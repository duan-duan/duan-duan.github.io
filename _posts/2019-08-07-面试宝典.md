---
title: 面试宝典
date: 2019-08-07 10:59:53
categories:
- 面试
tags:
- 事务

---
#面试宝典

## 1. java基础

### 1. String, StringBulider,StringBuffer

String的值是不可变的，这就导致每次对String的操作都会生成**新的String对象**

当**对字符串进行修改**的时候，需要使用 StringBuffer 和 StringBuilder 类。

由于 StringBuilder 相较于 StringBuffer 有速度优势，**所以多数情况下建议使用 StringBuilder 类**。然而在应用程序要求线程安全的情况下，则必须使用 StringBuffer 类

（1）如果要操作少量的数据用 String；

（2）多线程操作字符串缓冲区下操作大量数据 StringBuffer；

（3）**单线程操作字符串缓冲区下操作大量数据 StringBuilder**。

### 2. 序列化 Serializable

序列化的本质是 制定一定的规则，让Object 序列化后可以按照规则反序列化回来。

关于Serializable的使用，有几点需要说明：
1 Serializable只是一个接口，本身没有任何实现
2 对象的反序列化并没有调用对象的任何构造方法
3 serialVersionUID是用于记录文件版本信息的，最好能够自定义。否则，系统会自动生成一个serialVersionUID，文件或者对象的任何改变，都会改变serialVersionUID，导致反序列化的失败，如果自定义就没有这个问题。
4 如果某个属性不想实现序列化，可以采用transient修饰
5 Serializable的系统实现是采用ObjectInputStream和ObjectOutputStream实现的，这也是为什么调用ObjectInputStream和ObjectOutputStream时，需要对应的类实现Serializable接口。

java序列化 和hessian 序列化区别。dubbo 使用的的hessian2 

首先，hessian序列化比Java序列化高效很多，而且生成的字节流也要短很多。但相对来说没有Java序列化可靠，而且也不如Java序列化支持的全面。而之所以会出现这样的区别，则要从它们的实现方式来看。
先说Java序列化，具体工作原理就不说了，Java序列化会把要序列化的对象类的元数据和业务数据全部序列化从字节流，而且是把整个继承关系上的东西全部序列化了。它序列化出来的字节流是对那个对象结构到内容的完全描述，包含所有的信息，因此效率较低而且字节流比较大。但是由于确实是序列化了所有内容，所以可以说什么都可以传输，因此也更可用和可靠。
而hessian序列化，它的实现机制是着重于数据，附带简单的类型信息的方法。就像Integer a = 1，hessian会序列化成I 1这样的流，I表示int or Integer，1就是数据内容。而对于复杂对象，通过Java的反射机制，hessian把对象所有的属性当成一个Map来序列化，产生类似M className propertyName1 I 1 propertyName S stringValue（大概如此，确切的忘了）这样的流，包含了基本的类型描述和数据内容。而在序列化过程中，如果一个对象之前出现过，hessian会直接插入一个R index这样的块来表示一个引用位置，从而省去再次序列化和反序列化的时间。这样做的代价就是hessian需要对不同的类型进行不同的处理（因此hessian直接偷懒不支持short），而且遇到某些特殊对象还要做特殊的处理（比如StackTraceElement）。而且同时因为并没有深入到实现内部去进行序列化，所以在某些场合会发生一定的不一致，比如通过Collections.synchronizedMap得到的map。

### 3. SimpleDateFormat是线程不安全，

关于时间处理推荐进行如下，如果是jdk低于1.7用：
public final static String DATE_PATTERN = "yyyy-MM-dd";

```java
private static ThreadLocal<DateFormat> threadLocal = new ThreadLocal<DateFormat>() {
    @Override
    protected DateFormat initialValue() {
        return new SimpleDateFormat(DATE_TIME_PATTERN);
    }
};
```

1.8  线程安全   都是final 的 不可变的

```java
public static void main(String[] args) {
        DateTimeFormatter dateTimeFormatter1 = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss");
        DateTimeFormatter dateTimeFormatter2 = DateTimeFormatter.ofPattern("yyyy-MM-dd");
        DateTimeFormatter dateTimeFormatter3 = DateTimeFormatter.ofPattern("HH:mm:ss");
        LocalDateTime localDate1 = LocalDateTime.now();//年月日时分秒

        LocalDate localDate2 = LocalDate.now();//年月日

        LocalTime localDate3 = LocalTime.now();//时分秒

        System.out.println("年月日时分秒:"+dateTimeFormatter1.format(localDate1));

        System.out.println("年月日:"+dateTimeFormatter2.format(localDate2));

        System.out.println("时分秒:"+dateTimeFormatter3.format(localDate3));

    }

```

###  4. Object 

Object类有12个成员方法，分别是 clone():Object 、equals(Object):boolean、finalize():void、getClass() 、hashCode():int、notify():void、notifyAll():void、toString():String、wait():void、wait(long):void、wait(long,int):void

一个好的类。重写 equals ，toString  hashCode  这三个方法

对比两个对象是否相等。对于下面的 User 对象，只需姓名和年龄相等则认为是同一个对象。

需要重写对象的 equals 方法和 hashCode 方法

```java
 		@Override
    public String toString() {
        return this.id + " " + this.name + " " + this.age;
    }
    @Override
    public boolean equals(Object obj) {
        if(this == obj){
            return true;//地址相等
        }
        if(obj == null){
            return false;//非空性：对于任意非空引用x，x.equals(null)应该返回false。
        }
        if(obj instanceof User){
            User other = (User) obj;
            //需要比较的字段相等，则这两个对象相等
            if(equalsStr(this.name, other.name)
                    && equalsStr(this.age, other.age)){
                return true;
            }
        }
        return false;
    }
	 @Override
    public int hashCode() {
        int result = 17;
        result = 31 * result + (name == null ? 0 : name.hashCode());
        result = 31 * result + (age == null ? 0 : age.hashCode());
        return result;
    }
}
```
##### 1. 为什么要重写 equals 方法

因为不重写 equals 方法，执行 user1.equals(user2) 比较的就是两个对象的地址（即 user1 == user2），肯定是不相等的，见 Object 源码：

```java
    public boolean equals(Object obj) {
        return (this == obj);
    }
```

##### 2。为什么要重写 hashCode 方法

　　既然比较两个对象是否相等，使用的是 equals 方法，那么只要重写了 equals 方法就好了，干嘛又要重写 hashCode 方法呢？

其实当 equals 方法被重写时，通常有必要重写 hashCode 方法，以维护 hashCode 方法的常规协定，该协定声明相等对象必须具有相等的哈希码
**所以：hashCode 是用于散列数据的快速存取，如利用 HashSet/HashMap/Hashtable 类来存储数据时，都会根据存储对象的 hashCode 值来进行判断是否相同的。**

##### 3. 如何重写 hashCode
　　生成一个 int 类型的变量 result，并且初始化一个值，比如17
　　对类中每一个重要字段，也就是影响对象的值的字段，也就是 equals 方法里有比较的字段，进行以下操作：a. 计算这个字段的值 filedHashValue = filed.hashCode(); b. 执行 result = 31 * result + filedHashValue;
String 源码中也使用的 31，然后网上说有这两点原因：

###### 原因一：更少的乘积结果冲突

　　31是质子数中一个“不大不小”的存在，如果你使用的是一个如2的较小质数，那么得出的乘积会在一个很小的范围，很容易造成哈希值的冲突。而如果选择一个100以上的质数，得出的哈希值会超出int的最大范围，这两种都不合适。而如果对超过 50,000 个英文单词（由两个不同版本的 Unix 字典合并而成）进行 hash code 运算，并使用常数 31, 33, 37, 39 和 41 作为乘子，每个常数算出的哈希值冲突数都小于7个（国外大神做的测试），那么这几个数就被作为生成hashCode值得备选乘数了。

　　所以从 31,33,37,39 等中间选择了 31 的原因看原因二。

###### 原因二：31 可以被 JVM 优化

　　JVM里最有效的计算方式就是进行位运算了：

　　* 左移 << : 左边的最高位丢弃，右边补全0（把 << 左边的数据*2的移动次幂）。
　　* 右移 >> : 把>>左边的数据/2的移动次幂。
　　* 无符号右移 >>> : 无论最高位是0还是1，左边补齐0。 　　

​    所以 ： 31 * i = (i << 5) - i（左边 31*2=62,右边  2*2^5-2=62） - 两边相等，JVM就可以高效的进行计算啦。。。

### 4. 分布式Session的几种实现方式

1. 基于数据库的Session共享
2. 基于NFS共享文件系统
3. 基于memcached 的session，如何保证 memcached 本身的高可用性？
4. 基于resin/tomcat web容器本身的session复制机制
5. 基于TT/Redis 或 jbosscache 进行 session 共享。
 或者是：
   一、**Session Replication 方式管理 (即session复制)**
   ​    **简介：**将一台机器上的Session数据广播复制到集群中其余机器上
   ​    **使用场景**：机器较少，网络流量较小
   ​    **优点**：实现简单、配置较少、当网络中有机器Down掉时不影响用户访问
   ​    **缺点**：广播式复制到其余机器有一定廷时，带来一定网络开销
   二、**Session Sticky 方式管理**
   ​    **简介：**即粘性Session、当用户访问集群中某台机器后，强制指定后续所有请求均落到此机器上
   ​    **使用场景**：机器数适中、对稳定性要求不是非常苛刻
   ​    **优点**：实现简单、配置方便、没有额外网络开销
   ​    **缺点**：网络中有机器Down掉时、用户Session会丢失、容易造成单点故障
   三、**缓存集中式管理**
   ​    **简介：**将Session存入分布式缓存集群中的某台机器上，当用户访问不同节点时先从缓存中拿Session信息
   ​    **使用场景**：集群中机器数多、网络环境复杂
   ​    **优点**：可靠性好
   ​    **缺点**：实现复杂、稳定性依赖于缓存的稳定性、Session信息放入缓存时要有合理的策略写入
   
#### 1. Session和Cookie的区别和联系以及Session的实现原理

1、session保存在服务器，客户端不知道其中的信息；cookie保存在客户端，服务器能够知道其中的信息。  
2、session中保存的是对象，cookie中保存的是字符串。  
3、session不能区分路径，同一个用户在访问一个网站期间，所有的session在任何一个地方都可以访问到。而cookie中如果设置了路径参数，那么同一个网站中不同路径下的cookie互相是访问不到的。  
   4、session需要借助cookie才能正常<nobr oncontextmenu="return false;" onmousemove="kwM(3);" id="key3" onmouseover="kwE(event,3, this);" style="COLOR: #6600ff; BORDER-BOTTOM: 0px dotted; BACKGROUND-COLOR: transparent; TEXT-DECORATION: underline" onclick="return kwC();" onmouseout="kwL(event, this);" target="_blank">工作</nobr>。如果客户端完全禁止cookie，session将失效。

   ​       http是无状态的协议，客户每次读取web页面时，服务器都打开新的会话，而且服务器也不会自动维护客户的上下文信息，那么要怎么才能实现网上商店中的购物车呢，session就是一种保存上下文信息的机制，它是针对每一个用户的，变量的值保存在服务器端，通过SessionID来区分不同的客户,session是以cookie或URL重写为基础的，默认使用cookie来实现，系统会创造一个名为JSESSIONID的输出cookie，我们叫做session cookie,以区别persistent cookies,也就是我们通常所说的cookie,注意session cookie是存储于浏览器内存中的，并不是写到硬盘上的，这也就是我们刚才看到的JSESSIONID，我们通常情是看不到JSESSIONID的，但是当我们把浏览器的cookie禁止后，web服务器会采用URL重写的方式传递Sessionid，我们就可以在地址栏看到 sessionid=KWJHUG6JJM65HS2K6之类的字符串。
   ​       明白了原理，我们就可以很容易的分辨出persistent cookies和session cookie的区别了，网上那些关于两者安全性的讨论也就一目了然了，session cookie针对某一次会话而言，会话结束session cookie也就随着消失了，而persistent cookie只是存在于客户端硬盘上的一段文本（通常是加密的），而且可能会遭到cookie欺骗以及针对cookie的跨站脚本攻击，自然不如 session cookie安全了。
   ​       通常session cookie是不能跨窗口使用的，当你新开了一个浏览器窗口进入相同页面时，系统会赋予你一个新的sessionid，这样我们信息共享的目的就达不到了，此时我们可以先把sessionid保存在persistent cookie中，然后在新窗口中读出来，就可以得到上一个窗口SessionID了，这样通过session cookie和persistent cookie的结合我们就实现了跨窗口的session tracking（会话跟踪）。
   ​      在一些web开发的书中，往往只是简单的把Session和cookie作为两种并列的http传送信息的方式，session cookies位于服务器端，persistent cookie位于客户端，可是session又是以cookie为基础的，明白的两者之间的联系和区别，我们就不难选择合适的技术来开发web service了。
一、cookie机制和session机制的区别
　　具体来说cookie机制采用的是在客户端保持状态的方案，而session机制采用的是在服务器端保持状态的方案。
　　同时我们也看到，由于在服务器端保持状态的方案在客户端也需要保存一个标识，所以session机制可能需要借助于cookie机制来达到保存标识的目的，但实际上还有其他选择。
二、会话cookie和持久cookie的区别
　　如果不设置过期时间，则表示这个cookie生命周期为浏览器会话期间，只要关闭浏览器窗口，cookie就消失了。这种生命期为浏览会话期的cookie被称为会话cookie。会话cookie一般不保存在硬盘上而是保存在内存里。
　　如果设置了过期时间，浏览器就会把cookie保存到硬盘上，关闭后再次打开浏览器，这些cookie依然有效直到超过设定的过期时间。
　　存储在硬盘上的cookie可以在不同的浏览器进程间共享，比如两个IE窗口。而对于保存在内存的cookie，不同的浏览器有不同的处理方式。
三、如何利用实现自动登录
　　当用户在某个网站注册后，就会收到一个惟一用户ID的cookie。客户后来重新连接时，这个用户ID会自动返回，服务器对它进行检查，确定它是否为注册用户且选择了自动登录，从而使用户无需给出明确的用户名和密码，就可以访问服务器上的资源。
四、如何根据用户的爱好定制站点
　　网站可以使用cookie记录用户的意愿。对于简单的设置，网站可以直接将页面的设置存储在cookie中完成定制。然而对于更复杂的定制，网站只需仅将一个惟一的标识符发送给用户，由服务器端的数据库存储每个标识符对应的页面设置。
五、cookie的发送
1.创建Cookie对象
2.设置最大时效
3.将Cookie放入到HTTP响应报头
　　如果你创建了一个cookie，并将他发送到浏览器，默认情况下它是一个会话级别的cookie:存储在浏览器的内存中，用户退出浏览器之后被删除。如果你希望浏览器将该cookie存储在磁盘上，则需要使用maxAge，并给出一个以秒为单位的时间。将最大时效设为0则是命令浏览器删除该 cookie。
　　发送cookie需要使用HttpServletResponse的addCookie方法，将cookie插入到一个 Set-Cookie HTTP请求报头中。由于这个方法并不修改任何之前指定的Set-Cookie报头，而是创建新的报头，因此我们将这个方法称为是addCookie，而非setCookie。同样要记住响应报头必须在任何文档内容发送到客户端之前设置。
六、cookie的读取
1.调用request.getCookie
　　要获取有浏览器发送来的cookie，需要调用HttpServletRequest的getCookies方法，这个调用返回Cookie对象的数组，对应由HTTP请求中Cookie报头输入的值。
2.对数组进行循环，调用每个cookie的getName方法，直到找到感兴趣的cookie为止
　　cookie与你的主机(域)相关，而非你的servlet或JSP页面。因而，尽管你的servlet可能只发送了单个cookie，你也可能会得到许多不相关的cookie。
 例如：
```java
　　String cookieName = “userID”;
  Cookie cookies［］ = request.getCookies();
  if (cookies!=null){
  	for(int i=0;i
   	Cookie cookie = cookies［i］;
  if (cookieName.equals(cookie.getName())){
  			doSomethingWith(cookie.getValue());
  		}
 	 }
  }
```
七、如何使用cookie检测初访者
A.调用HttpServletRequest.getCookies()获取Cookie数组
B.在循环中检索指定名字的cookie是否存在以及对应的值是否正确
C.如果是则退出循环并设置区别标识
D.根据区别标识判断用户是否为初访者从而进行不同的操作
八、使用cookie检测初访者的常见错误
　　不能仅仅因为cookie数组中不存在在特定的数据项就认为用户是个初访者。如果cookie数组为null，客户可能是一个初访者，也可能是由于用户将cookie删除或禁用造成的结果。
　　但是，如果数组非null,也不过是显示客户曾经到过你的网站或域，并不能说明他们曾经访问过你的servlet。其它servlet、JSP页面以及非Java Web应用都可以设置cookie，依据路径的设置，其中的任何cookie都有可能返回给用户的浏览器。
　　正确的做法是判断cookie数组是否为空且是否存在指定的Cookie对象且值正确。
九、使用cookie属性的注意问题
　　属性是从服务器发送到浏览器的报头的一部分；但它们不属于由浏览器返回给服务器的报头。　
　　因此除了名称和值之外，cookie属性只适用于从服务器输出到客户端的cookie；服务器端来自于浏览器的cookie并没有设置这些属性。　
　　因而不要期望通过request.getCookies得到的cookie中可以使用这个属性。这意味着，你不能仅仅通过设置cookie的最大时效，发出它，在随后的输入数组中查找适当的cookie,读取它的值，修改它并将它存回Cookie，从而实现不断改变的cookie值。
十、如何使用cookie记录各个用户的访问计数
1.获取cookie数组中专门用于统计用户访问次数的cookie的值
2.将值转换成int型
3.将值加1并用原来的名称重新创建一个Cookie对象
4.重新设置最大时效
5.将新的cookie输出
十一、session在不同环境下的不同含义
　　session，中文经常翻译为会话，其本来的含义是指有始有终的一系列动作/消息，比如打电话是从拿起电话拨号到挂断电话这中间的一系列过程可以称之为一个session。
　　然而当session一词与网络协议相关联时，它又往往隐含了“面向连接”和/或“保持状态”这样两个含义。
　　session在Web开发环境下的语义又有了新的扩展，它的含义是指一类用来在客户端与服务器端之间保持状态的解决方案。有时候Session也用来指这种解决方案的存储结构。
十二、session的机制
　　session机制是一种服务器端的机制，服务器使用一种类似于散列表的结构(也可能就是使用散列表)来保存信息。
　　但程序需要为某个客户端的请求创建一个session的时候，服务器首先检查这个客户端的请求里是否包含了一个session标识－称为session id,如果已经包含一个session id则说明以前已经为此客户创建过session，服务器就按照session id把这个session检索出来使用(如果检索不到，可能会新建一个，这种情况可能出现在服务端已经删除了该用户对应的session对象，但用户人为地在请求的URL后面附加上一个JSESSION的参数)。
　　如果客户请求不包含session id，则为此客户创建一个session并且生成一个与此session相关联的session id，这个session id将在本次响应中返回给客户端保存。
十三、保存session id的几种方式
A．保存session id的方式可以采用cookie，这样在交互过程中浏览器可以自动的按照规则把这个标识发送给服务器。
B．由于cookie可以被人为的禁止，必须有其它的机制以便在cookie被禁止时仍然能够把session id传递回服务器，经常采用的一种技术叫做URL重写，就是把session id附加在URL路径的后面，附加的方式也有两种，一种是作为URL路径的附加信息，另一种是作为查询字符串附加在URL后面。网络在整个交互过程中始终保持状态，就必须在每个客户端可能请求的路径后面都包含这个session id。
C．另一种技术叫做表单隐藏字段。就是服务器会自动修改表单，添加一个隐藏字段，以便在表单提交时能够把session id传递回服务器。
十四、session什么时候被创建
　　一个常见的错误是以为session在有客户端访问时就被创建，然而事实是直到某server端程序(如Servlet)调用HttpServletRequest.getSession(true)这样的语句时才会被创建。
十五、session何时被删除
session在下列情况下被删除：
A．程序调用HttpSession.invalidate()
B．距离上一次收到客户端发送的session id时间间隔超过了session的最大有效时间
C．服务器进程被停止
　　再次注意关闭浏览器只会使存储在客户端浏览器内存中的session cookie失效，不会使服务器端的session对象失效。

###5. 分布式事务

#### 1. 基于XA协议的两阶段提交方案

 交易中间件与数据库通过 XA 接口规范，使用两阶段提交来完成一个全局事务， XA 规范的基础是两阶段提交协议。
 第一阶段是表决阶段，所有参与者都将本事务能否成功的信息反馈发给协调者；第二阶段是执行阶段，协调者根据所有参与者的反馈，通知所有参与者，步调一致地在所有分支上提交或者回滚。

####  2. TCC方案

 [TCC方案](https://wenku.baidu.com/view/be946bec0975f46527d3e104.html)在电商、金融领域落地较多。TCC方案其实是两阶段提交的一种改进。其将整个业务逻辑的每个分支显式的分成了Try、Confirm、Cancel三个操作。Try部分完成业务的准备工作，confirm部分完成业务的提交，cancel部分完成事务的回滚。基本原理如下图所示。

 事务开始时，业务应用会向事务协调器注册启动事务。之后业务应用会调用所有服务的try接口，完成一阶段准备。之后事务协调器会根据try接口返回情况，决定调用confirm接口或者cancel接口。如果接口调用失败，会进行重试。

 TCC方案让应用自己定义数据库操作的粒度，使得降低锁冲突、提高吞吐量成为可能。 当然TCC方案也有不足之处，集中表现在以下两个方面：

 - **对应用的侵入性强**。业务逻辑的每个分支都需要实现try、confirm、cancel三个操作，应用侵入性较强，改造成本高。
 - **实现难度较大**。需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。为了满足一致性的要求，confirm和cancel接口必须实现幂等。

 上述原因导致TCC方案大多被研发实力较强、有迫切需求的大公司所采用。微服务倡导服务的轻量化、易部署，而TCC方案中很多事务的处理逻辑需要应用自己编码实现，复杂且开发量大

#### 3. 基于消息的最终一致性方案

 消息一致性方案是通过消息中间件保证上、下游应用数据操作的[一致性](https://segmentfault.com/a/1190000011479826)。基本思路是将本地操作和发送消息放在一个事务中，保证本地操作和消息发送要么两者都成功或者都失败。下游应用向消息系统订阅该消息，收到消息后执行相应操作。

 消息方案从本质上讲是将分布式事务转换为两个本地事务，然后依靠下游业务的重试机制达到最终一致性。基于消息的最终一致性方案对应用侵入性也很高，应用需要进行大量业务改造，成本较高。

#### 4. GTS--分布式事务解决方案

 [GTS是一款分布式事务中间件](https://www.aliyun.com/aliware/txc?spm=5176.8142029.388261.386.a72376f4lqvQxv)，由阿里巴巴中间件部门研发，可以为微服务架构中的分布式事务提供一站式解决方案。

### 6. NIO

`Java NIO`的核心组件 包括：

- 通道（`Channel`）
- 缓冲区（`Buffer`）
- 选择器（`Selectors`）

数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中。Selector(选择区)用于监听多个通道的事件（比如：连接打开，数据到达）。因此，单个线程可以监听多个数据通道。

NIO和传统IO（一下简称IO）之间第一个最大的区别是，IO是面向流的，NIO是面向缓冲区的。

IO的各种流是阻塞的。这意味着，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了。 NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取。而不是保持线程阻塞，所以直至数据变得可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 线程通常将非阻塞IO的空闲时间用于在其它通道上执行IO操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）

#### Channel

首先说一下Channel，国内大多翻译成“通道”。Channel和IO中的Stream(流)是差不多一个等级的。只不过Stream是单向的，譬如：InputStream, OutputStream.而Channel是双向的，既可以用来进行读操作，又可以用来进行写操作。
NIO中的Channel的主要实现有：

- FileChannel
- DatagramChannel
- SocketChannel
- ServerSocketChannel

这里看名字就可以猜出个所以然来：分别可以对应文件IO、UDP和TCP（Server和Client）。下面演示的案例基本上就是围绕这4个类型的Channel进行陈述的。

#### Buffer

NIO中的关键Buffer实现有：ByteBuffer, CharBuffer, DoubleBuffer, FloatBuffer, IntBuffer, LongBuffer, ShortBuffer，分别对应基本数据类型: byte, char, double, float, int, long, short。当然NIO中还有MappedByteBuffer, HeapByteBuffer, DirectByteBuffer等这里先不进行陈述

#### Selector

Selector运行单线程处理多个Channel，如果你的应用打开了多个通道，但每个连接的流量都很低，使用Selector就会很方便。例如在一个聊天服务器中。要使用Selector, 得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子有如新的连接进来、数据接收等

### 7. 抽象类

抽象类必须用 `abstract` 修饰，子类必须实现抽象类中的抽象方法，如果有未实现的，那么子类也必须用 abstract 修饰。抽象类默认的权限修饰符为 `public`，可以定义为 public 或 procted，如果定义为 private，那么子类则无法继承。抽象类不能创建对象

#### 抽象类和普通类的区别

1. 抽象类必须用public、procted 修饰(如果为private修饰，那么子类则无法继承，也就无法实现其抽象方法）。默认缺省为 public
2. 抽象类无法创建对象
3. 如果一个子类继承抽象类，那么必须实现其所有的抽象方法。如果有未实现的抽象方法，那么必须定义为 abstract

#### 接口

接口中的变量隐式的使用 `public static final` 修饰，并且需要给出初始值。方法隐式的使用 `public abstract` 修饰(并且只能是 public ，如果是 private，procted，那么就编译报错)。

接口中的方法默认不能有具体的实现（JDK1.8开始可以有默认的实现）

**JDK8对接口的增强（默认接口方法和静态接口方法**） 1、允许接口使用 `default` 关键字 2、允许接口定义静态方法

```java
public interface Person {
	 default String isLive() {
		return "I am happy to live";
	 }
}
public interface Person {
	 public static void doSomething(String something) {
		System.out.println("I am Do :" + something);
	 }
}
```



#### 接口和抽象类的区别

1. 抽象类只能继承一次，但是可以实现多个接口
2. 接口和抽象类必须实现其中所有的方法，抽象类中如果有未实现的抽象方法，那么子类也需要定义为抽象类。抽象类中可以有非抽象的方法
3. 接口中的变量必须用 public static final 修饰，并且需要给出初始值。所以实现类不能重新定义，也不能改变其值。
4. 接口中的方法默认是 public abstract，也只能是这个类型。不能是 static，接口中的方法也不允许子类覆写，抽象类中允许有static 的方法

### 8. proxy和CGLib

一、原理区别：

java动态代理是利用反射机制生成一个实现代理接口的匿名类，在调用具体方法前调用InvokeHandler来处理。

而cglib动态代理是利用asm开源包，对代理对象类的class文件加载进来，通过修改其字节码生成子类来处理。

1、如果目标对象实现了接口，默认情况下会采用JDK的动态代理实现AOP 
2、如果目标对象实现了接口，可以强制使用CGLIB实现AOP 

3、如果目标对象没有实现了接口，必须采用CGLIB库，spring会自动在JDK动态代理和CGLIB之间转换

如何强制使用CGLIB实现AOP？
 （1）添加CGLIB库，SPRING_HOME/cglib/*.jar
 （2）在spring配置文件中加入<aop:aspectj-autoproxy proxy-target-class="true"/>

JDK动态代理和CGLIB字节码生成的区别？
 （1）JDK动态代理只能对实现了接口的类生成代理，而不能针对类
 （2）CGLIB是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法
  因为是继承，所以该类或方法最好不要声明成final 

#### CGlib比JDK快？ 

CGlib代理是通过FashClass机制直接调用方法，jdk6之前比使用Java反射效率要高

> 1）使用CGLib实现动态代理，CGLib底层采用ASM字节码生成框架，使用字节码技术生成代理类，在jdk6之前比使用Java反射效率要高。唯一需要注意的是，CGLib不能对声明为final的方法进行代理，因为CGLib原理是动态生成被代理类的子类。
>
> 2）在jdk6、jdk7、jdk8逐步对JDK动态代理优化之后，在调用次数较少的情况下，JDK代理效率高于CGLIB代理效率，只有当进行大量调用的时候，jdk6和jdk7比CGLIB代理效率低一点，但是到jdk8的时候，jdk代理效率高于CGLIB代理，总之，每一次jdk[版本升级](https://www.baidu.com/s?wd=版本升级&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)，jdk代理效率都得到提升，而CGLIB代理消息确有点跟不上步伐

DK代理是不需要第三方库支持，只需要JDK环境就可以进行代理，使用条件:

> 1）实现InvocationHandler
>
> 2）使用Proxy.newProxyInstance产生代理对象
>
> 3）被代理的对象必须要实现接口
>
> CGLib必须依赖于CGLib的类库，但是它需要类来实现任何接口代理的是指定的类生成一个子类，覆盖其中的方法，是一种继承但是针对接口编程的环境下推荐使用JDK的代理

### 9. 序列化

Java序列化：

　　**Java序列化会把要序列化的对象类的元数据和业务数据全部序列化为字节流**，而且是把整个继承关系上的东西全部序列化了。**它序列化出来的字节流是对那个对象结构到内容的完全描述**，包含所有的信息，因此**效率较低而且字节流比较大**。但是由于确实是序列化了所有内容，所以可以说**什么都可以传输，因此也更可用和可靠**。


hession序列化：

　　**它的实现机制是着重于数据**，**附带简单的类型信息的方法。**就像Integer a = 1，hessian会序列化成I 1这样的流，I表示int or Integer，1就是数据内容。而对于复杂对象，通过Java的反射机制，**hessian把对象所有的属性当成一个Map来序列化**，产生类似M className propertyName1 I 1 propertyName S stringValue（大概如此，确切的忘了）这样的流，**包含了基本的类型描述和数据内容**。而在序列化过程中，如果一个对象之前出现过，hessian会直接插入一个R index这样的块来表示一个引用位置，从而省去再次序列化和反序列化的时间。这样做的代价就是hessian需要对不同的类型进行不同的处理（因此hessian直接偷懒不支持short），而且遇到某些特殊对象还要做特殊的处理（比如StackTraceElement）。**而且同时因为并没有深入到实现内部去进行序列化，所以在某些场合会发生一定的不一致**，**比如通过Collections.synchronizedMap得到的map。**

获取当前class的所有字段，接着获取父类的所有字段。序列化的时候，所有字段都放在一个ArrayList里，然后依次写入到二进制流中，反序列化的时候，所有字段放在了一个HashMap里，HashMap的key不能重复，悲剧就出现了，如果子类和父类有同名的字段就会有问题，父类的值会把子类的值覆盖掉。

看看反序列化时，JavaDeserializer的getFieldMap方法，父类字段会把子类字段覆盖掉。

所以对于同名字段，子类的该字段值会被赋值两次，总是被父类的值覆盖，导致子类的字段值丢失。分析完了，再回过头来看看，ProductDraftDO有一个ActionTrace类型字段actionTrace,父类ProductDO也有一个ActionTrace类型字段actionTrace。由于反序列化时，ProductDraftDO的actionTrace一直被ProductDO的actionTrace覆盖，所以ProductDraftDO的actionTrace总是空的。至于为什么父类和子类都有一个ActionTrace，这是历史原因，这里不讨论。这个问题最初由海滔发现，我对代码做了一个完整的分析。

**所以，使用****hessian****序列化时，一定要注意子类和父类不能有同名字段。在使用****dubbo****时如果没有指定序列化协议，则也要注意这个问题**



### 9. Java异常Error和Exception的区别

要理解Java异常处理是如何工作的，你需要掌握以下三种类型的异常：

检查性异常：最具代表的检查性异常是用户错误或问题引起的异常，这是程序员无法预见的。例如要打开一个不存在文件时，一个异常就发生了，这些异常在编译时不能被简单地忽略。
运行时异常： 运行时异常是可能被程序员避免的异常。与检查性异常相反，运行时异常可以在编译时被忽略。
错误： 错误不是异常，而是脱离程序员控制的问题。错误在代码中通常被忽略。例如，当栈溢出时，一个错误就发生了，它们在编译也检查不到的。 
从图中可以看出所有异常类型都是内置类Throwable的子类，因而Throwable在异常类的层次结构的顶层

## 2. JVM

### jvm运行时数据区 五部分
1. 方法区(method area 英 ['meθəd] ['eərɪə])
2. 堆(heap 英 [hiːp])
3. 虚拟机栈(VM Stack)
4. 本地方法栈 (native method stack)
5. 程序计数器 (program counter register)

  新生代和老年代 永久代(jdk1.8 medaSpace)

#### 程序计数器 ： 当前线程所执行的字节码的行号指示器。
#### 虚拟机栈 ： 
每个方法执行的时候都会创建栈帧 用于 存储 局部变量表、操作数栈、动态链接，方法出口等信息。
局部变量表 存放编译期可知的各种基本数据类型（boolean,byte,char,short,int,float,long,double）,对象引用(reference)和returnAddress类型。
#### 本地方法栈
虚拟机使用到的Native方法服务。

#### 堆
存放对象实例  新生代和老年代  Eden,From survivor ,To survivor,Old 分配缓冲区 Thread Local Allocation Buffer, TLAB
#### 方法区
存储虚拟机加载的 类信息(class)、常量 (final)、静态变量 (static)、即时编译器编译后的代码等数据，也称 永久代 parmanent generation


| 名称       | 特征                                                     | 作用                                                         | 配置参数                            | 异常                                |
| ---------- | -------------------------------------------------------- | ------------------------------------------------------------ | ----------------------------------- | ----------------------------------- |
| 程序计数器 | 占用内存小，线程私有，生命周期与线程相同                 | 大致为字节码行号指示器                                       | 无                                  | 无                                  |
| 虚拟机栈   | 线程私有，生命周期与线程相同，使用连续的内存空间         | Java方法执行的内存模型，存储局部变量表、操作栈、动态链接、方法出口等信息 | -Xss                                | StackOverflowError,OutOfMemoryError |
| java堆     | 线程共享，生命周期与虚拟机相同，可以不使用连续的内存地址 | 保存对象实例，所有对象实例（包括数组）都要在堆上分配         | -Xms,-Xsx,-Xmn                      | OutOfMemoryError                    |
| 方法区     | 线程共享，生命周期与虚拟机相同，可以不使用连续的内存地址 | 存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据 | -XX:PermSize:16M -XX:MaxPermSize64M | OutOfMemoryError                    |


## 对象创建 

内存的分配算法：

分配内存 仅仅是 指针向空闲空间那边挪动一段与对象大小相等的距离 这种分配方式  指针碰撞  bump the pointer

维护一个列表，分配的时候从列表中找到一块足够打的空间划给对象实例   空闲列表  free list

serial, ParNew等带Compact过程收集器的，采用分配算法是 指针碰撞

CMS这种基于Mark-sweep算法的收集器 采用 空闲列表

分配内存并发问题：

1. 对分配内存空间的懂的工作进行同步处理-实际上虚拟机采用CAS配上失败重试的方法保证更新操作的原子性
2. 把内存分配的动作按照线程划分不同的空间之中进行 即 每个线程在java堆中预先分配一小块内存 称 本地线程分配缓冲(Thread Local Allocation Buffer, TLAB)

### 对象的内存布局 对象头， 实例数据和对齐填充

对象头 :
1. 存储对象自身运行时的数据包含(类的元数据信息，对象的哈希码，对象的GC分代年龄,锁状态标志，线程持有的锁，等信息)
2. 类型指针 即对象指向它的类元数据的指针，虚拟机通过这个指针确定这个对象是哪个类的实例。

### 对象的访问定位
1. 句柄访问  
2. 直接指针访问  速度快  


## 垃圾收集器和内存分配策略

1. 引用计数算法
2. 可达性分析算法 GCRoots      GCRoots 不可达 可判定对象回收 
3. 可作为GCRoots的对象
    1. 虚拟机栈（栈中的本地变量表）中引用的对象；
    2. 方法区中类静态属性引用的对象；
    3. 方法区中常量引用的对象；
    4. 本地方法栈中JNI（即一般说的Native方法）引用的对象。

引用 强引用 软引用 ，弱引用，虚引用

### 垃圾收集算法       策略就是算法    分配策略 就是  收集算法
标记-清除  mark-sweep
复制算法 copying
标记-整理  mark-compact
分代收集算法  新生代 复制算法， 老年代 标记整理算法。

### 垃圾收集器
1. serial  单线程 client模式下新生代收集器  垃圾回收时暂停其他所有工作线程直到结束 复制算法

2. parNew 实际上是serial多线程版本  并发  server模式下新生代收集器   和GMS 配合工作。复制算法

3. parallel scavenge  ['pærəlel]  ['skævɪn(d)ʒ],  新生代 并行    达到一个可控制的吞吐量 可以设置停顿时间和吞吐量大小  复制算法

4. serial old 单线程 client 老年代   垃圾回收时暂停其他所有工作线程直到结束 标记整理算法

5. parallel old 多线程     老年代 吞吐量    标记整理算法 

6. GMS(concurrent mark sweep) 最短回收停顿时间   老年代    标记清除。并发收集，低停顿， 在并发阶段 虽然不会导致用户线程停顿，但是会占用一部分线程，导致应用程序变慢，吞吐量下降。用户线程没有停顿概念
   1. CMS 对CPU资源面感
   
   2. 标记清楚 大量空间碎片
   
   3. 用户线程和收集线程并行， 产生浮动垃圾
   
      1. 初始标记。 标记老年代中所有的**GC Roots**引用的对象；标记老年代中被**年轻代中活着的对象**引用的对象(初始标记也会扫描新生代)；会导致stw 
   
      2. 并发标记   用户线程和GC 并行 从初次标记收集到的‘根’对象引用开始，遍历所有能被引用的对象。
   
      3. **并发可中断预清理(Concurrent precleaning)**：改变当运行第二阶段时，由应用程序线程产生的对象引用，以更新第二阶段的结果。标记在并发标记阶段引用发生变化的对象，如果发现对象的引用发生变化，则JVM会标记堆的这个区域为**Dirty Card**。那些能够从Dirty Card到达的对象也被标记（标记为存活），当标记做完后，这个Dirty Card区域就会消失。
   
      4. 重新标记。 ：由于并发预处理是并发的，对象引用可能发生进一步变化。因此，应用程序线程会再一次被暂停（stw）以更新这些变化，并且在进行实际的清理之前确保一个正确的对象引用视图。这一阶段十分重要，因为必须避免收集到仍被引用的对象。STW
   
      5. 并发清理。 清理垃圾对象，这个阶段收集器线程和应用程序线程并发执行。
   
      6. 并发重置(Concurrent reset)：CMS清除内部状态，为下次回收做准备。
   
         综上所述， 一次GC。停顿2次
   
      #### 1. 并发预处理阶段意义何在？
   
      并发预处理阶段做的工作还是标记，与4的重标记功能相似。既然相似为什么要有这一步？
   
      前面我们讲过，CMS是以获取最短停顿时间为目的的GC。重标记需要STW（Stop The World），因此重标记的工作尽可能多的在并发阶段完成来减少STW的时间。
   
      此阶段标记从**新生代晋升的对象**、**新分配到老年代的对象**以及在**并发阶段被修改了的对象**。
   
      #### 2. 如何确定老年代的对象是活着的？
   
      答案很简单，通过GC ROOT TRACING可到达的对象就是活着的。老年代进行GC时如何确保上图中Current Obj标记为活着的？答案是必须扫描新生代来确保。这也是为什么CMS虽然是老年代的gc，但仍要扫描新生代的原因。
   
      全量的扫描新生代和老年代会不会很慢？肯定会。CMS号称是停顿时间最短的GC，如此长的停顿时间肯定是不能接受的。如何解决呢？那就是**必须要有一个能够快速识别新生代和老年代活着的对象的机制**。
   
      新生代垃圾回收完剩下的对象全是活着的，并且活着的对象很少。如果能在**并发可中断预清理阶段**发生一次Minor GC，那STW remark的时间就会缩短很多。
   
      CMS 有两个参数：**CMSScheduleRemarkEdenSizeThreshold**、**CMSScheduleRemarkEdenPenetration**，默认值分别是2M、50%。
   
      - -XX:CMSScheduleRemarkEdenSizeThreshold（默认2m）：控制abortable-preclean阶段什么时候开始执行，即当eden使用达到此值时，才会开始abortable-preclean阶段。
      - -XX:CMSScheduleRemarkEdenPenetratio（默认50%）：控制abortable-preclean阶段什么时候结束执行。
   
      所以两个参数组合起来的意思是eden空间使用超过2M时启动可中断的并发预清理，直到eden空间使用率达到50%时中断，进入remark阶段。
   
      那可终止的预清理要执行多长时间来保证发生一次Minor GC呢？答案是没法保证。道理很简单，因为垃圾回收是JVM自动调度的，什么时候进行GC我们控制不了。
   
      但此阶段总有一个执行时间吧。CMS提供了一个参数CMSMaxAbortablePrecleanTime ，默认为5S。只要到了5S，不管发没发生Minor GC，有没有到CMSScheduleRemardEdenPenetration都会中止此阶段，进入remark。
   
      如果在5S内还是没有执行Minor GC怎么办？CMS提供CMSScavengeBeforeRemark参数，使remark前强制进行一次Minor GC。
   
      这样做利弊都有：
   
      - 好的一面是减少了remark阶段的停顿时间；
      - 坏的一面是Minor GC后紧跟着一个remark pause。如此一来，停顿时间也比较久
   
      #### 3. 进行Minor GC时如果有老年代引用新生代，怎么识别？
   
      有研究表明，在所有的引用中，老年代引用新生代这种场景不足1%。
   
      CMS将老年代的空间分成大小为512bytes的块，card table中的每个元素对应着一个块。
   
      **并发标记**时，如果某个对象的引用发生了变化，就标记该对象所在的块为 dirty card。**并发预清理**阶段就会重新扫描该块，将该对象引用的对象标识为可达。
   
      当有老年代引用新生代，对应的card table被标识为相应的值（card table中是一个byte，有八位，约定好每一位的含义就可区分哪个是引用新生代，哪个是并发标记阶段修改过的。**所以，Minor GC通过扫描card table就可以很快的识别老年代引用新生代。**
   
   #### CMS垃圾回收特点
   
   - cms只会回收老年代和永久带（1.8开始为元数据区，需要设置CMSClassUnloadingEnabled），不会收集年轻带；
   - cms是一种预处理垃圾回收器，它不能等到old内存用尽时回收，需要在内存用尽前，完成回收操作，否则会导致并发回收失败；所以cms垃圾回收器开始执行回收操作，有一个触发阈值，默认是老年代或永久带达到92%；
   
   
   
7. G1  不在物理隔离 老年代和新生代。并行与并发， 分代收集 空间整合。可预测停顿。   java堆内存分为多个大小相等的独立区域Region  ， 保留了新生代和老年代的概念，但不再物理隔离，都是一部分Region（不需要连续的集合）

serial   + serial 0ld。新生代 复制算法， 老年代 标记整理 算法。 暂停所有用户线程
parNew + serial 0ld  新生代 复制算法， 老年代 标记整理 算法。 暂停所有用户线程  用户线程没有停顿概念
可搭配 的收集器
serial  + cms 、serial 0ld
parNew + cms 、serial 0ld
parallel scavenge + serial 0ld、parallel old 
cms + serial 0ld 

对象优先在Eden分配
大对象直接进入老年代
长期存过的对象将进入老年代
动态对象年龄判定
空间分配担保

- 串行回收，Serial回收器，单线程回收，全程stw；
- 并行回收，名称以Parallel开头的回收器，多线程回收，全程stw；
- 并发回收，cms与G1，多线程分阶段回收，只有某阶段会stw

## 工具
- jps: 虚拟机进程状况工具   类似 nuix ps 
- jstat: 虚拟机统计信息监视工具       一般看这个 
- jinfo: java配置信息工具
- jmap: java内存映像工具  生成dump快照
- jhat:虚拟机对转储快照分析工具
- jstack: java堆栈跟踪工具 当前线程  处理死锁 

可视化工具
jconsole
visualVM

## 类加载机制
1. 类加载  周期： 加载，链接(验证，准备，解析)，初始化，使用，卸载

加载： 1，通过一个类的全限定名获取定义此类的二进制字节流 2. 将这个字节流锁代表的静态存储结构转化为方法区的 运行时数据结构， 3 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口

验证 ：确保Class文件字节流符合虚拟机要求，并且不会危害虚拟机自身， 1， 文件验证， 魔术开都， 主次版本号 等等 2 元数据验证  3. 字节码验证 4 符号引用验证 

准备：类变量分配内存 设置初始值   这里的变量只是静态变量 不包含实例 

解析：虚拟机将常量池内的符号引用替换为直接引用的过程。  类接口解析，字段解析  类方法解析 接口犯方法解析

 初始化：前面都是虚拟机主导控制， 到这个阶段，才是执行类中定义的java程序代码。执行类构造器<clinit>() 方法的过程，

1、<clinit>()方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块（static{}语句）中的语句合并产生的，编译器收集的顺序是有语句在源文件中出现的顺序所决定的，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块中可以赋值，但不能访问。

2、<clinit>()方法与类的构造函数（实例化<init>()方法）不同，他不需要显示地调用父类构造器，虚拟机会保证在子类的<clinit>()方法执行之前，父类的<clinit>()方法执行完毕。虚拟机第一个被执行的<clinit>()方法肯定是java.lang.Object()。

3、由于父类的<clinit>()方法先执行，所以父类定义的静态语句块要优先于子类的变量赋值操作。

4、如果一个类中没有静态语句块，也没有对变量的赋值操作，编译器不必为类生成<clinit>()方法。

5、接口不能使用静态语句块，但仍有变量初始化的赋值操作，会生成<clinit>()方法。接口的<clinit>()方法不需要先执行父类接口的<clinit>()方法，当父类接口中定义的变量被使用时，父接口才会被初始化。接口的实现类在初始化时不会执行接口的<clinit>()方法。

6、一个类的<clinit>()方法在多线程环境中执行时，只会有一个线程去执行<clinit>()方法，其余线程都需要阻塞等待，直到活动线程执行<clinit>()方法完毕。

这里 要把。类初始化  和 类实例化。取分开

### 初始化时机：
1. new，get static，put static，或者invoke static  new实例化对象，读取或者设置一个类的静态变量，调用一个类的静态方法
2. reflect 包的方法进行类反射调用
3. 初始化一个类，先出发其父类的初始化
4. 虚拟机启动时，指定的一个执行主类，虚拟机会先初始化这个主类
5. jdk1.7动态语言支持，MethodHandle实例最后解析结果  REF_getStatic,REF_putStatic,REF_invokeStatic的方法句柄，触发类的初始化

### 类加载器

1. 启动类加载器，C++实现的
2. 其他类加载器 java实现继承ClassLoader

顺序
1. 启动类加载器 BootStrap ClassLoader   加载java_home\lib
2. 扩展类加载器 Extension ClassLoader   加载java_home\lib\ext
3. 应用程序类加载器 Application ClassLoader  系统类加载器  默认的类加载器

双亲委派模型
除顶层的启动类加载器外，其余的类加载器都应该有自己的父类加载器。

工作过程：如果一个类加载器收到类加载请求，它首先不自己加载，而是把请求委派给父类加载器去加载，只有当父类无法完成，子加载器才会尝试自己加载。

Java类随着它的类加载器一起具备了一种带有优先级的层次关系。 确定了一个类在各种类加载器环境中都是同一个类

###  java 内存模型

   java内存模型(即Java Memory Model，简称JMM)本身是一种抽象的概念，并不真实存在，它描述的是一组规则或规范，通过这组规范定义了程序中各个变量（包括实例字段，静态字段和构成数组对象的元素）的访问方式。

- 主内存

  主要存储的是Java实例对象，所有线程创建的实例对象都存放在主内存中，不管该**实例对象是成员变量还是方法中的本地变量(也称局部变量)**，当然也包括了共享的类信息、常量、静态变量。由于是共享数据区域，多条线程对同一个变量进行访问可能会发现线程安全问题。

- 工作内存

  主要存储当前方法的所有本地变量信息(工作内存中存储着主内存中的变量副本拷贝)，每个线程只能访问自己的工作内存，即线程中的本地变量对其它线程是不可见的，就算是两个线程执行的是同一段代码，它们也会各自在自己的工作内存中创建属于当前线程的本地变量，当然也包括了字节码行号指示器、相关Native方法的信息。注意由于工作内存是每个线程的私有数据，线程间无法相互访问工作内存，因此存储在工作内存的数据不存在线程安全问题。

## 3. JAVA集合
集合类存放于 Java.util 包中，主要有 3 种:set(集)、list(列表包含 Queue)和 map(映射)。 
1. Collection:Collection 是集合 List、Set、Queue 的最基本的接口。 
2. Iterator:迭代器，可以通过迭代器遍历集合中的数据
3. Map:是映射表的基础接口 

#### 1. List 是有序的 Collection。
Java List 一共三个实现类: 分别是 ArrayList、Vector 和 LinkedList。 
**ArrayList**(数组) 它适合随机查找和遍历，不适合插入和删除。 
**Vector**(数组实现、线程同步)
LinkedList 是用链表结构存储数据的，很适合数据的动态插入和删除，随机访问和遍历速度比较 慢。另外，他还提供了 List 接口中没有定义的方法，专门用于操作表头和表尾元素，可以当作堆 栈、队列和双向队列使用。 

- 基于Array的List（Vector，ArrayList）适合查询，而LinkedList（链表）适合添加，删除操作;

#### 2. Set
元素无放入顺序，元素不可重复（注意：元素虽然无放入顺序，但是元素在set中的位置是有该元素的HashCode决定的，其位置其实是固定的）
**HashSet**(Hash 表)
**TreeSet**(底层由平衡二叉树实现)    SortedSet接口有一个实现类
**LinkHashSet**(HashSet+LinkedHashMap) 
对于 LinkedHashSet 而言，它继承与 HashSet、又基于 LinkedHashMap 来实现的。 
具有HashSet的查询速度，且内部使用链表维护元素的顺序(插入的次序)。于是在使用迭代器遍历Set时，结果会按元素插入的次序显示。

#### 3. Map  key+value.  以键值对的方式出现的 

Map接口有三个实现类HashMap，HashTable，LinkeHashMap 

 **HashMap**(数组**+**链表**+**红黑树)    高效，支持null；
 **ConcurrentHashMap**  segment+hashEntry  分段锁    jdk8之后 Node cas+ Synchroinzed

  **HashTable**(线程安全)   Synchroinzed. 低效，不支持null 
  **TreeMap**(可排序) 
  TreeMap 实现 SortedMap 接口，能够把它保存的记录根据键排序，默认是按键值的升序排序， 
  也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的。 
  **LinkHashMap**(记录插入顺序) 

 LinkedHashMap像HashMap一样允许null key，内部通过维护一个双向链表，当迭代输出时可以以插入顺序（通常情况下是插入顺序，还可以是访问顺序）输出，因此性能稍微比HashMap低一点。当重新插入一条数据（也就是put一个已经存在的key）时不会影响插入顺序。LinkedHashMap让用户从未指定的、混乱顺序的Map实现HashMap和HashTable中解脱出来。并且与TreeMap相比，没有增加插入代价。


TreeMap是基于红黑树实现的，这个map的key是有序的，两种方式：一种是key实现Comparable接口，一种是在构造函数中传入Comparator。



## 4. JAVA多线程并发
#### 9. JAVA锁

##### 1. 乐观锁
乐观锁是一种乐观思想，即认为读多写少，遇到并发写的可能性低，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，采取在写时先读出当前版本号，然后加锁操作(比较跟上一次的版本号，如果一样则更新)，如果失败则要重复读-比较-写的操作。 
java 中的乐观锁基本都是通过 CAS 操作实现的，CAS 是一种更新的原子操作，比较当前值跟传值是否一样，一样则更新，否则失败。

##### 2. 悲观锁
悲观锁是就是悲观思想，即认为写多，遇到并发写的可能性高，每次去拿数据的时候都认为别人会修改，所以每次在读写数据的时候都会上锁，这样别人想读写这个数据就会 block 直到拿到锁。java 中的悲观锁就是 Synchronized,AQS 框架下的锁则是先尝试 cas 乐观锁去获取锁，获取不到，才会转换为悲观锁，如 RetreenLock。

##### 11. 共享锁和独占锁
java 并发包提供的加锁模式分为独占锁和共享锁。
*独占锁*
独占锁模式下，每次只能有一个线程能持有锁，ReentrantLock 就是以独占方式实现的互斥锁。
独占锁是一种悲观保守的加锁策略，它避免了读/读冲突，如果某个只读线程获取锁，则其他读线
程都只能等待，这种情况下就限制了不必要的并发性，因为读操作并不会影响数据的一致性。
*共享锁*
共享锁则允许多个线程同时获取锁，并发访问共享资源，如:ReadWriteLock。共享锁则是一种乐观锁，它放宽了加锁策略，允许多个执行读操作的线程同时访问共享资源。
AQS 的内部类 Node 定义了两个常量 SHARED 和 EXCLUSIVE，他们分别标识 AQS 队列中等待线程的锁获取模式。
java 的并发包中提供了 ReadWriteLock，读-写锁。它允许一个资源可以被多个读操作访问，
或者被一个 写操作访问，但两者不能同时进行。

##### 12. 重量级锁(Mutex Lock)
Synchronized 是通过对象内部的一个叫做监视器锁(monitor)来实现的。但是监视器锁本质又是依赖于底层的操作系统的 Mutex Lock 来实现的。而操作系统实现线程之间的切换这就需要从用户态转换到核心态，这个成本非常高，状态之间的转换需要相对比较长的时间，这就是为什么Synchronized 效率低的原因。因此，这种依赖于操作系统 Mutex Lock 所实现的锁我们称之为“重量级锁”。JDK 中对 Synchronized 做的种种优化，其核心都是为了减少这种重量级锁的使用。JDK1.6 以后，为了减少获得锁和释放锁所带来的性能消耗，提高性能，引入了“轻量级锁”和“偏向锁”。

##### 13. 轻量级锁
锁的状态总共有四种:无锁状态、偏向锁、轻量级锁和重量级锁。
*锁升级*
> 随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁(但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级)。

“轻量级”是相对于使用操作系统互斥量来实现的传统锁而言的。但是，首先需要强调一点的是，轻量级锁并不是用来代替重量级锁的，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用产生的性能消耗。在解释轻量级锁的执行过程之前，先明白一点，轻量级锁所适应的场景是线程交替执行同步块的情况，如果存在同一时间访问同一锁的情况，就会导致轻量级锁膨胀为重量级锁。

##### 14. 偏向锁
Hotspot 的作者经过以往的研究发现大多数情况下锁不仅不存在多线程竞争，而且总是由同一线程多次获得。偏向锁的目的是在某个线程获得锁之后，消除这个线程锁重入(CAS)的开销，看起来让这个线程得到了偏护。引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次 CAS 原子指令，而偏向锁只需要在置换ThreadID 的时候依赖一次 CAS 原子指令(由于一旦出现多线程竞争的情况就必须撤销偏向锁，所以偏向锁的撤销操作的性能损耗必须小于节省下来的 CAS 原子指令的性能消耗)。上面说过，轻量级锁是为了在线程交替执行同步块时提高性能，而偏向锁则是在只有一个线程执行同步块时进一步提高性能。

##### 15. 分段锁

分段锁也并非一种实际的锁，而是一种思想ConcurrentHashMap是学习分段锁的最好实践。

##### 16. 锁优化

*减少锁持有时间*

只用在有线程安全要求的程序上加锁

*减小锁粒度*

将大对象(这个对象可能会被很多线程访问)，拆成小对象，大大增加并行度，降低锁竞争。降低了锁的竞争，偏向锁，轻量级锁，成功率才会提高。最典型的减小锁粒度的案例就是ConcurrentHashMap。

*锁分离*

最常见的锁分离就是读写锁ReadWriteLock，根据功能进行分离成读锁和写锁，这样读读不互斥，读写互斥，写写互斥，即保证了线程安全，又提高了性能，具体也可以查看【高并发java五】JDK并发包。1、读写分离思想可以延伸，只要操作互不影响，锁就可以分离。比如LinkedBlockingQueue 从头部取出，从尾部放数据。

*锁粗化* 

通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽量的短，即在使用完公共资源后，应该立即释放锁，但凡事都有一个度，如果对同一个锁不停的进行请求，同步和释放，其本身也会消耗系统的宝贵资源，反而不利于性能的优化。

*锁消除*

锁消除是在编译器级别的事情，在即时编译器时，如果发现不可能被共享的对象，则可以消除这些对象的锁操作，多数是因为程序员的编码不规范引起。

#### 10. 线程基本方法

线程相关的基本方法有 wait，notify，notifyAll，sleep，join，yield等。

![page73image53600016.png](http://wangrrui.com/2019-08-08-031547.png) 

##### 1. 线程等待 wait

调用该方法的线程进入WAITING状态，只有等待另外线程的通知或被中断才会返回，需要注意的是调用wait( )方法后，会释放对象的锁。因此，wait方法一般用在同步方法或者同步代码块中。

##### 2. 线程睡眠 sleep

sleep导致当前线程休眠，与wait 方法不同的是sleep不会释放当前占有的锁，sleep(long)会导致线程进入TIMED-WAITING状态，而wait()方法会导致当前线程进入WAITING状态

##### 3. 线程让步 yield

yield 会使当前线程让出 CPU 执行时间片，与其他线程一起重新竞争 CPU 时间片。一般情况下，优先级高的线程有更大的可能性成功竞争得到 CPU 时间片，但这又不是绝对的，有的操作系统对线程优先级并不敏感。

##### 4. 线程中断 interrupt

中断一个线程，其本意是给这个线程一个通知信号，会影响这个线程内部的一个中断标识位。这个线程本身并不会因此而改变状态(如阻塞，终止等)。

1. 调用 interrupt()方法并不会中断一个正在运行的线程。也就是说处于 Running 状态的线程并不会因为被中断而被终止，仅仅改变了内部维护的中断标识位而已。
2. 若调用 sleep()而使线程处于 TIMED-WATING 状态，这时调用 interrupt()方法，会抛出InterruptedException,从而使线程提前结束 TIMED-WATING 状态。
3. 许多声明抛出 InterruptedException 的方法(如 Thread.sleep(long mills 方法))，抛出异常前，都会清除中断标识位，所以抛出异常后，调用 isInterrupted()方法将会返回 false.
4. 中断状态是线程固有的一个标识位，可以通过此标识位安全的终止线程。比如,你想终止一个线程 thread 的时候，可以调用 thread.interrupt()方法，在线程的 run 方法内部可以根据 thread.isInterrupted()的值来优雅的终止线程。

##### 5. Join 等待其他线程终止

join() 方法，等待其他线程终止，在当前线程中调用一个线程的 join() 方法，则当前线程转为阻塞状态，回到另一个线程结束，当前线程再由阻塞状态变为就绪状态，等待 cpu 的宠幸。

##### 6. 为什么要用join() 方法？

很多情况下，主线程生成并启动了子线程，需要用到子线程返回的结果，也就是需要主线程需要在子线程结束后再结束，这时候就要用到 join() 方法。
```java
System.out.println(Thread.currentThread().getName() + "线程运行开始!");
 Thread6 thread1 = new Thread6();
 thread1.setName("线程 B");
 thread1.join();
System.out.println("这时 thread1 执行完毕之后才能执行主线程");
```

##### 7 线程唤醒 notify

Object 类中的 notify() 方法，唤醒在此对象监视器上等待的单个线程，如果所有线程都在此对象上等待，则会选择唤醒其中一个线程，选择是任意的，并在对实现做出决定时发生，线程通过调用其中一个 wait() 方法，在对象的监视器上等待，直到当前的线程放弃此对象上的锁定，才能继续执行被唤醒的线程，被唤醒的线程将以常规方式与在该对象上主动同步的其他所有线程进行竞争。类似的方法还有 notifyAll() ，唤醒再次监视器上等待的所有线程。

##### 8 其他方法

1. sleep():强迫一个线程睡眠N毫秒。
2. isAlive(): 判断一个线程是否存活。
3. join(): 等待线程终止。
4. activeCount(): 程序中活跃的线程数。
5. enumerate(): 枚举程序中的线程。
6. currentThread(): 得到当前线程。
7. isDaemon(): 一个线程是否为守护线程。
8. setDaemon(): 设置一个线程为守护线程。(用户线程和守护线程的区别在于，是否等待主线程依赖于主线程结束而结束)
9. setName(): 为线程设置一个名称。
10. wait(): 强迫一个线程等待。
11. notify(): 通知一个线程继续运行。
12. setPriority(): 设置一个线程的优先级。
13. getPriority()::获得一个线程的优先级。

#### 11. 线程上下文切换

巧妙地利用时间片轮转的方式，CPU给每个任务都服务一定的时间，然后把当前任务的状态保存下来，在加载下一个任务的状态后，继续服务下一任务，任务的状态保存及加载，这段过程叫做上下问切换。时间片轮转的方式使多个任务在同一颗CPU上执行变成了可能。

![page75image54469600.png](http://wangrrui.com/2019-08-08-071117.png) 

##### 1. 进程

(有时候也称做任务)是指一个程序运行的实例。在 Linux 系统中，线程就是能并行运行并且与他们的父进程(创建他们的进程)共享同一地址空间(一段内存区域)和其他资源的轻量级的进程。

##### 2. 上下文

是指某一时间点CPU寄存器和程序计数器的内容。

##### 3. 寄存器

是 CPU 内部的数量较少但是速度很快的内存(与之对应的是 CPU 外部相对较慢的 RAM 主内存)。寄存器通过对常用值(通常是运算的中间值)的快速访问来提高计算机程序运行的速度。

##### 4. 程序计数器

是一个专用的寄存器，用于表明指令序列中 CPU 正在执行的位置，存的值为正在执行的指令的位置或者下一个将要被执行的指令的位置，具体依赖于特定的系统。

##### 5. PCB-“切换桢”

上下文切换可以认为是内核(操作系统的核心)在 CPU 上对于进程(包括线程)进行切换，上下文切换过程中的信息是保存在进程控制块(PCB, process control block)中的。PCB 还经常被称作“切换桢”(switchframe)。信息会一直保存到 CPU 的内存中，直到他们被再次使用。

##### 6. 上下文切换的活动

1. 刮起一个进程，将这个进程在CPU中的状态（上下文）存储与内存中的某处
2. 在内存中检索下一个进程的上下文并将其在CPU的寄存器中恢复
3. 跳转到程序计数器所指向的位置（即跳转到进程被中断时的代码行），以恢复该进程在程序中。

##### 7. 引起线程上下文切换的原因

1. 当前执行任务的时间片用完之后，系统cpu正常调度下一个任务
2. 当前执行任务碰到IO阻塞，调度器将此任务挂起，继续下一个任务
3. 多个任务抢占锁资源，当前任务没有抢到锁资源，被调度器挂起，继续下一个任务
4. 用户代码挂起当前任务，让出cpu时间
5. 硬件中断

#### 12. 同步锁与死锁

##### 1. 同步锁

当多个线程同时访问同一个数据时，很容易出现问题。为了避免这种情况出现，我们要保证线程同步互斥，就是指并发执行的多个线程，在同一时间只允许一个线程访问共享数据，java中可以使用synchronized 关键字来取得一个对象的同步锁。

##### 2. 死锁

多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。

- 互斥条件：一个资源每次只能被一个线程使用
- 请求与保持条件：一个线程因请求资源而阻塞时，对已获得的资源保持不放
- 不剥夺条件：进程已经获得的资源，在未使用完之前，不能强行剥夺
- 
- 循环等待条件：若干线程之间形成一种头尾相接的循环等待资源关系

消除死锁的一个很好的解决办法就是指定获取锁的顺序，举例如下：

比如某个线程只有获得A锁和B锁才能对某资源进行操作，在多线程条件下，如何避免死锁？
获得锁的顺序是一定的，比如规定，只有获得A锁的线程才有资格获取B锁，按顺序获取锁就可以避免死锁

#### 13. 线程池原理

多线程实现方式主要有四种：

- 继承Thread类
- 实现Runnable接口、
- 实现Callable接口通过FutureTask包装器来创建Thread线程、
- 使用ExecutorService、Callable、Future实现有返回结果的多线程。

其中前两种方式线程执行完后都没有返回值，后两种是带返回值的。

```java
public class MyThread extends Thread {  
　　public void run() {  
　　 System.out.println("MyThread.run()");  
　　}  
}
MyThread myThread1 = new MyThread();  
MyThread myThread2 = new MyThread();  
myThread1.start();  
myThread2.start(); 


public class MyThread extends OtherClass implements Runnable {  
　　public void run() {  
　　 System.out.println("MyThread.run()");  
　　}  
}
MyThread myThread = new MyThread();  
Thread thread = new Thread(myThread);  
thread.start();


public class SomeCallable<V> extends OtherClass implements Callable<V> {
    @Override
    public V call() throws Exception {
        // TODO Auto-generated method stub
        return null;
    }
}

Callable<V> oneCallable = new SomeCallable<V>();   
//由Callable<Integer>创建一个FutureTask<Integer>对象：   
FutureTask<V> oneTask = new FutureTask<V>(oneCallable);   
//注释：FutureTask<Integer>是一个包装器，它通过接受Callable<Integer>来创建，它同时实现了Future和Runnable接口。 
//由FutureTask<Integer>创建一个Thread对象：   
Thread oneThread = new Thread(oneTask);   
oneThread.start();   
//至此，一个线程就创建完成了
```

**4、使用ExecutorService、Callable、Future实现有返回结果的线程**

ExecutorService、Callable、Future三个接口实际上都是属于Executor框架。返回结果的线程是在JDK1.5中引入的新特征，有了这种特征就不需要再为了得到返回值而大费周折了。而且自己实现了也可能漏洞百出。
可返回值的任务必须实现Callable接口。类似的，无返回值的任务必须实现Runnable接口。
执行Callable任务后，可以获取一个Future的对象，在该对象上调用get就可以获取到Callable任务返回的Object了。
注意：get方法是阻塞的，即：线程无返回结果，get方法会一直等待。
再结合线程池接口ExecutorService就可以实现传说中有返回结果的多线程了。
下面提供了一个完整的有返回结果的多线程测试例子，在JDK1.5下验证过没问题可以直接使用。代码如下：

```java
import java.util.concurrent.*;  
import java.util.Date;  
import java.util.List;  
import java.util.ArrayList;  
  
/** 
* 有返回值的线程 
*/  
@SuppressWarnings("unchecked")  
public class Test {  
public static void main(String[] args) throws ExecutionException,InterruptedException {  
   System.out.println("----程序开始运行----");  
   Date date1 = new Date();  
   int taskSize = 5;  
   // 创建一个线程池  
   ExecutorService pool = Executors.newFixedThreadPool(taskSize);  
   // 创建多个有返回值的任务  
   List<Future> list = new ArrayList<Future>();  
   for (int i = 0; i < taskSize; i++) {  
    Callable c = new MyCallable(i + " ");  
    // 执行任务并获取Future对象  
    Future f = pool.submit(c);  
    // System.out.println(">>>" + f.get().toString());  
    list.add(f);  
   }  
   // 关闭线程池  
   pool.shutdown();  
   // 获取所有并发任务的运行结果  
   for (Future f : list) {  
    // 从Future对象上获取任务的返回值，并输出到控制台  
    System.out.println(">>>" + f.get().toString());  
   }  
  
   Date date2 = new Date(); 
   System.out.println("----程序结束运行----，程序运行时间【"  
     + (date2.getTime() - date1.getTime()) + "毫秒】");  
}  
}  
  
class MyCallable implements Callable<Object> {  
  private String taskNum;  
  MyCallable(String taskNum) {  
     this.taskNum = taskNum;  
  }  
  
	public Object call() throws Exception {  
     System.out.println(">>>" + taskNum + "任务启动");  
     Date dateTmp1 = new Date();  
     Thread.sleep(1000);  
     Date dateTmp2 = new Date();  
     long time = dateTmp2.getTime() - dateTmp1.getTime();  
     System.out.println(">>>" + taskNum + "任务终止");  
     return taskNum + "任务返回运行结果,当前任务时间【" + time + "毫秒】";  
	}  
} 
```






线程池做的工作主要是控制运行的线程的数量，处理过程中将任务放入队列，然后在线程创建后启动这些任务，如果线程数量超过最大数量，超出数量的线程排队等候，等其他线程执行完毕，再从队列中取出任务执行，主要特点： 线程复用，控制最大并发数，管理线程。

##### 1. 线程复用

每一个Thread的类都有一个start方法，当调用start方法时java虚拟机会调用类的run方法，那么该类的run()方法中就调用类Runnable对象的run()方法。我们可以继承重写Thread类，在其start方法中添加不断循环调用传递过来的Runnable对象，这就是线程池的实现原理。循环方法中一段获取Runnable时Queue实现的。在获取下一个Runnable之前可以时阻塞的。

##### 2. 线程池的组成

一般线程池主要分为以下4个组成部分：

1. 线程池管理器：用于创建并管理线程池
2. 工作线程：线程池中的线程
3. 任务接口：每个任务必须实现的接口，用于工作线程调度其运行
4. 任务队列：用于存放待处理的任务，提供一种缓冲机制

TreadPoolExecutor的构造方法如下：

```java
public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue<Runnable> workQueue) {
  this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,
  Executors.defaultThreadFactory(), defaultHandler);
}
```

1. corePoolSize: 指定线程池中线程的数量
2. maximumPoolSize:指定了线程池中的最大线程数量。 
3. keepAliveTime:当前线程池数量超过 corePoolSize 时，多余的空闲线程的存活时间，即多次时间内会被销毁。
4. unit:keepAliveTime 的单位。
5. workQueue:任务队列，被提交但尚未被执行的任务。
6. threadFactory:线程工厂，用于创建线程，一般用默认的即可。 
7. handler:拒绝策略，当任务太多来不及处理，如何拒绝任务。

##### 3. 拒绝策略

线程池中的线程已经用完了，无法继续为新任务服务，同时，等待队列也已经排满了，再也 塞不下新任务了。这时候我们就需要拒绝策略机制合理的处理这个问题。
JDK 内置的拒绝策略如下:
1. AbortPolicy : 直接抛出异常，阻止系统正常运行。
2. CallerRunsPolicy : 只要线程池未关闭，该策略直接在调用者线程中，运行当前被丢弃的 任务。显然这样做不会真的丢弃任务，但是，任务提交线程的性能极有可能会急剧下降。
3. DiscardOldestPolicy : 丢弃最老的一个请求，也就是即将被执行的一个任务，并尝试再 次提交当前任务。
4. DiscardPolicy : 该策略默默地丢弃无法处理的任务，不予任何处理。如果允许任务丢失，这是最好的一种方案。

以上内置拒绝策略均实现了 RejectedExecutionHandler 接口，若以上策略仍无法满足实际需要，完全可以自己扩展 RejectedExecutionHandler 接口。

##### 4. java线程池工作过程

1. 线程池刚创建时，里面没有一个线程。任务队列是作为参数传进来的。不过，就算队列里面有任务，线程池也不会马上执行它们。

2. 当调用 execute() 方法添加一个任务时，线程池会做如下判断:
	1. 如果正在运行的线程数量小于 corePoolSize，那么马上创建线程运行这个任务;
	2. 如果正在运行的线程数量大于或等于 corePoolSize，那么将这个任务放入队列;
	3. 如果这时候队列满了，而且正在运行的线程数量小于 maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务;
	4. 如果队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会抛出异常 RejectExecutionException。
	
3. 当一个线程完成任务时，它会从队列中取下一个任务来执行。

4. 当一个线程无事可做，超过一定的时间(keepAliveTime)时，线程池会判断，如果当前运行的线程数大于 corePoolSize，那么这个线程就被停掉。所以线程池的所有任务完成后，它最终会收缩到 corePoolSize 的大小。

   **Java 中的阻塞队列** 

   ArrayBlockingQueue :由数组结构组成的有界阻塞队列。 

   LinkedBlockingQueue :由链表结构组成的有界阻塞队列。 

   PriorityBlockingQueue :支持优先级排序的无界阻塞队列。 

   DelayQueue:使用优先级队列实现的无界阻塞队列。 

   SynchronousQueue:不存储元素的阻塞队列。 

   LinkedTransferQueue:由链表结构组成的无界阻塞队列。 

   LinkedBlockingDeque:由链表结构组成的双向阻塞队列 

5. newFixedThreadPool   固定线程数 使用的LinkedBlockingQueue. 队列容量 Interge.MAX.VALUE

6. singleThreadExecutor.   单线程  使用的LinkedBlockingQueue. 队列容量 Interge.MAX.VALUE

7. cachedThreadPool       使用的SynchronousQueue   无界队列

8. scheduledThreadPoolExecutor        使用过的DelayQueue。      无界队列

**原子包** **java.util.concurrent.atomic****(锁自旋)** 

JDK1.5 的原子包:java.util.concurrent.atomic 这个包里面提供了一组原子类。其基本的特性就 是在多线程环境下，当有多个线程同时执行这些类的实例包含的方法时，具有排他性，即当某个 线程进入方法，执行其中的指令时，不会被其他线程打断，而别的线程就像自旋锁一样，一直等 

到该方法执行完成，才由 JVM 从等待队列中选择一个另一个线程进入，这只是一种逻辑上的理解。 

相对于对于 synchronized 这种阻塞算法，CAS 是非阻塞算法的一种常见实现。 

#### 15. countDownLanch。CyclicBarrier，  Semaphore  



除了锁， 13个aotomic 原子类也可以实现 线程通讯，  还有exchanger  也可以实现线程通讯

countDownLanch CyclicBarrier 可以实现同时启动多个线程 

CountDownLatch 基于 AQS 的共享模式的使用，而 CyclicBarrier 基于 Condition 来实现的。  应用场景 需要等待线程

Semaphore类里面的两个内部类FairSync和NonfairSync都继承自AbstractQueuedSynchronizer。应用场景  控制同时执行的线程数

Exchanger用于进行线程间的数据交换 Exchanger类提供了两个方法，String exchange(V x):用于交换，启动交换并等待另一个线程调用exchange；String exchange(V x,long timeout,TimeUnit unit)：用于交换，启动交换并等待另一个线程调用exchange，并且设置最大等待时间，当等待时间超过timeout便停止等待。

构造时，内部创建了一个**Participant**对象，**Participant**是Exchanger的一个内部类，本质就是一个[ThreadLocal](https://segmentfault.com/a/1190000015558915)，用来保存线程本地变量**Node**：

单位槽交换
***首先到达的线程：\***

1. 如果当前线程是首个到达的线程，会将**slot**字段指向自身的**Node**结点，表示槽位被占用；
2. 然后，线程会自旋一段时间，如果经过一段时间的自旋还是等不到配对线程到达，就会进入阻塞。（**这里之所以不直接阻塞，而是自旋，是出于线程上下文切换开销的考虑，属于一种优化手段**）
***稍后到达的配对线程：\***
如果当前线程（配对线程）不是首个到达的线程，则到达时槽（**slot**）已经被占用，此时**slot**指向首个到达线程自身的**Node**结点。配对线程会将**slot**置空，并取Node中的**item**作为交换得到的数据返回，另外，配对线程会把自身携带的数据存入**Node**的**match**字段中，并唤醒`Node.parked`所指向的线程（也就是先到达的线程）。
***首先到达的线程被唤醒：\***
线程被唤醒后，由于**match**不为空（存放了配对线程携带过来的数据），所以会退出自旋，然后将**match**对应的值返回。
这样，线程A和线程B就实现了数据交换，**整个过程都没有用到同步操作**。
多位槽交换。 并发量大时

有区别的是CyclicBarrier的计数器由自己控制，而CountDownLatch的计数器则由使用者来控制，在CyclicBarrier中线程调用await方法不仅会将自己阻塞还会将计数器减1，而在CountDownLatch中线程调用await方法只是将自己阻塞而不会减少计数器的值。

另外，CountDownLatch只能拦截一轮，而CyclicBarrier可以实现循环拦截。一般来说用CyclicBarrier可以实现CountDownLatch的功能，而反之则不能，例如上面的赛马程序就只能使用CyclicBarrier来实现。总之，这两个类的异同点大致如此，至于何时使用CyclicBarrier，何时使用CountDownLatch，还需要读者自己去拿捏。

除此之外，CyclicBarrier还提供了：resert()、getNumberWaiting()、isBroken()等比较有用的方法

16 .ThreadLocal

当使用ThreadLocal维护变量时，ThreadLocal为每个使用该变量的线程提供独立的变量副本，所以每一个线程都可以独立地改变自己的副本，而不会影响其它线程所对应的副本。在ThreadLocal类中有一个Map，用于存储每一个线程的变量副本，Map中元素的键为线程对象，而值对应线程的变量副本。使用ThreadLocal的典型场景如数据库连接管理，线程会话管理等场景，只适用于独立变量副本的情况，如果变量为全局共享的，则不适用在高并发下使用。

Condition原理分析

ConditionObject是同步器AbstractQueuedSynchronizer的内部类，因为Condition的操作需要获取相关联的锁，所以作为同步器的内部类也较为合理。每个Condition对象都包含着一个队列，该队列是Condition对象实现等待/通知功能的关键。下面将分析Condition的实现，主要包括：等待队列、等待和通知

等待队列

等待队列是一个FIFO的队列，在队列中的每个节点都包含了一个线程引用，该线程就是在Condition对象上等待的线程，如果一个线程调用了Condition.await()方法，那么该线程将会释放锁、构造成节点加入等待队列并进入等待状态 
一个Condition包含一个等待队列，Condition拥有首节点（firstWaiter）和尾节点（lastWaiter）。当前线程调用Condition.await()方法，将会以当前线程构造节点，并将节点从尾部加入等待队列，等待队列的基本结构如下图所示 
Condition拥有首尾节点的引用，而新增节点只需要将原有的尾节点nextWaiter指向它，并且更新尾节点即可。上述节点引用更新的过程并没有使用CAS保证，原因在于调用await()方法的线程必定是获取了锁的线程，也就是说该过程是由锁来保证线程安全的。在Object的监视器模型上，一个对象拥有一个同步队列和等待队列，而并发包中的Lock（更确切地说是同步器）拥有一个同步队列和多个等待队列，其对应关系如下图所示 
调用Condition的await()方法（或者以await开头的方法），会使当前线程进入等待队列并释放锁，同时线程状态变为等待状态。当从await()方法返回时，当前线程一定获取了Condition相关联的锁。如果从队列（同步队列和等待队列）的角度看await()方法，当调用await()方法时，相当于同步队列的首节点（获取了锁的节点）移动到Condition的等待队列中
调用该方法的线程成功获取了锁的线程，也就是同步队列中的首节点，该方法会将当前线程构造成节点并加入等待队列中，然后释放同步状态，唤醒同步队列中的后继节点，然后当前线程会进入等待状态。当等待队列中的节点被唤醒，则唤醒节点的线程开始尝试获取同步状态。如果不是通过其他线程调用Condition.signal()方法唤醒，而是对等待线程进行中断，则会抛出InterruptedException

通知

调用Condition的signal()方法，将会唤醒在等待队列中等待时间最长的节点（首节点），在唤醒节点之前，会将节点移到同步队列中 
调用该方法的前置条件是当前线程必须获取了锁，可以看到signal()方法进行了isHeldExclusively()检查，也就是当前线程必须是获取了锁的线程。接着获取等待队列的首节点，将其移动到同步队列并使用LockSupport唤醒节点中的线程 
节点从等待队列移动到同步队列的过程如下图所示 
通过调用同步器的enq(Node node)方法，等待队列中的头节点线程安全地移动到同步队列。当节点移动到同步队列后，当前线程再使用LockSupport唤醒该节点的线程。被唤醒后的线程，将从await()方法中的while循环中退出（isOnSyncQueue(Node node)方法返回true，节点已经在同步队列中），进而调用同步器的acquireQueued()方法加入到获取同步状态的竞争中。成功获取同步状态（或者说锁）之后，被唤醒的线程将从先前调用的await()方法返回，此时该线程已经成功地获取了锁。Condition的signalAll()方法，相当于对等待队列中的每个节点均执行一次signal()方法，效果就是将等待队列中所有节点全部移动到同步队列中，并唤醒每个节点的线程。

#### 16. Fork/join 框架 

是把一个大任务分割成若干个小任务，最终汇总每个小任务的结果得到大任务的结果

用了 工作窃取算法- 某个线程从其他队列里窃取任务来执行，

ForkJoinTask。   RecursiveAction. 用于没有返回结果的任务 RecursiveTask 用于有返回记过的任务

 ForkJoinPool ，执行ForkJoinTask

实现原理 ForkJoinPool有ForkJoinTask数组和ForkJoinWorkerThread数组组成，ForkJoinTask数组负责将存放程序提交给ForkJoinPool的任务，ForkJoinWorkerThread数组负责执行这些任务。

​    



## 5. 设计模式

### 1. 单例模式

**单例模式的应用场景**

单例模式(Singleton Pattern)是指确保一个类在任何情况下都绝对只有一个实例，并 提供一个全局访问点。单例模式是创建型模式。单例模式在现实生活中应用也非常广泛。 例如，国家主席、公司 CEO、部门经理等。在 J2EE 标准中，ServletContext、 ServletContextConfig 等;在 Spring 框架应用中 ApplicationContext;数据库的连接 池也都是单例形式。

#### 饿汉式单例

饿汉式单例是在类加载的时候就立即初始化，并且创建单例对象。绝对线程安全，在线 程还没出现以前就是实例化了，不可能存在访问安全问题。 优点:没有加任何的锁、执行效率比较高，在用户体验上来说，比懒汉式更好。 缺点:类加载的时候就初始化，不管用与不用都占着空间，浪费了内存，有可能占着茅 坑不拉屎。
Spring 中 IOC 容器 ApplicationContext 本身就是典型的饿汉式单例。接下来看一段代 码:

```java
public class HungrySingleton { 
  //先静态、后动态 
  //先属性、后方法
  //先上后下
  private static final HungrySingleton hungrySingleton = new HungrySingleton();
  private HungrySingleton(){}
  public static HungrySingleton getInstance(){ 
    return hungrySingleton;
  } 
}

```

还有另外一种写法，利用静态代码块的机制:

```java
//饿汉式静态块单例
public class HungryStaticSingleton {
  private static final HungryStaticSingleton hungrySingleton;
  static {
  	hungrySingleton = new HungryStaticSingleton();
  }
  private HungryStaticSingleton(){}
  public static HungryStaticSingleton getInstance(){
  return hungrySingleton; 
  }
}
```

这两种写法都非常的简单，也非常好理解，饿汉式适用在单例对象较少的情况。下面我 们来看性能更优的写法。

#### 懒汉式单列
懒汉式单例的特点是:被外部类调用的时候内部类才会加载，下面看懒汉式单例的简单 实现 LazySimpleSingleton:
```java
//懒汉式单例 //在外部需要使用的时候才进行实例化 
public class LazySimpleSingleton {
  private LazySimpleSingleton(){} 
  //静态块，公共内存区域
  private static LazySimpleSingleton lazy = null; 
  public static LazySimpleSingleton getInstance(){
  if(lazy == null){
  	lazy = new LazySimpleSingleton();
  }
  return lazy; 
  }
}
```

然后写一个线程类 ExectorThread 类:

```java
public class ExectorThread implements Runnable{ 
  @Override
	public void run() {
		LazySimpleSingleton singleton = LazySimpleSingleton.getInstance(); 
 	  System.out.println(Thread.currentThread().getName() + ":" + singleton);
	} 
}
```

客户端测试代码:

```java
public class LazySimpleSingletonTest { 
 	public static void main(String[] args) {
    Thread t1 = new Thread(new ExectorThread()); 
    Thread t2 = new Thread(new ExectorThread()); 
    t1.start();
  	t2.start();
  	System.out.println("End"); 
  	}
}
```

运行结果: 多运行几次，会出现创建多个实例

```
End
Thread-0:com.duan.demo.singleton.LazySimpleSingleton@5724147d
Thread-1:com.duan.demo.singleton.LazySimpleSingleton@56214c1

Process finished with exit code 0
```
```java
//升级版
public class LazySimpleSingleton {
  private LazySimpleSingleton(){}
  //静态块，公共内存区域
  private static LazySimpleSingleton lazy = null;
  public synchronized static LazySimpleSingleton getInstance(){
    if(lazy == null){
    lazy = new LazySimpleSingleton();
  }
  return lazy; }
}

// 再次升级 volatile 双重检查	
public class LazyDoubleCheckSingleton {
  private volatile static LazyDoubleCheckSingleton lazy = null;
  private LazyDoubleCheckSingleton(){}
  public static LazyDoubleCheckSingleton getInstance(){
    if(lazy == null){
    	synchronized (LazyDoubleCheckSingleton.class){
        if(lazy == null){
          lazy = new LazyDoubleCheckSingleton(); 
          //1.分配内存给这个对象
          //2.初始化对象
          //3.设置 lazy 指向刚分配的内存地址
        } 
    }
  }
  return lazy; }
}


//再次升级  静态内部类的方法
//这种形式兼顾饿汉式的内存浪费，也兼顾 synchronized 性能问题 //完美地屏蔽了这两个缺点
public class LazyInnerClassSingleton {
  private LazyInnerClassSingleton(){}
  //默认使用 LazyInnerClassGeneral 的时候，会先初始化内部类
  //如果没使用的话，内部类是不加载的 private LazyInnerClassSingleton(){}
  //每一个关键字都不是多余的
  //static 是为了使单例的空间共享
  //保证这个方法不会被重写，重载
  public static final LazyInnerClassSingleton getInstance(){
    //在返回结果以前，一定会先加载内部类
    return LazyHolder.LAZY; 
  }
  //默认不加载
  private static class LazyHolder{
  	private static final LazyInnerClassSingleton LAZY = new LazyInnerClassSingleton(); 
  }
}


```
这种形式兼顾饿汉式的内存浪费，也兼顾 synchronized 性能问题。内部类一定是要在方 法调用之前初始化，巧妙地避免了线程安全问题。

**反射破坏单例**

大家有没有发现，上面介绍的单例模式的构造方法除了加上 private 以外，没有做任何处 理。如果我们使用反射来调用其构造方法，然后，再调用 getInstance()方法，应该就会 两个不同的实例。现在来看一段测试代码，以 LazyInnerClassSingleton 为例:

```java
public class LazyInnerClassSingletonTest { 
  public static void main(String[] args) {
    try{
      //很无聊的情况下，进行破坏
      Class<?> clazz = LazyInnerClassSingleton.class;
      //通过反射拿到私有的构造方法
      Constructor c = clazz.getDeclaredConstructor(null); 
      //强制访问，强吻，不愿意也要吻
      c.setAccessible(true);
      //暴力初始化
      Object o1 = c.newInstance();
      //调用了两次构造方法，相当于 new 了两次 //犯了原则性问题，
      Object o2 = c.newInstance();
      System.out.println(o1 == o2); Object o2 = c.newInstance();
    }catch (Exception e){ 
      e.printStackTrace();
    } 
  }
}
```
显然，是创建了两个不同的实例。现在，我们在其构造方法中做一些限制，一旦出现多 次重复创建，则直接抛出异常。来看优化后的代码:

```java
//史上最牛 B 的单例模式的实现方式
public class LazyInnerClassSingleton {
  //默认使用 LazyInnerClassGeneral 的时候，会先初始化内部类 
  //如果没使用的话，内部类是不加载的
  private LazyInnerClassSingleton(){
    if(LazyHolder.LAZY != null){
      throw new RuntimeException("不允许创建多个实例");
    } 
  }
  //每一个关键字都不是多余的
  //static 是为了使单例的空间共享
  //final 保证这个方法不会被重写，重载
  public static final LazyInnerClassSingleton getInstance(){
    //在返回结果以前，一定会先加载内部类
    return LazyHolder.LAZY; 
	}
  //默认不加载
  private static class LazyHolder{
     private static final LazyInnerClassSingleton LAZY = new LazyInnerClassSingleton(); 
  }
}
```

#### 注册式单例
注册式单例又称为登记式单例，就是将每一个实例都登记到某一个地方，使用唯一的标 识获取实例。注册式单例有两种写法:一种为容器缓存，一种为枚举登记。先来看枚举 式单例的写法，来看代码，创建 EnumSingleton 类:

```java
public enum EnumSingleton { 
  INSTANCE;
  private Object data; public Object getData() {
    return data;
  }
  public void setData(Object data) { 
    this.data = data;
  }
  public static EnumSingleton getInstance(){
  	return INSTANCE; 
  }
}
```
枚举式单例也是《Effective Java》书中推荐的一种单例实现写法。在 JDK 枚举的语法特殊性，以及反射也为枚举保 驾护航，让枚举式单例成为一种比较优雅的实现。

### 2. 原型模式
原型模式(Prototype Pattern)是指原型实例指定创建对象的种类，并且通过拷贝这些 原型创建新的对象。
原型模式主要适用于以下场景:
1. 类初始化消耗资源较多。
2. new 产生的一个对象需要非常繁琐的过程(数据准备、访问权限等)
3. 构造函数比较复杂
4. 循环体中生产大量对象时。

在 Spring 中，原型模式应用得非常广泛。例如 scope=“prototype”，在我们经常用 的 JSON.parseObject()也是一种原型模式



### 3. 委派模式 

委派模式的定义及应用场景

委派模式不属于 GOF23 种设计模式中。委派模式(Delegate Pattern)的基本作用就是 负责任务的调用和分配任务，跟代理模式很像，可以看做是一种特殊情况下的静态代理 的全权代理，但是代理模式注重过程，而委派模式注重结果。委派模式在 Spring 中应用 非常多，大家常用的 DispatcherServlet 其实就是用到了委派模式。现实生活中也常有委 派的场景发生，例如:老板(Boss)给项目经理(Leader)下达任务，项目经理会根据 实际情况给每个员工派发工作任务，待员工把工作任务完成之后，再由项目经理汇报工 作进度和结果给老板。 

在 Spring 源码中，只要以 Delegate 结尾的 都是实现了委派模式。例如:BeanDefinitionParserDelegate 根据不同类型委派不同的 逻辑解析 BeanDefinition。 

### 4. 策略模式 

策略模式(Strategy Pattern)是指定义了算法家族、分别封装起来，让它们之间可以互相替换，此模式让算法的变化不会影响到使用算法的用户。 

**策略模式的应用场景** 

1、假如系统中有很多类，而他们的区别仅仅在于他们的行为不同。 2、一个系统需要动态地在几种算法中选择一种。 **用策略模式实现选择支付方式的业务场景** 大家都知道，我们咕泡学院的架构师课程经常会有优惠活动，优惠策略会有很多种可能 如:领取优惠券抵扣、返现促销、拼团优惠 

**策略模式在 JDK 源码中的体现
** 首先来看一个比较常用的比较器 Comparator 接口，我们看到的一个大家常用的 compare()方法，就是一个策略抽象实现: 

```java
public interface Comparator<T> { 
	int compare(T o1, T o2);
	...
}
```

Comparator 抽象下面有非常多的实现类，我们经常会把 Comparator 作为参数传入作 为排序策略，例如 Arrays 类的 parallelSort 方法等: 

```java
public class Arrays { 
  ...
  public static <T> void parallelSort(T[] a, int fromIndex, int toIndex, Comparator<? super T> cmp) {
  	...	
  } 
  ...
}
```

还有 TreeMap 的构造方法: 

```java
public class TreeMap<K,V>
extends AbstractMap<K,V>
implements NavigableMap<K,V>, Cloneable, java.io.Serializable
{
	...
	public TreeMap(Comparator<? super K> comparator) { 
    this.comparator = comparator;
} 
  ...
}
```



**策略模式的优缺点** 

优点:
 1、策略模式符合开闭原则。
 2、避免使用多重条件转移语句，如 if...else...语句、switch 语句 

3、使用策略模式可以提高算法的保密性和安全性。
 缺点: 

1、客户端必须知道所有的策略，并且自行决定使用哪一个策略类。 

2、代码中会产生非常多策略类，增加维护难度。 

### 5. 模版模式

模板模式通常又叫模板方法模式(Template Method Pattern)是指定义一个算法的骨架，并允许子类为一个或者多个步骤提供实现。模板方法使得子类可以在不改变算法结构的情况下，重新定义算法的某些步骤，属于行为性设计模式。模板方法适用于以下应 用场景: 
1. 一次性实现一个算法的不变的部分，并将可变的行为留给子类来实现。 

2. 各子类中公共的行为被提取出来并集中到一个公共的父类中，从而避免代码重复 

优点: 
1. 利用模板方法将相同处理逻辑的代码放到抽象父类中，可以提高代码的复用性。
2. 将不同的代码不同的子类中，通过对子类的扩展增加新的行为，提高代码的扩展性。
3. 把不变的行为写在父类上，去除子类的重复代码，提供了一个很好的代码复用平台， 符合开闭原则。

缺点: 
1. 类数目的增加，每一个抽象类都需要一个子类来实现，这样导致类的个数增加。 

2. 类数量的增加，间接地增加了系统实现的复杂度。 

3. 继承关系自身缺点，如果父类添加新的抽象方法，所有子类都要改一遍。

### 6. 适配器模式

适配器模式的应用场景
适配器模式(Adapter Pattern)是指将一个类的接口转换成客户期望的另一个接口，使 原本的接口不兼容的类可以一起工作，属于结构型设计模式。
适配器适用于以下几种业务场景: 
1. 已经存在的类，它的方法和需求不匹配(方法结果相同或相似)的情况。
2. 适配器模式不是软件设计阶段考虑的设计模式，是随着软件维护，由于不同产品、不 同厂家造成功能类似而接口不相同情况下的解决方案。有点亡羊补牢的感觉。 生活中也非常的应用场景，例如电源插转换头、手机充电转换头、显示器转接头。

例子： 多种登录方式
适配器模式在源码中的体现
Spring 中适配器模式也应用得非常广泛，例如:SpringAOP 中的 AdvisorAdapter 类， 它有三个实现类 MethodBeforeAdviceAdapter、AfterReturningAdviceAdapter 和 ThrowsAdviceAdapter

SpringMVC 中的 HandlerAdapter 类，它也有多个子类，类图如下: 

其适配调用的关键代码还是在 DispatcherServlet 的 doDispatch()方法中 

适配器模式的优缺点
优点:
1. 能提高类的透明性和复用，现有的类复用但不需要改变。 
2. 目标类和适配器类解耦，提高程序的扩展性。 
3. 在很多业务场景中符合开闭原则。
缺点:
1. 适配器编写过程需要全面考虑，可能会增加系统的复杂性。
2. 增加代码阅读难度，降低代码可读性，过多使用适配器会使系统代码变得凌乱。

### 7. 代理模式

代理模式(Proxy Pattern)的定义也非常简单，是指为其他对象提供一种代理，以控制对这个对象的访问。 代理对象在客服端和目标对象之间起到中介作用，代理模式属于结构型设计模式。使用 代理模式主要有两个目的:一保护目标对象，二增强目标对象。下面我们来看一下代理 模式的类结构图: 

Subject 是顶层接口，RealSubject 是真实对象(被代理对象)，Proxy 是代理对象，代 理对象持有被代理对象的引用，客户端调用代理对象方法，同时也调用被代理对象的方 法，但是在代理对象前后增加一些处理。在代码中，我们想到代理，就会理解为是代码 增强，其实就是在原本逻辑前后增加一些逻辑，而调用者无感知。代理模式属于结构型 模式，有静态代理和动态代理。 






工厂模式 只对结果负责，封装创建过程

单例模式 保证独一无二

原型模式 一个猴毛 变出千万个

代理模式 找人办事，增强职责

委派模式。干活是你的，功劳是我的

策略模式 用户选择 结果单一

模版模式 流程标准化 自己实现定制

适配器模式 兼容转换头

装饰起模式 包装 同宗同源

观察者模式 任务完成通知



## 6. 数据库

常见的分库分表策略

取模， 分区， 路由表

取模： 按照订单 id ，用户id 分库分表，扩容困难，可以设计的时候提前布局，提前预估后两年的使用量，升级有限考虑提升硬件

分区： 按照时间，范围，晋西分库分表 ，存在热点数据，但是易于扩展，避免数据迁移。

二.聚集索引

聚集（clustered）索引，也叫聚簇索引。

定义：数据行的物理顺序与列值（一般是主键的那一列）的逻辑顺序相同，一个表中只能拥有一个聚集索引。
MySQL 索引类型有：唯一索引，主键（聚集）索引，非聚集索引，全文索引。
非聚集（unclustered）索引。

定义：该索引中索引的逻辑顺序与磁盘上行的物理存储顺序不同，一个表中可以拥有多个非聚集索引。
其实按照定义，除了聚集索引以外的索引都是非聚集索引，只是人们想细分一下非聚集索引，分成普通索引，唯一索引，全文索引。
他结构顺序与实际存放顺序不一定一致。
非聚集索引的二次查询问题

非聚集索引叶节点仍然是索引节点，只是有一个指针指向对应的数据块，此如果使用非聚集索引查询，而查询列中包含了其他该索引没有覆盖的列，那么他还要进行第二次的查询，查询节点上对应的数据行的数据。
如何解决非聚集索引的二次查询问题

复合索引（覆盖索引）

建立两列以上的索引，即可查询复合索引里的列的数据而不需要进行回表二次查询，如index(col1, col2)，执行下面的语句
使用聚集索引的查询效率要比非聚集索引的效率要高，但是如果需要频繁去改变聚集索引的值，写入性能并不高，因为需要移动对应数据的物理位置。
非聚集索引在查询的时候可以的话就避免二次查询，这样性能会大幅提升。
不是所有的表都适合建立索引，只有数据量大表才适合建立索引，且建立在选择性高的列上面性能会更好。

### B+Tree的定义

B+Tree是B树的变种，有着比B树更高的查询性能，来看下m阶B+Tree特征：

1、有m个子树的节点包含有m个元素（B-Tree中是m-1）

2、根节点和分支节点中不保存数据，只用于索引，所有数据都保存在叶子节点中。

3、所有分支节点和根节点都同时存在于子节点中，在子节点元素中是最大或者最小的元素。

4、叶子节点会包含所有的关键字，以及指向数据记录的指针，并且叶子节点本身是根据关键字的大小从小到大顺序链接

B+树的特征：
- 有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点
- 所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接
- 所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素

B+树的优势：
- 单一节点存储更多的元素，使得查询的IO次数更少
- 所有查询都要查找到叶子节点，查询性能稳定
- 所有叶子节点形成有序链表，便于范围查询

B+树的优势
1、更加高效的单元素查找
a、首先B+树的中间节点不存储卫星数据，所以同样大小的磁盘页可以容纳更多的节点元素，如此一来，相同数量的数据下，B+树就相对来说要更加矮胖些，磁盘IO的次数更少。

 b、由于只有叶子节点才保存卫星数据，B+树每次查询都要到叶子节点；而B树每次查询则不一样，最好的情况是根节点，最坏的情况是叶子节点，没有B+树稳定。

2、叶子节点形成有顺链表，范围查找性能更优

总结

1.单节点可以存储更多的元素，使得查询磁盘IO次数更少。

2.所有查询都要查找到叶子节点，查询性能稳定。

3.所有叶子节点形成有序链表，便于范围查询。

PS:在数据库的聚集索引（Clustered Index）中，叶子节点直接包含卫星数据。在非聚集索引（NonClustered Index）中，叶子节点带有指向卫星数据的指针。



B TREE和B+TREE区别是什么？

1、B+TREE 关键字的搜索采用的是左闭合区间，之所以采用左闭合区间是因为他要最好的去支持自增id，这也是mysql的设计初衷。即，如果id = 1命中，会继续往下查找，直到找到叶子节点中的1。

2、B+TREE 根节点和支节点没有数据区，关键字对应的数据只保存在叶子节点中。即只有叶子节点中的关键字数据区才会保存真正的数据内容或者是内容的地址。而在B树种，如果根节点命中，则会直接返回数据。并且在B+TREE中，叶子节点不会去保存子节点的引用。

3、B+TREE叶子节点是顺序排列的，并且相邻的节点具有顺序引用的关系，如上图中叶子节点之间有指针相连接。

1、B+TREE是B TREE的变种，B TREE能解决的问题，B+TREE也能够解决（降低树的高度，增大节点存储数据量）

2、	B+TREE扫库和扫表能力更强，如果我们要根据索引去进行数据表的扫描，对B TREE进行扫描，需要把整棵树遍历一遍，而B+TREE只需要遍历他的所有叶子节点即可（叶子节点之间有引用）。

3、B+TREE磁盘读写能力更强，他的根节点和支节点不保存数据区，所有根节点和支节点同样大小的情况下，保存的关键字要比B TREE要多。而叶子节点不保存子节点引用。所以，B+TREE读写一次磁盘加载的关键字比B TREE更多。

4、B+TREE排序能力更强，如上面的图中可以看出，B+TREE天然具有排序功能。

5、B+TREE查询效率更加稳定，每次查询数据，查询IO次数一定是稳定的。当然这个每个人的理解都不同，因为在B TREE如果根节点命中直接返回，确实效率更高。


联合索引：

单列索引：节点中的关键字[name]
联合索引：节点中的关键字[name, phoneNum]
可以把单列索引看成特殊的联合索引，联合索引的比较也是根据最左匹配原则。
联合索引列的选择原则：
（1）	经常用的列优先（最左匹配原则）
（2）	离散度高的列优先（离散度高原则）
（3）	宽度小的列优先，（最少空间原则）

下面简单举例平时经常会遇到的问题：
如，平时经常使用的查询sql如下：
Select * from users where name = ?
Select * from users where name = ? and pahoneNum = ?

为了加快检索速度，为上面的查询sql创建索引如下：
Create index idx_name on users(name)
Create index idx_name_phoneNum on users(name, phoneNum)

在上面解决方案中，根据最左匹配原则，idx_name为冗余索引， where name = ?同样可以利用索引idx_name_phoneNum进行检索。冗余索引会增减维护B+TREE平衡时的性能消耗，并且占用磁盘空间

通过前面的学习，我们可以很容易的明白如下一下结论：
1、索引列的数据长度满足业务的情况下能少则少。
2、表中的索引并不是越多越好。
3、Where 条件中，like 9%， like %9%， like%9，三种方式都用不到索引。后两种方式对于索引是无效的。第一种9%是不确定的，决定于列的离散型，结论上讲可以用到，如果发现离散情况特别差的情况下，查询优化器觉得走索引查询性能更差，还不如全表扫描。
4、Where条件中 NOT IN 无法使用索引
5、多用指定查询，只返回自己想要的列，少用select *。
6、查询条件中使用函数，索引将会失效，这和列的离散型有关，一旦使用到函数，函数具有不确定性。
7、联合索引中，如果不是按照索引最左列开始查找，无法使用索引。
8、对联合索引精确匹配最左前列并范围匹配另一列，可以使用到索引。
9、联合索引中，如果查询有某个列的范围查询，其右边所有的列都无法使用索引。



MySQL 主要分为 Server 层和引擎层，Server 层主要包括连接器、查询缓存、分析器、优化器、执行器，同时还有一个日志模块（binlog），这个日志模块所有执行引擎都可以共用,redolog 只有 InnoDB 有。
引擎层是插件式的，目前主要包括，MyISAM,InnoDB,Memory 等。
SQL 等执行过程分为两类，一类对于查询等过程如下：权限校验—》查询缓存—》分析器—》优化器—》权限校验—》执行器—》引擎
对于更新等语句执行流程如下：分析器----》权限校验----》执行器—》引擎—redo log prepare—》binlog—》redo log commit

##### 查询语句的执行顺序：

１.客户端通过TCP连接发送连接请求到mysql连接器，连接器会对该请求进行权限验证及连接资源分配（max_connections，8小时超时）

２.建立连接后客户端发送一条语句，mysql收到该语句后，通过命令分发器判断其是否是一条select语句，如果是，在开启查询缓存的情况下，先在查询缓存中查找该SQL是否完全匹配，如果完全匹配，验证当前用户是否具备查询权限，如果权限验证通过，直接返回结果集给客户端，该查询也就完成了。如果不匹配继续向下执行。（注意：此步并不做词法及语法分析，也就是用不到分析器，这块原来我也很疑惑，如果不做分析mysql怎么知道我要查什么？解释如下：{MySQL将缓存存放在一个引用表中，通过一个哈希值引用，这个哈希值包括了以下因素，即查询本身、当前要查询的数据库、客户端协议的版本等一些其他可能影响返回结果的信息。  当判断缓存是否命中时，MySQL不会进行解析查询语句，而是直接使用SQL语句和客户端发送过来的其他原始信息。所以，任何字符上的不同，例如空格、注解等都会导致缓存的不命中。} 其实说白了大概就是拿着你的SQL和原始缓存的SQL比对）

3.如果在查询缓存中未匹配成功，则将语句交给分析器作语法分析，MySQL需要知道到底要查哪些东西，如果语法不对，就会返回语法错误中断查询。

4.分析器的工作完成后，将语句传递给预处理器，检查数据表和数据列是否存在，解析别名看是否存在歧义等

5.语句解析完成后，MySQL就知道要查什么了，之后会将语句传递给优化器进行优化（通过索引选择最快的查找方式），并生成执行计划。

6.之后交给执行器去具体执行该语句，在执行之前，会先检查该用户是否具有查询权限，如果有，继续执行该语句。执行器开始执行后，会逐渐将数据保存到结果集中，同时会逐步将数据缓存到查询缓存中，最终将结果集返回给客户端。（缓存到查询缓存受到几个参数的影响 1.query_cache_type 是否打开查询缓存,默认为OFF  2.query_cache_size:查询缓存使用的总内存空间,默认值为1M   3.query_cache_limit 对于大于该值的结果集不会被缓存，默认值1M，在8.0版本后该参数被移除了）（如果该SQL执行过程中超过了慢查询阀值，该SQL会被记录到慢查询日志中） 

##### 更新语句的执行顺序：

１.客户端通过TCP连接发送连接请求到mysql连接器，连接器会对该请求进行权限验证及连接资源分配（max_connections，8小时超时）

 2.建立连接后客户端发送一条语句，mysql收到该语句后，通过命令分发器判断其是否是一条更新语句，如果是，则直接发送给分析器做语法分析。

 3.分析器阶段，MySQL需要知道到底要查哪些东西，如果语法不对，就会返回语法错误中断查询

 4.分析器的工作完成后，将语句传递给预处理器，检查数据表和数据列是否存在，解析别名看是否存在歧义等

 5.语句解析完成后，MySQL就知道要查什么了，之后会将语句传递给优化器进行优化（通过索引选择最快的查找方式），并生成执行计划。

 6.执行器根据生成的执行计划去open table，此时会先去查看该表上是否有元数据（MDL）排他锁（如果有元数据共享锁则无影响），如果有元数据排他锁，则事物被阻塞，进入等待状态（时间由lock_wait_timeout决定，默认是一年。。。。），等元数据锁被释放，继续执行。如果无元数据锁或者是有元数据共享锁，则该事务在表上加元数据共享锁（因为元数据共享读锁之间是不冲突的，如果表上有元数据共享锁，我们执行alter table这样的DDL语句时，会进入等待状态，因为DDL语句需要在表上加元数据排他锁）

 7.进入引擎层（默认innodb），去innodb_buffer_pool里面的data dictionary得到表得相关信息

 8.根据表信息去innodb_buffer_pool里面的lock info查看是否有相关的锁信息，如果有则等待（因为要加排它锁），如果没有则加排它锁，更新lock info。

 9.取读取相关数据页到innodb_buffer_pool中（如果数据页本身就在缓存中，则不用从硬盘读取）

10.将页中的原始数据（快照）保存到undo log buffer中（undo log buffer会以相关参数定义的规则进行刷盘操作写入到undo tablespace中）

11.在innodb_buffer_pool中将相关页面更新，该页变成脏页（脏页会以相关参数定义的规则进行刷盘操作写入所属表空间中）

12.页面修改完成后，会把修改后的物理页面保存到redo log buffer中，（redo log buffer会以相关参数定义的规则进行刷盘操作写入到redo tablespace中）

13.如果开启binlog，则更新数据的逻辑语句也会记录在binlog_cache中（binlog会以相关参数定义的规则进行刷盘操作写入到binlog file 中）

14.如果该表上有二级索引并且本次操作会影响到二级索引，则会把相关的二级索引修改写入到innodb_buffer_pool中的change buffer里（change buffer 会以相关参数定义的规则进行刷盘操作写入所属表空间中）

15.前期的准备工作到此已经做完了，之后便是事务的commit或者rollback操作。一般情况下执行的是commit操作

16.执行commit操作后（mysql默认开启自动提交，如果手动开始事务begin，则需要显示提交commit），由于要保证redolog与binlog的一致性，redolog采用2阶段提交方式。

17.将undo log buffer及redo log buffer刷盘（innodb_flush_log_at_trx_commit=1），并将该事务的redolog标记为prepare状态。

18.将binlog_cache数据刷盘（sync_binlog=1）

19.如果开启了主从结构，此时会将binlog_cache中的信息通过io线程发送给从机，如果开启了半同步复制则需要等待从机落盘（relay log）并反馈。如果是异步复制则无需等待(默认是异步复制)

20.待binlog落盘完成，再将redolog中该事务信息标记为commit，释放相关锁资源。此时一个更新事务的操作已经完成，返回给客户端成功更新提示。

21.标记undolog中该事务修改页的原始快照信息为delete，当无其他事务引用该原始数据时(MVCC)，再将其删除

22.如果此时触发了脏页刷盘操作，会先将脏页写入到double write buffer中（防止写入过程中出现断页，因为mysql页面默认为16K，linux操作系统最大为4K，如果写了8K时系统挂了，这个数据页将不完整，标记为损坏）然后再写到期所在表空间的相应位置。

采用 redo log 两阶段提交的方式就不一样了，写完 binglog 后，然后再提交 redo log 就会防止出现上述的问题，从而保证了数据的一致性。那么问题来了，有没有一个极端的情况呢？假设 redo log 处于预提交状态，binglog 也已经写完了，这个时候发生了异常重启会怎么样呢？ 这个就要依赖于 MySQL 的处理机制了，MySQL 的处理过程如下：

- 判断 redo log 是否完整，如果判断是完整的，就立即提交。
- 如果 redo log 只是预提交但不是 commit 状态，这个时候就会去判断 binlog 是否完整，如果完整就提交 redo log, 不完整就回滚事务。

这样就解决了数据一致性的问题。



那么sql的执行顺序呢？

FROM: 对前2个表执行笛卡尔积，生成虚表vt1
ON: 对vt1应用on条件，只有满足join_condition条件的才能插入虚表vt2
OUTER(join)：如果指定了 OUTER JOIN保留表(preserved table)中未找到的行将行作为外部行添加到vt2，生成t3，如果from包含两个以上表，则对上一个联结生成的结果表和下一个表重复执行步骤和步骤直接结束
WHERE: 对vt3进行where筛选，只有满足where条件的才能插入vt4
GROUP BY: 对vt4按group by字段分组，得到vt5
HAVING：对vt5应用HAVING筛选器只有使 having_condition 为true的组才插入vt6
SELECT：处理select列表产生vt7
DISTINCT：将重复的行从vt7中去除产生vt8
ORDER BY：将vt8的行按order by子句中的列 列表排序生成一个游标vc9
LIMIT（Mysql）: 从vc9的开始处选择指定数量的行生成vt10 并返回调用者

#### 锁原理

首先我们知道InnoDB默认支持的是行锁，但这并不代表InnoDB不支持表锁。必须明白这一点在InnoDB中并不是在数据行上加锁，而是在对应的索引上加锁，这一点和oracle并不同，后者是在数据行上加锁的。这种实现的特点是：只有通过索引条件检索数据的时候加的是行锁，否则加表锁！假如检索条件没有用到索引，也是加表锁！

2、InnoDB锁的特性

在不通过索引条件查询的时候，InnoDB使用的确实是表锁！
由于 MySQL 的行锁是针对索引加的锁,不是针对记录加的锁,所以虽然是访问不同行 的记录,但是如果是使用相同的索引键,是会出现锁冲突的。
当表有多个索引的时候,不同的事务可以使用不同的索引锁定不同的行,另外,不论 是使用主键索引、唯一索引或普通索引,InnoDB 都会使用行锁来对数据加锁。
即便在条件中使用了索引字段,但是否使用索引来检索数据是由 MySQL 通过判断不同 执行计划的代价来决定的,如果 MySQL 认为全表扫 效率更高,比如对一些很小的表,它 就不会使用索引,这种情况下 InnoDB 将使用表锁,而不是行锁。因此,在分析锁冲突时, 别忘了检查 SQL 的执行计划,以确认是否真正使用了索引。

3、使用相同索引值但是不同行引发的冲突，这个的主要原因还是由于Gap Lock（间隙锁）
4、当使用索引检索数据时不同事务可以操作不同行数据

6.Gap Lock

    间隙锁，是在索引的间隙之间加上锁，这是为什么Repeatable Read隔离级别下能防止幻读的主要原因。
    
    幻读：事务A执行了一次读操作，此时事务B在事务A的影响区间内更新了一条数据，此时事务A在执行一次读操作时，会发现出现了不合理的数据。
为什么说gap锁是RR隔离级别下防止幻读的主要原因。

快照读：简单的select操作，没有lock in share mode或for update

当前读：官方文档的术语叫locking read，也就是insert，update，delete,select…in share mode和select…for update，当前读会在所有扫描到的索引记录上加锁，不管它后面的where条件到底有没有命中对应的行记录。

    首先了解到InnoDB索引的数据结构是B+树，其索引是有序性的，（具体原理可以看这篇文章：https://blog.csdn.net/qq_38238296/article/details/88362635 ）如何保证两次当前读返回一致的记录，那就需要在第一次当前读与第二次当前读之间，其他的事务不会插入新的满足条件的记录并提交。注意是当前读。
    
    根据索引的有序性，我们可以从上面的例子推断出满足where条件的数据，只能插入在num=（1,3]U[3,4)两个区间里面，只要我们将这两个区间锁住，那么就不会发生幻读。
用Gap锁的原理来解释的话：因为主键索引和唯一索引的值只有一个，所以满足检索条件的只有一行，故并不会出现幻读，所以并不会加上Gap锁。

#####  MVCC

MySQL InnoDB存储引擎，实现的是基于多版本并发控制协议—MVCC(Multi-Version Concurrency Control) MVCC最大的好处，相信也是耳熟能详：读不加锁，读写不冲突。在读多写少的OLTP应用中，读写不冲突是非常重要的，极大的增加了系统的并发性能，这也是为什么现阶段，几乎所有的RDBMS，都支持了MVCC。

数据库默认隔离级别：**RR（Repeatable Read，可重复读），MVCC主要适用于Mysql的RC,RR隔离级别**

英文全称为Multi-Version Concurrency Control,翻译为中文即 多版本并发控制。在小编看来，他无非就是乐观锁的一种实现方式。在Java编程中，如果把乐观锁看成一个接口，MVCC便是这个接口的一个实现类而已

###### 特点

1.MVCC其实广泛应用于数据库技术，像Oracle,PostgreSQL等也引入了该技术，即适用范围广

2.MVCC并没有简单的使用数据库的行锁，而是使用了行级锁，row_level_lock,而非InnoDB中的innodb_row_lock.

###### 基本原理

MVCC的实现，通过保存数据在某个时间点的快照来实现的。这意味着一个事务无论运行多长时间，在同一个事务里能够看到数据一致的视图。根据事务开始的时间不同，同时也意味着在同一个时刻不同事务看到的相同表里的数据可能是不同的。

###### 基本特征

- 每行数据都存在一个版本，每次数据更新时都更新该版本。
- 修改时Copy出当前版本随意修改，各个事务之间无干扰。
- 保存时比较版本号，如果成功（commit），则覆盖原记录；失败则放弃copy（rollback）

###### InnoDB存储引擎MVCC的实现策略

在每一行数据中额外保存两个隐藏的列：当前行创建时的版本号和删除时的版本号（可能为空，其实还有一列称为回滚指针，用于事务回滚，不在本文范畴）。这里的版本号并不是实际的时间值，而是系统版本号。每开始新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询每行记录的版本号进行比较。

每个事务又有自己的版本号，这样事务内执行CRUD操作时，就通过版本号的比较来达到数据版本控制的目的。

补充：

1.MVCC手段只适用于Msyql隔离级别中的读已提交（Read committed）和可重复读（Repeatable Read）.

2.Read uncimmitted由于存在脏读，即能读到未提交事务的数据行，所以不适用MVCC.

原因是MVCC的创建版本和删除版本只要在事务提交后才会产生。

3.串行化由于是会对所涉及到的表加锁，并非行锁，自然也就不存在行的版本控制问题。

4.通过以上总结，可知，MVCC主要作用于事务性的，有行锁控制的数据库模型。

###### 关于Mysql中MVCC的总结

客观上，我们认为他就是乐观锁的一整实现方式，就是每行都有版本号，保存时根据版本号决定是否成功。

但由于Mysql的写操作会加排他锁（前文有讲），如果锁定了还算不算是MVCC？

了解乐观锁的小伙伴们，都知道其主要依靠版本控制，即消除锁定，二者相互矛盾，so从某种意义上来说，Mysql的MVCC并非真正的MVCC，他只是借用MVCC的名号实现了读的非阻塞而已。



传统RDBMS（关系数据库管理系统）加锁的一个原则，就是2PL (二阶段锁)：Two-Phase Locking。相对而言，2PL比较容易理解，说的是锁操作分为两个阶段：加锁阶段与解锁阶段，并且保证加锁阶段与解锁阶段不相交。下面，仍旧以MySQL为例，来简单看看2PL在MySQL中的实现

transaction         mysql
begin;                 加锁阶段
insert into           加insert对应的锁
update table      加update对应的锁
delete from        加delete对应的锁
commit              解锁阶段
将insert、update、delete的锁全部解开





在InnoDB中，行级锁并不是直接锁记录，而是锁索引。索引分为主键索引和非主键索引两种，如果一条sql语句操作了主键索引，MySQL就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL会先锁定该非主键索引，再锁定相关的主键索引。

当两个事务同时执行，一个锁住了主键索引，在等待其他相关索引。另一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。

避免死锁，这里只介绍常见的三种

如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。
在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率；
对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率；

死锁检测：       

对于死锁，我们可以通过参数 innodb_lock_wait_timeout 根据实际业务场景来设置超时时间，InnoDB引擎默认值是50s。

发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑（默认是开启状态）。

elete
1、删除整张表的数据：

delete from table_name;


2、删除部分数据，添加where子句：

delete from table_name where...;

3、说明 
　　１）、属于DML语言，每次删除一行，都在事务日志中为所删除的每行记录一项。产生rollback，事务提交之后才生效;如果有相应的 trigger,执行的时候将被触发，如果删除大数据量的表速度会很慢。 
　　２）、删除表中数据而不删除表的结构(定义)，同时也不释放空间。

truncate
1、只能操作表，将表中数据全部删除，在功能上和不带where子句的delete语句相同：

truncate table table_name;

2、说明 
　　１)、默认情况下，truncate通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。所以使用的系统和事务日志资源少，可以使用reuse storage; truncate会将高水线复位(回到最开始). 
　　2 )、 truncate是DDL语言, 操作立即生效,自动提交，原数据不放到rollback segment中,不能回滚. 操作不触发trigger. 
　　3 )、删除内容、释放空间但不删除表的结构(定义)。

drop
1、drop语句将删除表的结构，以及被依赖的约束(constrain),触发器(trigger),索引(index);

drop table table_name;

2、说明 
　　1）、删除之后，依赖于该表的存储过程/函数将保留,但是变为invalid状态. 
　　2）、drop也属于DDL语言，立即执行，执行速度最快 
　　3）、删除内容和定义，释放空间。

区别
1、表和索引所占空间： 
　　当表被TRUNCATE 后，这个表和索引所占用的空间会恢复到初始大小； 
　　DELETE操作不会减少表或索引所占用的空间； 
　　DROP语句将表所占用的空间全释放掉。 
　　 
2、应用范围： 
　　TRUNCATE 只能对table； 
　　DELETE可以是table和view。

3、执行速度： 
　　drop > truncate > delete

4、delete from删空表后，会保留一个空的页，truncate在表中不会留有任何页。

5、DELETE语句执行删除的过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。 
　　TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。

6、当使用行锁执行 DELETE 语句时，将锁定表中各行以便删除。truncate始终锁定表和页，而不是锁定各行。

7、如果有identity产生的自增id列，delete from后仍然从上次的数开始增加，即种子不变； 
　使用truncate删除之后，种子会恢复到初始值

总结
1、delete 语句可以使用where子句实现部分删除，而truncate不可以，会将表中的整个数据全部删除，使用时，可以按需求选择； 
2、如果想从表中删除所有的数据，不要使用delete，可以使用truncate语句，因为这样执行速度更快。truncate语句实际是删除原来的表然后重新建立一个新表； 
3、在没有备份情况下，谨慎使用 drop 与 truncate。要删除表结构使用drop; 
4、对于由 FOREIGN KEY 约束引用的表，不能使用 TRUNCATE TABLE，而应使用不带 WHERE 子句的 DELETE 语句。由于 TRUNCATE TABLE 不记录在日志中，所以它不能激活触发器。

#### 什么 情况下数据库索引会失效

1.如果条件中有or，即使其中有条件带索引也不会使用(这也是为什么尽量少用or的原因)

注意：要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引

2.对于多列索引，不是使用的第一部分，则不会使用索引

3.like查询是以%开头

4.如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引

5.如果mysql估计使用全表扫描要比使用索引快,则不使用索引

查看索引的使用情况：

show status like ‘Handler_read%’;

注意：
handler_read_key:这个值越高越好，越高表示使用索引查询到的次数
handler_read_rnd_next:这个值越高，说明查询低效

##### null和空值的区别

1、空值不占空间，NULL值占空间。当字段不为NULL时，也可以插入空值。

2、当使用 IS NOT NULL 或者 IS NULL 时，只能查出字段中没有不为NULL的或者为 NULL 的，不能查出空值。

3、判断NULL 用IS NULL 或者 is not null,SQL 语句函数中可以使用IFNULL()函数来进行处理，判断空字符用 =''或者<>''来进行处理。

4、在进行count()统计某列的记录数的时候，如果采用的NULL值，会别系统自动忽略掉，但是空值是会进行统计到其中的。

5、MySql中如果某一列中含有NULL，那么包含该列的索引就无效了。这一句不是很准确。

6：实际到底是使用NULL值还是空值('')，根据实际业务来进行区分。个人建议在实际开发中如果没有特殊的业务场景，可以直接使用空值

##6. Dubbo

### http和rcp 的区别

- 第一层：应用层。定义了用于在网络中进行通信和传输数据的接口；
- 第二层：表示层。定义不同的系统中数据的传输格式，编码和解码规范等；
- 第三层：会话层。管理用户的会话，控制用户间逻辑连接的建立和中断；
- 第四层：传输层。管理着网络中的端到端的数据传输；
- 第五层：网络层。定义网络设备间如何传输数据；
- 第六层：链路层。将上面的网络层的数据包封装成数据帧，便于物理层传输；
- 第七层：物理层。这一层主要就是传输这些二进制数据。

实际应用过程中，五层协议结构里面是没有表示层和会话层的。应该说它们和应用层合并了。我们应该将重点放在应用层和传输层这两个层面。因为HTTP是应用层协议，而TCP是传输层协议。好，知道了网络的分层模型以后我们可以更好地理解为什么RPC服务相比HTTP服务要Nice一些！

对于在接口不多、系统与系统交互较少的情况下，解决信息孤岛初期常使用的一种通信手段；优点就是简单、直接、开发方便。利用现成的http协议进行传输。我们记得之前本科实习在公司做后台开发的时候，主要就是进行接口的开发，还要写一大份接口文档，严格地标明输入输出是什么？说清楚每一个接口的请求方法，以及请求参数需要注意的事项等。
接口可能返回一个JSON字符串或者是XML文档。然后客户端再去处理这个返回的信息，从而可以比较快速地进行开发。但是对于大型企业来说，内部子系统较多、接口非常多的情况下，RPC框架的好处就显示出来了，首先就是长链接，不必每次通信都要像http一样去3次握手什么的，减少了网络开销；其次就是RPC框架一般都有注册中心，有丰富的监控管理；发布、下线接口、动态扩展等，对调用方来说是无感知、统一化的操作。

#### 总结

RPC服务和HTTP服务还是存在很多的不同点的，一般来说，RPC服务主要是针对大型企业的，而HTTP服务主要是针对小企业的，因为RPC效率更高，而HTTP服务开发迭代会更快。总之，选用什么样的框架不是按照市场上流行什么而决定的，而是要对整个项目进行完整地评估，从而在仔细比较两种开发框架对于整个项目的影响，最后再决定什么才是最适合这个项目的。一定不要为了使用RPC而每个项目都用RPC，而是要因地制宜，具体情况具体分析。

### 1.RPC原理解析

#### 1.1 什么是RPC

RPC(Remote Procedure Call Protocol)——远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP/IP或UDP，为通信程序之间携带信息数据。RPC将原来的本地调用转变为调用远端的服务器上的方法，给系统的处理能力和吞吐量带来了近似于无限制提升的可能。在OSI网络通信模型中，RPC跨域了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。

#### 1.2 RPC架构

一个完整的RPC架构里面包含了四个核心的组件，分别是Client，Client Stub，Server以及Server Stub，这个Stub可以理解为存根。

- 客户端(Client)，服务的调用方。
- 客户端存根(Client Stub)，存放服务端的地址消息，再将客户端的请求参数打包成网络消息，然后通过网络远程发送给服务方。
- 服务端(Server)，真正的服务提供者。
- 服务端存根(Server Stub)，接收客户端发送过来的消息，将消息解包，并调用本地的方法。

#### 1.3 RPC调用过程
(1) 客户端（client）以本地调用方式（即以接口的方式）调用服务；
(2) 客户端存根（client stub）接收到调用后，负责将方法、参数等组装成能够进行网络传输的消息体（将消息体对象序列化为二进制）；
(3) 客户端通过sockets将消息发送到服务端；
(4) 服务端存根( server stub）收到消息后进行解码（将消息对象反序列化）；
(5) 服务端存根( server stub）根据解码结果调用本地的服务；
(6) 本地服务执行并将结果返回给服务端存根( server stub）；
(7) 服务端存根( server stub）将返回结果打包成消息（将结果消息对象序列化）；
(8) 服务端（server）通过sockets将消息发送到客户端；
(9) 客户端存根（client stub）接收到结果消息，并进行解码（将结果消息发序列化）；
(10) 客户端（client）得到最终结果。
RPC的目标是要把2、3、4、7、8、9这些步骤都封装起来。
注意：无论是何种类型的数据，最终都需要转换成二进制流在网络上进行传输，数据的发送方需要将对象转换为二进制流，而数据的接收方则需要把二进制流再恢复为对象。

1、Remoting：远程通讯，提供对多种NIO框架抽象封装，包括“同步转异步”和“请求-响应”模式的信息交换方式。
2、Cluster：服务框架，提供基于接口方法的透明远程过程调用，包括多协议支持，以及软负载均衡，失败容错，地址路由，动态配置等集群支持。
3、Registry：服务注册，基于注册中心目录服务，使服务消费方能动态的查找服务提供方，使地址透明，使服务提供方可以平滑增加或减少机器

Provider: 暴露服务的服务提供方。
Consumer: 调用远程服务的服务消费方。
Registry: 服务注册与发现的注册中心。
Monitor: 统计服务的调用次调和调用时间的监控中心。
Container: 服务运行容器，常见的容器有Spring容器。

调用关系说明：

1. 服务容器负责启动，加载，运行服务提供者。
2. 服务提供者在启动时，向注册中心注册自己提供的服务。
3. 服务消费者在启动时，向注册中心订阅自己所需的服务。
4. 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。
5. 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。
6. 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心Monitor。

Dubbo :是一个rpc框架，soa框架
作为RPC:支持各种传输协议，如dubbo,hession,json,fastjson，底层采用mina,netty长连接进行传输！典型的provider和cusomer模式!
作为SOA:具有服务治理功能，提供服务的注册和发现！用zookeeper实现注册中心！启动时候服务端会把所有接口注册到注册中心，并且订阅configurators,服务消费端订阅provide，configurators,routers,订阅变更时，zk会推送providers,configuators，routers,启动时注册长连接，进行通讯！proveider和provider启动后，后台启动定时器，发送统计数据到monitor！提供各种容错机制和负载均衡策略！

描述一个服务从发布到被消费的详细过程：
一个服务的发布暴露过程：
首先设置一个项目的别名，然后是定义注册中心和设定传输协议，之后定义服务名！服务接口以jar形式导入到provider！
一个服务发布暴露首先由spring的spacehander 把相关的xml或者注解全部转化为springBean,之后通过ServiceConfig.exerp()方法把bean传化为传输所需的url和参数注册到注册中心，发布后provder端的ref(helloImpl)通过protocl(传输协议，如dubboprotocl,hessionprotocl)转化为Invoker对象，即调用信息，包括类，方法，参数等等，再通过proxy操作（代理）如jdkproxy代理转为为Exporter对象，这就是整个的服务暴露过程！
消费过程：
一个Renfence类，通过RenfenceConfig的init 调用proxy的refer方法生产一个invoker,invoker再通过proctol转化成具体的ref(hello),进行消费
首先 ReferenceConfig 类的 init 方法调用 Protocol 的 refer 方法生成 Invoker 实例(如上图中的红色部分)，这是服务消费的关键。接下来把 Invoker 转换为客户端需要的接口(如：HelloWorld)

#### 集群容错

1. Failover cluster  失败的时候自动切换并重试其他服务器。 通过retries=2。 来设置重试次数 默许得failover

2. failfast cluster 快速失败，只发起一次调用  ; 写操作。比如新增记录的时候， 非幂等请求

3. failsafe cluster  失败安全。 出现异常时，直接忽略异常

4. failback cluster 失败自动恢复。 后台记录失败请求，定时重发

5. forking cluster 并行调用多个服务器，只要一个成功就返回。 只能应用在读请求

6. broadcast cluster  广播调用所有提供者，逐个调用。其中一台报错就会返回异常

#### 配置的优先级

消费端优先最高 – 服务端

1. reference method
2. service method
3. reference
4. service 
5. consumer
6. provider

推荐用法
在provider端尽可能配置consumer端的属性
比如timeout、retires、线程池大小、LoadBalance


#### 负载均衡

在集群负载均衡时，Dubbo提供了多种均衡策略，缺省为random随机调用。可以自行扩展负载均衡策略

1. Random LoadBalance
   随机，按权重设置随机概率。
   在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。
2. RoundRobin LoadBalance
   轮循，按公约后的权重设置轮循比率。
   存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。

3. LeastActive LoadBalance
   最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。
   使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。
4. ConsistentHash LoadBalance
   一致性Hash，相同参数的请求总是发到同一提供者。
   当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。

#### dubbo支持的协议：

dubbo、RMI、hessian、webservice、http、Thrift

官方调用过程

首先服务消费者通过代理对象 Proxy 发起远程调用，接着通过网络客户端 Client 将编码后的请求发送给服务提供方的网络层上，也就是 Server。Server 在收到请求后，首先要做的事情是对数据包进行解码。然后将解码后的请求发送至分发器 Dispatcher，再由分发器将请求派发到指定的线程池上，最后由线程池调用具体的服务。这就是一个远程调用请求的发送与接收过程。至于响应的发送与接收过程，这张图中没有表现出来。对于这两个过程，我们也会进行详细分析
在一些比较特殊的网络环境下(网络传输慢,并发多)可能由于服务响应慢,Dubbo自身的超时重试机制(服务端的处理时间超过了设定的超时时间时，就会有重复请求)可能会带来一些麻烦。
常见的应用场景故障:  1、发送邮件(重复) ；2、账户注册(重复).

解决方案:
1.对于核心的服务中心，去除dubbo超时重试机制，并重新评估设置超时时间。
(a),最好接口增加幂等性
(1)、去掉超时重试机制  
    <dubbo:provider delay="-1" timeout="6000"  retries="0"/> 
(2)、重新评估设置超时时间
    <dubbo:service interface="*.*" ref="*"  timeout="延长服务时间"/>
2.业务处理代码必须放在服务端，客户端只做参数验证和服务调用，不涉及业务流程处理

1、去掉dubbo提供的重试机制，让dubbo提供服务层如果出现超时就抛出异常，那么将不会出现数据不一致的问题，但是这里有个问题，是否所有的可能出现数据重复的服务都采用这个方式，那么dubbo重试机制的意义在哪里呢 ？
2、采用幂等性操作，在调用方创建一个唯一ID，当订单服务端存储的时候发现id已经存在，由orm层跑错（如果在程序中判断是否已经存在，可能存在超时服务还没有insert的时候），然后将不会存储相同的数据。但是这种方法还是感觉怪怪的。
请问对这个问题有没有优雅的解决方法 ？



#### Dubbo服务之优雅停机

在Java程序中可以通过添加关闭钩子，实现在程序退出时关闭资源、平滑退出的功能。
使用Runtime.addShutdownHook(Thread hook)方法，可以注册一个JVM关闭的钩子，这个钩子可以在以下几种场景被调用：

1. 程序正常退出
2. 使用System.exit()
3. 终端使用Ctrl+C触发的中断
4. 系统关闭
5. 使用Kill pid命令干掉进程


## 7. spring

### 1. bean 的加载过程
抛开一些细节处理和扩展功能，一个 Bean 的创建过程无非是：
获取完整定义 -> 实例化 -> 依赖注入 -> 初始化 -> 类型转换。
作为一个完善的框架，Spring 需要考虑到各种可能性，还需要考虑到接入的扩展性。
所以有了复杂的循环依赖的解决，复杂的有参数构造器的匹配过程，有了 BeanPostProcessor 来对实例化或初始化的 Bean 进行扩展修改。
先有个整体设计的思维，再逐步击破针对这些特殊场景的设计，整个 Bean 加载流程迎刃而解。

Spring 的工作流进行一个粗略的概括，主要为两大环节：Spring 作为 Ioc 框架，实现了依赖注入，由一个中心化的 Bean 工厂来负责各个 Bean 的实例化和依赖管理。各个 Bean 可以不需要关心各自的复杂的创建过程，达到了很好的解耦效果。

- **解析**，读 xml 配置，扫描类文件，从配置或者注解中获取 Bean 的定义信息，注册一些扩展功能。
- **加载**，通过解析完的定义信息获取 Bean 实例。

从 getBean 的调用链创建的流程图，为了能够很好地理解 Bean 加载流程，省略一些异常、日志和分支处理和一些特殊条件的判断。

从上面的流程图中，可以看到一个 Bean 加载会经历这么几个阶段（用绿色标记）：

- **获取 BeanName**，对传入的 name 进行解析，转化为可以从 Map 中获取到 BeanDefinition 的 bean name。
- **合并 Bean 定义**，对父类的定义进行合并和覆盖，如果父类还有父类，会进行递归合并，以获取完整的 Bean 定义信息。
- **实例化**，使用构造或者工厂方法创建 Bean 实例。
- **属性填充**，寻找并且注入依赖，依赖的 Bean 还会递归调用 `getBean` 方法获取。
- **初始化**，调用自定义的初始化方法。
- **获取最终的 Bean**，如果是 FactoryBean 需要调用 getObject 方法，如果需要类型转换调用 TypeConverter 进行转化。

整个流程最为复杂的是对循环依赖的解决方案，后续会进行重点分析。

spring通过AbstractBeanFactory.getBean(..)创建bean的整个过程中，如果是singleton 会优先通过
SingletonBeanRegistry.getSingleton(..)来获取单例对象，因此spring中的单例模式并不是使用设计
模式中常用的方法来实现的，而是在集合中维护，并通过同步（synchronization）保证对象只会创建一份来实现的，如果不不存在，则会执行对象创建的逻辑，而由于spring提供的是一个IOC容器，不止于创建对象，还包括对象依赖、声明周期（init、destroy）管理以及对象各个阶段的回调拓展（BeanPostProcessor系列接口）等等
由于在对象执行初始化之前，要将对象实例化、依赖注入完毕，因此就会有循环引用的问题。
比如单例的循环依赖：
A实例的初始化 在依赖于B对象的实例（不是构造器上的依赖关系，而是成员属性中的依赖关系，构造器这种依赖关系是无解的）
        class A{
            B b;
            //setter..
        }
        class B {
            A a;
            //setter..
        }
这种关系在spring中的创建过程是：
先根据A的无参构造器创建出A 在执行初始化方法前 会先进行getBean(B) 注入到A中，这时 会触发
B对象的创建，而B在初始化方法前，也会注入A 这时getBean(A) 能获取到A实例的earlyExpose的一个引用，因此B注入成功
这时B进行初始化完毕，A 注入成功，A初始化完毕。
prototype类型的对象这样的循环依赖就比较容易了，最终会产生 （A1 -> B -> A2） 3个对象。而单例对象 由于只能存在2个对象所以才需要特殊处理一下来实现。

####  Spring DI 循环依赖

1.所有单例的bean在创建完成之前都会提早曝光。

2.提早曝光就是预先为这个bean创建好一个ObjectFactory。一旦发现是循环依赖。就调用ObjectFactory.getObject返回实例。

3.Spring判断循环依赖的条件是
earlySingletonObjects里是否存在这个ObjectFactory.getObject生成的object。（或者另外一个map,两个互斥的）

singletonObjects：第一级缓存，里面放置的是实例化好的单例对象；

earlySingletonObjects：第二级缓存，里面存放的是提前曝光的单例对象；

singletonFactories：第三级缓存，里面存放的是要被实例化的对象的对象工厂。

所以当一个Bean调用构造函数进行实例化后，即使属性还未填充，就可以通过三级缓存向外暴露依赖的引用值（所以循环依赖问题的解决也是基于Java的引用传递），这也说明了另外一点，基于构造函数的注入，如果有循环依赖，Spring是不能够解决的。还要说明一点，Spring默认的Bean Scope是单例的，而三级缓存中都包含singleton，可见是对于单例Bean之间的循环依赖的解决，Spring是通过三级缓存来实现的。


### 2. @Resource和@Autowired区别

@Resource和@Autowired都可以用来装配bean，都可以用于字段或setter方法。
@Autowired默认按类型装配，默认情况下必须要求依赖对象必须存在，如果要允许null值，可以设置它的required属性为false。
@Resource默认按名称装配，当找不到与名称匹配的bean时才按照类型进行装配。名称可以通过name属性指定，如果没有指定name属性，当注解写在字段上时，默认取字段名，当注解写在setter方法上时，默认取属性名进行装配。
注意：如果name属性一旦指定，就只会按照名称进行装配。
@Autowire和@Qualifier配合使用效果和@Resource一样：
```java
@Autowired(required = false) @Qualifier("example")
private Example example;

@Resource(name = "example")
private Example example;
```
@Resource装配顺序
1. 如果同时指定name和type，则从容器中查找唯一匹配的bean装配，找不到则抛出异常
2. 如果指定name属性，则从容器中查找名称匹配的bean装配，找不到则抛出异常
3. 如果指定type属性，则从容器中查找类型唯一匹配的bean装配，找不到或者找到多个抛出异常
4. 如果都不指定，则自动按照byName方式装配，如果没有匹配，则回退一个原始类型进行匹配，如果匹配则自动装配

| 注解对比 | @Resource  | @Autowire  |
| -------- | ---------- | ---------- |
| 注解来源 | JDK        | Spring     |
| 装配方式 | 优先按名称 | 优先按类型 |
| 属性     | name、type | required   |

### 3. 编程式和声明式事务的区别

Spring提供了对编程式事务和声明式事务的支持，编程式事务允许用户在代码中精确定义事务的边界，而声明式事务（基于AOP）有助于用户将操作与事务规则进行解耦。 
简单地说，编程式事务侵入到了业务代码里面，但是提供了更加详细的事务管理；而声明式事务由于基于AOP，所以既能起到事务管理的作用，又可以不影响业务代码的具体实现。 声明式。aop。注解 

#### 3.2 如何实现编程式事务？

Spring提供两种方式的编程式事务管理，分别是：使用TransactionTemplate和直接使用PlatformTransactionManager。

##### 3.2.1 使用TransactionTemplate

采用TransactionTemplate和采用其他Spring模板，如JdbcTempalte和HibernateTemplate是一样的方法。它使用回调方法，把应用程序从处理取得和释放资源中解脱出来。如同其他模板，TransactionTemplate是线程安全的。代码片段：

```java
TransactionTemplate tt = new TransactionTemplate(); // 新建一个TransactionTemplate
Object result = tt.execute(
  new TransactionCallback(){  
    public Object doTransaction(TransactionStatus status){  
      updateOperation();  
      return resultOfUpdateOperation();  
    }  
 	}); // 执行execute方法进行事务管理
```

##### 3.2.2使用PlatformTransactionManager

示例代码如下：

```java
DataSourceTransactionManager dataSourceTransactionManager = new DataSourceTransactionManager(); //定义一个某个框架平台的TransactionManager，如JDBC、Hibernate
dataSourceTransactionManager.setDataSource(this.getJdbcTemplate().getDataSource()); // 设置数据源
DefaultTransactionDefinition transDef = new DefaultTransactionDefinition(); // 定义事务属性
transDef.setPropagationBehavior(DefaultTransactionDefinition.PROPAGATION_REQUIRED); // 设置传播行为属性
TransactionStatus status = dataSourceTransactionManager.getTransaction(transDef); // 获得事务状态
try {
  // 数据库操作
  dataSourceTransactionManager.commit(status);// 提交
} catch (Exception e) {
  dataSourceTransactionManager.rollback(status);// 回滚
}
```

### 4. BeanFactory.和FactoryBean区别？

FactoryBean 是spring内部实现的一种规范&开头作为BeanName

Spring中的所有容器都是FactoryBean 因为容器本身也是容器管理的，有root创建 都是单例放在IOC容器中

BeanFactory 是Bean工厂的顶层规范，只规定了getBean()方法

### 5. springboot starter原理

Spring根路径下建立META-INF/spring.factories文件，并声明配置项路径

```spring.factories
org.springframework.boot.autoconfigure.EnableAutoConfiguration=\
com.le.start.diamond.DiamondAutoConfiguration
```

@configuration @componentScan 等

SpringContext创初始化 刷新等 refresh（）激活

@EnableAutoConfiguration 开启配置，约定大与配置 EnablerAutoConfigurationImportSelector 作为Starter自动化倒入的关键选项，  isEnabled（）函数   调用getCandidateConfigurations函数 这个函数里面明确了 加载META-INF/spring.factories文件的东西

也就是说Spring启动的是你会扫描说有JAR中的spring.factories定义的类

ConfigurationClassPostProcessor作为Spring扩展点是 SpringBoot一系列功能的基础入口。

### 6. Springboot main 做了什么？
SpringTestApplication类执行main方法，main方法调用SpringApplication的run方法。
run方法干了两件事：

1. 创建SpringApplication对象
2. 利用创建好的SpringApplication对象调用run方法
在执行SpringApplication的run方法时首先会创建一个SpringApplication类的对象，利用构造方法创建SpringApplication对象时会调用initialize方法
```java
private void initialize(Object[] sources) {
    // 在sources不为空时，保存配置类
    if (sources != null && sources.length > 0) {
        this.sources.addAll(Arrays.asList(sources));
    }
    // 判断是否为web应用
    this.webEnvironment = deduceWebEnvironment();
    // 获取并保存容器初始化类，通常在web应用容器初始化使用
    // 利用loadFactoryNames方法从路径MEAT-INF/spring.factories中找到所有的ApplicationContextInitializer
    setInitializers((Collection) getSpringFactoriesInstances(
        ApplicationContextInitializer.class));
    // 获取并保存监听器
    // 利用loadFactoryNames方法从路径MEAT-INF/spring.factories中找到所有的ApplicationListener
    setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class));
    // 从堆栈信息获取包含main方法的主配置类
    this.mainApplicationClass = deduceMainApplicationClass();
  
}
```

###### 调用run方法

```java

public ConfigurableApplicationContext run(String... args) {
    StopWatch stopWatch = new StopWatch();
    stopWatch.start();
    ConfigurableApplicationContext context = null;
    FailureAnalyzers analyzers = null;
    // 配置属性
    configureHeadlessProperty();
    // 获取监听器
    // 利用loadFactoryNames方法从路径MEAT-INF/spring.factories中找到所有的SpringApplicationRunListener
    SpringApplicationRunListeners listeners = getRunListeners(args);
    // 启动监听
    // 调用每个SpringApplicationRunListener的starting方法
    listeners.starting();
    try {
        // 将参数封装到ApplicationArguments对象中
        ApplicationArguments applicationArguments = new DefaultApplicationArguments(
            args);
        // 准备环境
        // 触发监听事件——调用每个SpringApplicationRunListener的environmentPrepared方法
        ConfigurableEnvironment environment = prepareEnvironment(listeners,
            applicationArguments);
        // 从环境中取出Banner并打印
        Banner printedBanner = printBanner(environment);
        // 依据是否为web环境创建web容器或者普通的IOC容器
        context = createApplicationContext();
        analyzers = new FailureAnalyzers(context);
        // 准备上下文
        // 1.将environment保存到容器中
        // 2.触发监听事件——调用每个SpringApplicationRunListeners的contextPrepared方法
        // 3.调用ConfigurableListableBeanFactory的registerSingleton方法向容器中注入applicationArguments与printedBanner
        // 4.触发监听事件——调用每个SpringApplicationRunListeners的contextLoaded方法
        prepareContext(context, environment, listeners, applicationArguments,
            printedBanner);
        // 刷新容器，完成组件的扫描，创建，加载等
        refreshContext(context);
        afterRefresh(context, applicationArguments);
        // 触发监听事件——调用每个SpringApplicationRunListener的finished方法
        listeners.finished(context, null);
        stopWatch.stop();
        if (this.logStartupInfo) {
            new StartupInfoLogger(this.mainApplicationClass)
                .logStarted(getApplicationLog(), stopWatch);
        }
        // 返回容器
        return context;
    }
    catch (Throwable ex) {
        handleRunFailure(context, listeners, analyzers, ex);
        throw new IllegalStateException(ex);
    }
}
```

SpringApplication.run一共做了两件事

创建SpringApplication对象；在对象初始化时保存事件监听器，容器初始化类以及判断是否为web应用，保存包含main方法的主配置类。
调用run方法；准备spring的上下文，完成容器的初始化，创建，加载等。会在不同的时机触发监听器的不同事件。

### 7. 面试Spring之bean的生命周期]
找工作的时候有些人会被问道Spring中Bean的生命周期，其实也就是考察一下对Spring是否熟悉，工作中很少用到其中的内容，那我们简单看一下。
在说明前可以思考一下Servlet的生命周期：实例化，初始init，接收请求service，销毁destroy；
Spring上下文中的Bean也类似，如下
1、实例化一个Bean－－也就是我们常说的new；
2、按照Spring上下文对实例化的Bean进行配置－－也就是IOC注入；
3、如果这个Bean已经实现了BeanNameAware接口，会调用它实现的setBeanName(String)方法，此处传递的就是Spring配置文件中Bean的id值
4、如果这个Bean已经实现了BeanFactoryAware接口，会调用它实现的setBeanFactory(setBeanFactory(BeanFactory)传递的是Spring工厂自身（可以用这个方式来获取其它Bean，只需在Spring配置文件中配置一个普通的Bean就可以）；
5、如果这个Bean已经实现了ApplicationContextAware接口，会调用setApplicationContext(ApplicationContext)方法，传入Spring上下文（同样这个方式也可以实现步骤4的内容，但比4更好，因为ApplicationContext是BeanFactory的子接口，有更多的实现方法）；
6、如果这个Bean关联了BeanPostProcessor接口，将会调用postProcessBeforeInitialization(Object obj, String s)方法，BeanPostProcessor经常被用作是Bean内容的更改，并且由于这个是在Bean初始化结束时调用那个的方法，也可以被应用于内存或缓存技术；
7、如果Bean在Spring配置文件中配置了init-method属性会自动调用其配置的初始化方法。
8、如果这个Bean关联了BeanPostProcessor接口，将会调用postProcessAfterInitialization(Object obj, String s)方法、；
注：以上工作完成以后就可以应用这个Bean了，那这个Bean是一个Singleton的，所以一般情况下我们调用同一个id的Bean会是在内容地址相同的实例，当然在Spring配置文件中也可以配置非Singleton，这里我们不做赘述。
9、当Bean不再需要时，会经过清理阶段，如果Bean实现了DisposableBean这个接口，会调用那个其实现的destroy()方法；
10、最后，如果这个Bean的Spring配置中配置了destroy-method属性，会自动调用其配置的销毁方法。
以上10步骤可以作为面试或者笔试的模板，另外我们这里描述的是应用Spring上下文Bean的生命周期，如果应用Spring的工厂也就是BeanFactory的话去掉第5步就Ok了。

## 8. Zookeeper 概念 

Zookeeper 是一个分布式协调服务，可用于服务发现，分布式锁，分布式领导选举，配置管理等。 Zookeeper 提供了一个类似于 Linux 文件系统的树形结构(可认为是轻量级的内存文件系统，但 只适合存少量信息，完全不适合存储大量文件或者大文件)，同时提供了对于每个节点的监控与 通知机制。 

Zookeeper 角色
 Zookeeper 集群是一个基于主从复制的高可用集群，每个服务器承担如下三种角色中的一种 

Leader

1. 一个 Zookeeper 集群同一时间只会有一个实际工作的 Leader，它会发起并维护与各 Follwer 及 Observer 间的心跳。 

2. 所有的写操作必须要通过 Leader 完成再由 Leader 将写操作广播给其它服务器。只要有超过 半数节点(不包括 observeer 节点)写入成功，该写请求就会被提交(类 2PC 协议)。 

Follower

1. 一个 Zookeeper 集群可能同时存在多个 Follower，它会响应 Leader 的心跳， 

2. Follower 可直接处理并返回客户端的读请求，同时会将写请求转发给 Leader 处理， 

3. 并且负责在 Leader 处理写请求时对请求进行投票。 

Observer

角色与 Follower 类似，但是无投票权。Zookeeper 需保证高可用和强一致性，为了支持更多的客 户端，需要增加更多 Server;Server 增多，投票阶段延迟增大，影响性能;引入 Observer， Observer 不参与投票; Observers 接受客户端的连接，并将写请求转发给 leader 节点; 加入更 多 Observer 节点，提高伸缩性，同时不影响吞吐率。 

ZAB协议

事务编号 **Zxid**(事务请求计数器**+ epoch**) 

在 ZAB ( ZooKeeper Atomic Broadcast , ZooKeeper 原子消息广播协议) 协议的事务编号 Zxid 设计中，Zxid 是一个 64 位的数字，其中低 32 位是一个简单的单调递增的计数器，针对客户端每 一个事务请求，计数器加 1;而高 32 位则代表 Leader 周期 epoch 的编号，每个当选产生一个新 的 Leader 服务器，就会从这个 Leader 服务器上取出其本地日志中最大事务的 ZXID，并从中读取 epoch 值，然后加 1，以此作为新的 epoch，并将低 32 位从 0 开始计数。 

Zxid(Transaction id)类似于 RDBMS 中的事务 ID，用于标识一次更新操作的 Proposal(提议) ID。为了保证顺序性，该 zkid 必须单调递增。 

**epoch** 

epoch:可以理解为当前集群所处的年代或者周期，每个 leader 就像皇帝，都有自己的年号，所 以每次改朝换代，leader 变更之后，都会在前一个年代的基础上加 1。这样就算旧的 leader 崩溃 恢复之后，也没有人听他的了，因为 follower 只听从当前年代的 leader 的命令。 

**Zab** 协议有两种模式**-**恢复模式(选主)、广播模式(同步) 

Zab 协议有两种模式，它们分别是恢复模式(选主)和广播模式(同步)。当服务启动或者在领导 者崩溃后，Zab 就进入了恢复模式，当领导者被选举出来，且大多数 Server 完成了和 leader 的状 态同步以后，恢复模式就结束了。状态同步保证了 leader 和 Server 具有相同的系统状态。 

**ZAB**协议**4**阶段
 **Leader election**(选举阶段**-**选出准 **Leader**) 

1. Leader election(选举阶段):节点在一开始都处于选举阶段，只要有一个节点得到超半数 节点的票数，它就可以当选准 leader。只有到达 广播阶段(broadcast) 准 leader 才会成 为真正的 leader。这一阶段的目的是就是为了选出一个准 leader，然后进入下一个阶段。 

**Discovery**(发现阶段**-**接受􏰀议、生成 **epoch**、接受 **epoch**) 

2. Discovery(发现阶段):在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。这个一阶段的主要目的是发现当前大多数节点接收的最新提议，并且 准 leader 生成新的 epoch，让 followers 接受，更新它们的 accepted Epoch 

一个 follower 只会连接一个 leader，如果有一个节点 f 认为另一个 follower p 是 leader，f 在尝试连接 p 时会被拒绝，f 被拒绝之后，就会进入重新选举阶段。 

3. Synchronization(同步阶段**-**同步 **follower** 副本) 
Synchronization(同步阶段):同步阶段主要是利用 leader 前一阶段获得的最新提议历史， 同步集群中所有的副本。只有当 大多数节点都同步完成，准 leader 才会成为真正的 leader。 follower 只会接收 zxid 比自己的 lastZxid 大的提议。 

**Broadcast**(广播阶段**-leader** 消息广播) Broadcast(广播阶段):到了这个阶段，Zookeeper 集群才能正式对外提供事务服务， 

4.ZAB 提交事务并不像 2PC 一样需要全部 follower 都 ACK，只需要得到超过半数的节点的 ACK 就 

并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。 可以了。 

**ZAB** 协议 **JAVA** 实现(**FLE-****发现阶段和同步合并为** **Recovery Phase****(恢复阶段)**) 

协议的 Java 版本实现跟上面的定义有些不同，选举阶段使用的是 Fast Leader Election(FLE)， 它包含了 选举的发现职责。因为 FLE 会选举拥有最新提议历史的节点作为 leader，这样就省去了 发现最新提议的步骤。实际的实现将 发现阶段 和 同步合并为 Recovery Phase(恢复阶段)。所 以，ZAB 的实现只有三个阶段:Fast Leader Election;Recovery Phase;Broadcast Phase。 

投票机制

每个 sever 首先给自己投票，然后用自己的选票和其他 sever 选票对比，权重大的胜出，使用权 重较大的更新自身选票箱。具体选举过程如下: 
1. 每个 Server 启动以后都询问其它的 Server 它要投票给谁。对于其他 server 的询问， server 每次根据自己的状态都回复自己推荐的 leader 的 id 和上一次处理事务的 zxid(系 统启动时每个 server 都会推荐自己)
2. 收到所有 Server 回复以后，就计算出 zxid 最大的哪个 Server，并将这个 Server 相关信 息设置成下一次要投票的 Server。
3. 计算这过程中获得票数最多的的 sever 为获胜者，如果获胜者的票数超过半数，则改 server 被选为 leader。否则，继续这个过程，直到 leader 被选举出来
4. leader 就会开始等待 server 连接
5. Follower 连接 leader，将最大的 zxid 发送给 leader
6. Leader 根据 follower 的 zxid 确定同步点，至此选举阶段完成。
7. 选举阶段完成 Leader 同步后通知 follower 已经成为 uptodate 状态
8. Follower 收到 uptodate 消息后，又可以重新接受 client 的请求进行服务了

目前有 5 台服务器，每台服务器均没有数据，它们的编号分别是 1,2,3,4,5,按编号依次启动，它们 的选择举过程如下: 

1. 服务器 1 启动，给自己投票，然后发投票信息，由于其它机器还没有启动所以它收不到反 馈信息，服务器 1 的状态一直属于 Looking。
2. 服务器 2 启动，给自己投票，同时与之前启动的服务器 1 交换结果，由于服务器 2 的编号 大所以服务器 2 胜出，但此时投票数没有大于半数，所以两个服务器的状态依然是 LOOKING。
3. 服务器 3 启动，给自己投票，同时与之前启动的服务器 1,2 交换信息，由于服务器 3 的编 号最大所以服务器 3 胜出，此时投票数正好大于半数，所以服务器 3 成为领导者，服务器 1,2 成为小弟。
4. 服务器 4 启动，给自己投票，同时与之前启动的服务器 1,2,3 交换信息，尽管服务器 4 的
编号大，但之前服务器 3 已经胜出，所以服务器 4 只能成为小弟。 5. 服务器 5 启动，后面的逻辑同服务器 4 成为小弟。


Zookeeper 工作原理(原子广播) 
1. Zookeeper 的核心是原子广播，这个机制保证了各个 server 之间的同步。实现这个机制
的协议叫做 Zab 协议。Zab 协议有两种模式，它们分别是恢复模式和广播模式。
2. 当服务启动或者在领导者崩溃后，Zab 就进入了恢复模式，当领导者被选举出来，且大多
数 server 的完成了和 leader 的状态同步以后，恢复模式就结束了。
3. 状态同步保证了 leader 和 server 具有相同的系统状态
4. 一旦 leader 已经和多数的 follower 进行了状态同步后，他就可以开始广播消息了，即进
入广播状态。这时候当一个 server 加入 zookeeper 服务中，它会在恢复模式下启动，发 现 leader，并和 leader 进行状态同步。待到同步结束，它也参与消息广播。Zookeeper 服务一直维持在 Broadcast 状态，直到 leader 崩溃了或者 leader 失去了大部分的 followers 支持。
5. 广播模式需要保证 proposal 被按顺序处理，因此 zk 采用了递增的事务 id 号(zxid)来保 证。所有的提议(proposal)都在被提出的时候加上了 zxid。
6. 实现中 zxid 是一个 64 为的数字，它高 32 位是 epoch 用来标识 leader 关系是否改变， 每次一个 leader 被选出来，它都会有一个新的 epoch。低 32 位是个递增计数。
7. 当 leader 崩溃或者 leader 失去大多数的 follower，这时候 zk 进入恢复模式，恢复模式 需要重新选举出一个新的 leader，让所有的 server 都恢复到一个正确的状态。

Znode 有四种形式的目录节点 

1. PERSISTENT:持久的节点。
2. EPHEMERAL:暂时的节点。
3. PERSISTENT_SEQUENTIAL:持久化顺序编号目录节点。 
4. EPHEMERAL_SEQUENTIAL:暂时化顺序编号目录节点。 

## 9.分布式缓存 

**24.1.1.** 缓存雪崩 
缓存雪崩我们可以简单的理解为:由于原有缓存失效，新缓存未到期间所有原本应该访问缓存的请求都 去查询数据库了，而对数据库 CPU 和内存造成巨大压力，严重的会造成数据库宕机。从而形成一系列 连锁反应，造成整个系统崩溃。一般有三种处理办法: 

1. 一般并发量不是特别多的时候，使用最多的解决方案是加锁排队。
2. 给每一个缓存数据增加相应的缓存标记，记录缓存的是否失效，如果缓存标记失效，则更新数据缓 

存。
 3. 为 key 设置不同的缓存失效时间。 

**24.1.2.** 缓存穿透 

缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有。这样就导致用户查询的时候，在 缓存中找不到，每次都要去数据库再查询一遍，然后返回空(相当于进行了两次无用的查询)。这样请 求就绕过缓存直接查数据库，这也是经常提的缓存命中率问题。 

有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈 希到一个足够大的 bitmap 中，一个一定不存在的数据会被这个 bitmap 拦截掉，从而避免了对底层存 储系统的查询压力。另外也有一个更为简单粗暴的方法，如果一个查询返回的数据为空(不管是数据不 存在，还是系统故障)，我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。 通过这个直接设置的默认值存放到缓存，这样第二次到缓冲中获取就有值了，而不会继续访问数据库。 
**24.1.3.** 缓存预热 缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候， 
先查询数据库，然后再将数据缓存的问题!用户直接查询事先被预热的缓存数据! 
**24.1.4.** 缓存更新
 缓存更新除了缓存服务器自带的缓存失效策略之外(Redis 默认的有 6 中策略可供选择)，我们还可以 
根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种: (1)定时去清理过期的缓存; 
(2)当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数 据并更新缓存。 

**24.1.5.** 缓存降级 
当访问量剧增、服务出现问题(如响应时间慢或不响应)或非核心服务影响到核心流程的性能时，仍然 需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开 关实现人工降级。降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的 (如加入购物车、结算)。 

## 10. hashmap

hashMap内部的数据结构原理？

数组+链表 （单向链表） 验证：Node<k,v>{key, value,Node<k,v> next} 内部类  next 单向链表。一个属性 node[ ] table。数组 

hash算法作用：为了Node节点落点的一个前戏运算

落点数组，默认大小16 位移运算 最高。1<<4  

一定要得到一个0-15之间 16个整型 的一个数组下标

引进一个hash算法 得到数组的下标 对key.hashCode(). int 32位

高16位和底16位进行一个异或运算。  得到的结果就是hash 算法

hash算法： 用key.hashCode（）高16位和底16位 异或运算

3. 请你描述一下put 的流程

   1. 先根据hash算法得到了一个结果result

   判断了一下当前Node节点数组是否为空，如果为空，初始化数组

   将数组的默认大小16 赋值给newCap16*

   2. 初始化数组大小

   3. 根据hash算法值&n-1， 得到数组下标，然后在判断
      根据key，value值 组装成Node节点  对hash值。& 运算（数组长度-1）01111   相对于15取模 效率更好

   4. 得到了数组下标值不为空，

      1. key值相同，值覆盖

      2. 同时下面是链表， 红黑树（jdk1.8引入） 大于8个节点 就转换为红黑树

      3. 判断数组容量大小的改变
         链表不断增加 变为红黑树
         数组0.75*数组长度 就进行扩容， 0.75扩容因子。扩容的标准 resize（） 常量size++ 
         resize 另外一个功能 扩容。初始化
         数组的大小变大，2倍扩容  让数组的大小 保持2 n次幂  最大 2的30次方
         
         将原来的数组移动的新的数组上
         
         	1.  数组有元素 ， 下面为空。对hash 重新 异或 size-1 
          	2.  数组有元素，下面不null，链表 判断10000 老数组长度 & 是否为0 倒数第5位是否为0 
               链表节点的位置，1 原来的位置，2. 原来的位置+oldCap16
          	3.  数组有元素下面不null ，红黑树   split 重新拆分	

TreeNode 红黑树 jdk1.8 内部类

为什么要 2的倍数？

这里，因为hashmap求数组位置的时候都是直接hashcode&数组大小，以前我只是想到一个数&一个数肯定会小，可以使得index在size之内取值。没有考虑到hash碰撞的问题，后来看到一篇博客后才理解。

首先取模操作时可以保证均分的，但是取模操作性能比较差，所以hashMap使用了近乎取模的&，并且是与上一个size-1的数，达成了变相取模的操作。

但是这时候，size是不是2的 n次方倍就至关重要了。

举例如下，如果数组大小是15（非2次方幂）和16（2次方幂）的区别。

1110（15-1）&hashcode

1111（16-1）&hashcode

假设hashcode从 1 到30，我们来看看结果:


可以明显的看出来，在15的时候，这种&操作代替取模操作的特点不存在，也就是说，不能保证hashcode对应的key放到数组均分,size = 15的时候，明显所有尾数带1的数组位置上都不会有值。而后者 16 就可以保证均分，避免了摸操作，又均匀分配。这就是为什么hashMap的 size 一定是2的 n次方倍

2、ConcurrentHashMap使用什么技术来保证线程安全？
jdk1.7：Segment+HashEntry来进行实现的；
jdk1.8：放弃了Segment臃肿的设计，采用Node+CAS+Synchronized来保证线程安全；

3、ConcurrentHashMap的get方法是否要加锁，为什么？
不需要，get方法采用了unsafe方法，来保证线程安全。
4、ConcurrentHashMap迭代器是强一致性还是弱一致性？HashMap呢？
弱一致性，hashmap强一直性。
ConcurrentHashMap可以支持在迭代过程中，向map添加新元素，而HashMap则抛出了ConcurrentModificationException，
因为HashMap包含一个修改计数器，当你调用他的next()方法来获取下一个元素时，迭代器将会用到这个计数器。
5、ConcurrentHashMap1.7和1.8的区别：

jdk1.8的实现降低锁的粒度，jdk1.7锁的粒度是基于Segment的，包含多个HashEntry，而jdk1.8锁的粒度就是Node

数据结构：jdk1.7 Segment+HashEntry；jdk1.8 数组+链表+红黑树+CAS+synchronized

ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入锁ReentrantLock，在ConcurrentHashMap里扮演锁的角色，HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组，Segment的结构和HashMap类似，是一种数组和链表结构， 一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素， 每个Segment守护者一个HashEntry数组里的元素,当对HashEntry数组的数据进行修改时，必须首先获得它对应的Segment锁

ConcurrentHashMap(1.7及之前)中主要实体类就是三个：ConcurrentHashMap（整个Hash表）,Segment（桶），HashEntry（节点）

get操作不需要锁。
除非读到的值是空的才会加锁重读，我们知道HashTable容器的get方法是需要加锁的，那么ConcurrentHashMap的get操作是如何做到不加锁的呢？原因是它的get方法里将要使用的共享变量都定义成volatile

static final class Segment<K,V> extends ReentrantLock implements Serializable {}

 V remove(Object key, int hash, Object value) {  
 lock(); 
 ...
 ```java
 V put(K key, int hash, V value, boolean onlyIfAbsent) {  
     lock();  
     try {  
         int c = count;  
         if (c++ > threshold) // ensure capacity  
             rehash();  
         HashEntry<K,V>[] tab = table;  
         int index = hash & (tab.length - 1);  
         HashEntry<K,V> first = tab[index];  
         HashEntry<K,V> e = first;  
         while (e != null && (e.hash != hash || !key.equals(e.key)))  
             e = e.next;  
         V oldValue;  
         if (e != null) {  
             oldValue = e.value;  
             if (!onlyIfAbsent)  
                 e.value = value;  
         }  
         else {  
             oldValue = null;  
             ++modCount;  
             tab[index] = new HashEntry<K,V>(key, hash, first, value);  
             count = c; // write-volatile  
         }  
         return oldValue;  
     } finally {  
         unlock();  
     }  
 }
 ```

###### hashmap产生死锁的详解

关键就在于rehash过程。在前面我们说了是 HashMap 的**get()方法造成的死锁。**既然是 get()造成的死锁，一定是跟put()进去元素的位置有关，所以我们从 put()方法开始看起。

1. 对索引数组中的元素遍历
2. 对链表上的每一个节点遍历：用 next 取得要转移那个元素的下一个，将 e 转移到新 Hash 表的头部，因为可能有元素，所以先将 e.next 指向新 Hash 表的第一个元素（如果是第一次就是 null)，这时候新 Hash 的第一个元素是 e，但是 Hash 指向的却是 e 没转移时候的第一个，所以需要将 Hash 表的第一个元素指向 e
3. 循环2，直到链表节点全部转移
4. 循环1，直到所有索引数组全部转移 
   经过这几步，我们会发现转移的时候是逆序的。假如转移前链表顺序是1->2->3，那么转移后就会变成3->2->1。这时候就有点头绪了，死锁问题不就是因为1->2的同时2->1造成的吗？所以，**HashMap 的死锁问题就出在这个transfer()函数上**。



jdk版本1.7 Hashmap的扩容需要满足两个条件：**当前数据存储的数量（即size()）大小必须大于等于阈值；当前加入的数据是否发生了hash冲突。**

因为上面这两个条件，所以存在下面这些情况

（1）、就是hashmap在存值的时候（默认大小为16，负载因子0.75，阈值12），可能达到最后存满16个值的时候，再存入第17个值才会发生扩容现象，因为前16个值，每个值在底层数组中分别占据一个位置，并没有发生hash碰撞。

（2）、当然也有可能存储更多值（超多16个值，最多可以存26个值）都还没有扩容。原理：前11个值全部hash碰撞，存到数组的同一个位置（这时元素个数小于阈值12，不会扩容），后面所有存入的15个值全部分散到数组剩下的15个位置（这时元素个数大于等于阈值，但是每次存入的元素并没有发生hash碰撞，所以不会扩容），前面11+15=26，所以在存入第27个值的时候才同时满足上面两个条件，这时候才会发生扩容现象。

注：jdk版本1.7





幂等 1. 状态机。  标示 等

数据库和缓存双写如何保证 一致性

更新缓存 代价， 如果要调用很多接口 计算   更新数据库，失效缓存

失效缓存，

根据当前场景的容忍度，选择哪种

先操作数据库 还是先操作缓存

最终一致性。用到消息中间件，    删除指令发送的mq 中间件，  



Object.hashCode(); 

## 11. redis

1.Memcached单个key-value大小有限，一个value最大只支持1MB，而Redis最大支持512MB
2.Memcached只是个内存缓存，对可靠性无要求；而Redis更倾向于内存数据库，因此对对可靠性方面要求比较高
3.从本质上讲，Memcached只是一个单一key-value内存Cache；而Redis则是一个数据结构内存数据库，支持五种数据类型，因此Redis除单纯缓存作用外，还可以处理一些简单的逻辑运算，Redis不仅可以缓存，而且还可以作为数据库用
4.新版本（3.0）的Redis是指集群分布式，也就是说集群本身均衡客户端请求，各个节点可以交流，可拓展行、可维护性更强大。

Redis作为一个高性能的key-value数据库具有以下特征： 
- 多样的数据模型 
- 持久化 
- 主从同步 

Redis支持丰富的数据类型，最为常用的数据类型主要由五种：String、Hash、List、Set和Sorted Set。Redis通常将数据存储于内存中，或被配置为使用虚拟内存。Redis有一个很重要的特点就是它可以实现持久化数据，通过两种方式可以实现数据持久化：使用RDB快照的方式，将内存中的数据不断写入磁盘；或使用类似MySQL的AOF日志方式，记录每次更新的日志。前者性能较高，但是可能会引起一定程度的数据丢失；后者相反。 Redis支持将数据同步到多台从数据库上，这种特性对提高读取性能非常有益。

1、aof文件比rdb更新频率高，优先使用aof还原数据。
2、aof比rdb更安全也更大
3、rdb性能比aof好
4、如果两个都配了优先加载AOF
**RDB**
rdb是Redis DataBase缩写
功能核心函数rdbSave(生成RDB文件)和rdbLoad（从文件加载内存）两个函数
**AOF:**
Aof是Append-only file缩写
每当执行服务器(定时)任务或者函数时flushAppendOnlyFile 函数都会被调用， 这个函数执行以下两个工作
aof写入保存：
WRITE：根据条件，将 aof_buf 中的缓存写入到 AOF 文件
SAVE：根据条件，调用 fsync 或 fdatasync 函数，将 AOF 文件保存到磁盘中。
**存储结构:**
 内容是redis通讯协议(RESP )格式的命令文本存储。

RESP 是redis客户端和服务端之前使用的一种通讯协议；
RESP 的特点：实现简单、快速解析、可读性好

**Redis 有哪些架构模式？讲讲各自的特点**
 **单机版**
特点：简单
问题：
1、内存容量有限 2、处理能力有限 3、无法高可用。
**主从复制**
Redis 的复制（replication）功能允许用户根据一个 Redis 服务器来创建任意多个该服务器的复制品，其中被复制的服务器为主服务器（master），而通过复制创建出来的服务器复制品则为从服务器（slave）。 只要主从服务器之间的网络连接正常，主从服务器两者会具有相同的数据，主服务器就会一直将发生在自己身上的数据更新同步 给从服务器，从而一直保证主从服务器的数据相同。
特点：
1、master/slave 角色
2、master/slave 数据相同
3、降低 master 读压力在转交从库
问题：
无法保证高可用
没有解决 master 写的压力
**哨兵**
Redis sentinel 是一个分布式系统中监控 redis 主从服务器，并在主服务器下线时自动进行故障转移。其中三个特性：
监控（Monitoring）：  Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。
提醒（Notification）： 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。
自动故障迁移（Automatic failover）： 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作。
特点：
1、保证高可用
2、监控各个节点
3、自动故障迁移
缺点：主从模式，切换需要时间丢数据
没有解决 master 写的压力
**集群（proxy 型）**
Redis-Cluster采用无中心结构，每个节点保存数据和整个集群状态,每个节点都和其他所有节点连接。
特点：
1、无中心架构（不存在哪个节点影响性能瓶颈），少了 proxy 层。
2、数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布。16384 节点
3、可扩展性，可线性扩展到 1000 个节点，节点可动态添加或删除。
4、高可用性，部分节点不可用时，集群仍可用。通过增加 Slave 做备份数据副本
5、实现故障自动 failover，节点之间通过 gossip 协议交换状态信息，用投票机制完成 Slave到 Master 的角色提升。
缺点：
1、资源隔离性较差，容易出现相互影响的情况。
2、数据通过异步复制,不保证数据的强一致性
**使用过Redis分布式锁么，它是怎么实现的？**
先拿setnx来争抢锁，抢到之后，再用expire给锁加一个过期时间防止锁忘记了释放。
**如果在setnx之后执行expire之前进程意外crash或者要重启维护了，那会怎么样？**
set指令有非常复杂的参数，这个应该是可以同时把setnx和expire合成一条指令来用的！
缓存穿透
一般的缓存系统，都是按照key去缓存查询，如果不存在对应的value，就应该去后端系统查找（比如DB）。一些恶意的请求会故意查询不存在的key,请求量很大，就会对后端系统造成很大的压力。这就叫做缓存穿透。
如何避免？
1：对查询结果为空的情况也进行缓存，缓存时间设置短一点，或者该key对应的数据insert了之后清理缓存。
2：对一定不存在的key进行过滤。可以把所有的可能存在的key放到一个大的Bitmap中，查询时通过该bitmap过滤。
缓存雪崩
当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，会给后端系统带来很大压力。导致系统崩溃。
如何避免？
1：在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。
2：做二级缓存，A1为原始缓存，A2为拷贝缓存，A1失效时，可以访问A2，A1缓存失效时间设置为短期，A2设置为长期
3：不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀。

1. Redis有哪些数据结构？

字符串String、字典Hash、列表List、集合Set、有序集合SortedSet。 
如果你是Redis中高级用户，还需要加上下面几种数据结构HyperLogLog、Geo、Pub/Sub。 
如果你说还玩过Redis Module，像BloomFilter，RedisSearch，Redis-ML，面试官得眼睛就开始发亮了。

2. 使用过Redis分布式锁么，它是什么回事？

先拿setnx来争抢锁，抢到之后，再用expire给锁加一个过期时间防止锁忘记了释放。 
这时候对方会告诉你说你回答得不错，然后接着问如果在setnx之后执行expire之前进程意外crash或者要重启维护了，那会怎么样？ 
这时候你要给予惊讶的反馈：唉，是喔，这个锁就永远得不到释放了。紧接着你需要抓一抓自己得脑袋，故作思考片刻，好像接下来的结果是你主动思考出来的，然后回答：我记得set指令有非常复杂的参数，这个应该是可以同时把setnx和expire合成一条指令来用的！对方这时会显露笑容，心里开始默念：嗯，这小子还不错。

3. Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？

使用keys指令可以扫出指定模式的key列表。 
对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？ 
这个时候你要回答redis关键的一个特性：redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。

4. 使用过Redis做异步队列么，你是怎么用的？

一般使用list结构作为队列，rpush生产消息，lpop消费消息。当lpop没有消息的时候，要适当sleep一会再重试。 
如果对方追问可不可以不用sleep呢？list还有个指令叫blpop，在没有消息的时候，它会阻塞住直到消息到来。 
如果对方追问能不能生产一次消费多次呢？使用pub/sub主题订阅者模式，可以实现1:N的消息队列。 
如果对方追问pub/sub有什么缺点？在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如rabbitmq等。 
如果对方追问redis如何实现延时队列？我估计现在你很想把面试官一棒打死如果你手上有一根棒球棍的话，怎么问的这么详细。但是你很克制，然后神态自若的回答道：使用sortedset，拿时间戳作为score，消息内容作为key调用zadd来生产消息，消费者用zrangebyscore指令获取N秒之前的数据轮询进行处理。 
到这里，面试官暗地里已经对你竖起了大拇指。但是他不知道的是此刻你却竖起了中指，在椅子背后。

5. 如果有大量的key需要设置同一时间过期，一般需要注意什么？

如果大量的key过期时间设置的过于集中，到过期的那个时间点，redis可能会出现短暂的卡顿现象。一般需要在时间上加一个随机值，使得过期时间分散一些。

6. Redis如何做持久化的？

bgsave做镜像全量持久化，aof做增量持久化。因为bgsave会耗费较长时间，不够实时，在停机的时候会导致大量丢失数据，所以需要aof来配合使用。在redis实例重启时，优先使用aof来恢复内存的状态，如果没有aof日志，就会使用rdb文件来恢复。 
如果再问aof文件过大恢复时间过长怎么办？你告诉面试官，Redis会定期做aof重写，压缩aof文件日志大小。如果面试官不够满意，再拿出杀手锏答案，Redis4.0之后有了混合持久化的功能，将bgsave的全量和aof的增量做了融合处理，这样既保证了恢复的效率又兼顾了数据的安全性。这个功能甚至很多面试官都不知道，他们肯定会对你刮目相看。 
如果对方追问那如果突然机器掉电会怎样？取决于aof日志sync属性的配置，如果不要求性能，在每条写指令时都sync一下磁盘，就不会丢失数据。但是在高性能的要求下每次都sync是不现实的，一般都使用定时sync，比如1s1次，这个时候最多就会丢失1s的数据。

7. Pipeline有什么好处，为什么要用pipeline？

可以将多次IO往返的时间缩减为一次，前提是pipeline执行的指令之间没有因果相关性。使用redis-benchmark进行压测的时候可以发现影响redis的QPS峰值的一个重要因素是pipeline批次指令的数目。

8. Redis的同步机制了解么？

从从同步。第一次同步时，主节点做一次bgsave，并同时将后续修改操作记录到内存buffer，待完成后将rdb文件全量同步到复制节点，复制节点接受完成后将rdb镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。

9. 是否使用过Redis集群，集群的原理是什么？

Redis Sentinal着眼于高可用，在master宕机时会自动将slave提升为master，继续提供服务。 
Redis Cluster着眼于扩展性，在单个redis内存不足时，使用Cluster进行分片存储。

10 当redis内存超出物理内存限制，上缠上不允许内存数据和磁盘进行频繁交换，设置了参数MaxMemroy 超出后执行策略

noeviction   不会在执行写请求，默认策略

volatile-lru 尝试淘汰设置过期时间的key，最少使用的key 优先淘汰，没设置过期时间的不会淘汰

volatile-ttl  根据key的剩余寿命ttl值，越小优先淘汰

volatile-random。随机 淘汰  过期key集合中的随机的key

allkey-lru 全体key ，没有设置时间的也可能淘汰

allkey-random 所有key 随机



String 内部用的 字节数组 长度短 用emb形式存储， 超过44 用raw形式存储

hash  字典dict内部结构包含2个hashtale 通常一个有值，另一个扩容会用到， 渐进式搬迁 hashtable 和java 的hashmap 接近   

set 底层也是字典结构，只不过所有的value都是null

zset hash 元素少的时候用的压缩列表 ziplist 存储

list 用的快速列表  quicklist 是 ziplist和linkedlist的混合体。他将linkdelist按段切分，每段用ziplist 存储 双向指针链接起来，默认ziplist 长度8k 字节

zset 内部实现是一个hash字典加一个跳跃列表skiplist 

heyperLogLog。可以不是很精确的 确定 网页点击量

redis 单线程 用的NIO。 多路复用 select的事件轮询API ，非阻塞IO 

## 12. mybatis

1、#{ }是预编译处理，MyBatis在处理#{ }时，它会将sql中的#{ }替换为？，然后调用PreparedStatement的set方法来赋值，传入字符串后，会在值两边加上单引号，如上面的值 “4,44,514”就会变成“ ‘4,44,514’ ”；

2、${ }是字符串替换， MyBatis在处理${ }时,它会将sql中的${ }替换为变量的值，传入的数据不会加两边加上单引号。

注意：使用${ }会导致sql注入，不利于系统的安全性！

SQL注入：就是通过把SQL命令插入到Web表单提交或输入域名或页面请求的查询字符串，最终达到欺骗服务器执行恶意的SQL命令。常见的有匿名登录（在登录框输入恶意的字符串）、借助异常获取数据库信息等

应用场合：

1、#{ }：主要用户获取DAO中的参数数据,在映射文件的SQL语句中出现#{}表达式,底层会创建预编译的SQL；

2、   ${}.  :   主要用于获取配置文件数据,DAO接口中的参数信息,当$出现在映射文件的SQL语句中时创建的不是预编译的SQL,而是字符串的拼接,有可能会导致SQL注入问题.所以一般使用$接收dao参数时,这些参数一般是字段名,表名等,例如order by ${column}。

之前没深究这个问题，看其他人的回答，总结原因有：
1、PreparedStatement是预编译的
2、PreparedStatement参数不是简单拼接生成sql，而是先用?占位，之后再根据参数产生sql

但是上述原因都禁不起深究，毕竟不论是PreparedStatement还是Statement不都是最终传sql进数据库么？以上两个原因都无法避免sql注入。

深究一下，特别是查看网页
https://blog.csdn.net/lisehouniao/article/details/51523497
后，发现最重要的原因应该是：
PreparedStatement不是将参数简单拼凑成sql，而是做了一些预处理，将参数转换为string，两端加单引号，将参数内的一些特殊字符（换行，单双引号，斜杠等）做转义处理，这样就很大限度的避免了sql注入。

PageHelper首先将前端传递的参数保存到page这个对象中，接着将page的副本存放入ThreadLoacl中，这样可以保证分页的时候，参数互不影响，接着利用了mybatis提供的拦截器，取得ThreadLocal的值，重新拼装分页SQL，完成分页。



四大天王 statementHander。paramterHandler  resultsetHandler excutor

sqlSessionFactoryBulder。 sqlSessionFactory  sqlSession excutor

创建                                     应用全局                   一个请求一个。   mapper



## 13. 秒杀系统

所以，一般优化设计思路：将请求拦截在系统上游，降低下游压力。
在一个并发量大，实际需求小的系统中，应当尽量在前端拦截无效流量，降低下游服务器和数据库的压力，不然很可能造成数据库读写锁冲突，甚至导致死锁，最终请求超时。

**限流：**前端直接限流，允许少部分流量流向后端。

**削峰：**瞬时大流量峰值容易压垮系统，解决这个问题是重中之重。常用的消峰方法有异步处理、缓存和消息中间件等技术。

**异步处理：**秒杀系统是一个高并发系统，采用异步处理模式可以极大地提高系统并发量，其实异步处理就是削峰的一种实现方式。

**内存缓存：**秒杀系统最大的瓶颈一般都是数据库读写，由于数据库读写属于磁盘IO，性能很低，如果能够把部分数据或业务逻辑转移到内存缓存，效率会有极大地提升。

**消息队列：**消息队列可以削峰，将拦截大量并发请求，这也是一个异步处理过程，后台业务根据自己的处理能力，从消息队列中主动的拉取请求消息进行业务处理。

**可拓展：**当然如果我们想支持更多用户，更大的并发，最好就将系统设计成弹性可拓展的，如果流量来了，拓展机器就好了，像淘宝、京东等双十一活动时会临时增加大量机器应对交易高峰。

1. 解决方法:1>用户点击之后，按钮变灰2>写js 用户几秒之类只能请求一次

2. 站点层 使用集群等部署

   根据用户携带过来的 唯一标识（例如主键）,进行内存去重、3分钟只允许请求一次策略

3. 服务层 业务逻辑层 向上级屏蔽数据层1>对于写数据 可以使用队列的方式 队列中,例如商品数量1000个,可以设置队列中前1000个去写库,其它请求都提示已抢完2>读数据，采用缓存方式实现

4. 数据层 数据库 存数数据 sql nosql从性能上进行考虑1>选择性能好的数据库 例如oracle DB2等，支持更大的并发



**什么是秒杀**

秒杀场景一般会在电商网站举行一些活动或者节假日在12306网站上抢票时遇到。对于电商网站中一些稀缺或者特价商品，电商网站一般会在约定时间点对其进行限量销售，因为这些商品的特殊性，会吸引大量用户前来抢购，并且会在约定的时间点同时在秒杀页面进行抢购。

**秒杀系统场景特点**

- 秒杀时大量用户会在同一时间同时进行抢购，网站瞬时访问流量激增。
- 秒杀一般是访问请求数量远远大于库存数量，只有少部分用户能够秒杀成功。
- 秒杀业务流程比较简单，一般就是下订单减库存。

**秒杀架构设计理念**

**限流：** 鉴于只有少部分用户能够秒杀成功，所以要限制大部分流量，只允许少部分流量进入服务后端。

**削峰：**对于秒杀系统瞬时会有大量用户涌入，所以在抢购一开始会有很高的瞬间峰值。高峰值流量是压垮系统很重要的原因，所以如何把瞬间的高流量变成一段时间平稳的流量也是设计秒杀系统很重要的思路。实现削峰的常用的方法有利用缓存和消息中间件等技术。

**异步处理：**秒杀系统是一个高并发系统，采用异步处理模式可以极大地提高系统并发量，其实异步处理就是削峰的一种实现方式。

**内存缓存：**秒杀系统最大的瓶颈一般都是数据库读写，由于数据库读写属于磁盘IO，性能很低，如果能够把部分数据或业务逻辑转移到内存缓存，效率会有极大地提升。

**可拓展：**当然如果我们想支持更多用户，更大的并发，最好就将系统设计成弹性可拓展的，如果流量来了，拓展机器就好了。像淘宝、京东等双十一活动时会增加大量机器应对交易高峰。

**架构方案**

**一般秒杀系统架构**

**设计思路**

**将请求拦截在系统上游，降低下游压力：**秒杀系统特点是并发量极大，但实际秒杀成功的请求数量却很少，所以如果不在前端拦截很可能造成数据库读写锁冲突，甚至导致死锁，最终请求超时。

**充分利用缓存：**利用缓存可极大提高系统读写速度。

**消息队列：**消息队列可以削峰，将拦截大量并发请求，这也是一个异步处理过程，后台业务根据自己的处理能力，从消息队列中主动的拉取请求消息进行业务处理。

**前端方案**

**浏览器端(js)：**

页面静态化：将活动页面上的所有可以静态的元素全部静态化，并尽量减少动态元素。通过CDN来抗峰值。

禁止重复提交：用户提交之后按钮置灰，禁止重复提交

用户限流：在某一时间段内只允许用户提交一次请求，比如可以采取IP限流

**后端方案**

**服务端控制器层(网关层)**

限制uid（UserID）访问频率：我们上面拦截了浏览器访问的请求，但针对某些恶意攻击或其它插件，在服务端控制层需要针对同一个访问uid，限制访问频率。

**服务层**

上面只拦截了一部分访问请求，当秒杀的用户量很大时，即使每个用户只有一个请求，到服务层的请求数量还是很大。比如我们有100W用户同时抢100台手机，服务层并发请求压力至少为100W。

采用消息队列缓存请求：既然服务层知道库存只有100台手机，那完全没有必要把100W个请求都传递到数据库啊，那么可以先把这些请求都写到消息队列缓存一下，数据库层订阅消息减库存，减库存成功的请求返回秒杀成功，失败的返回秒杀结束。

利用缓存应对读请求：对类似于12306等购票业务，是典型的读多写少业务，大部分请求是查询请求，所以可以利用缓存分担数据库压力。

利用缓存应对写请求：缓存也是可以应对写请求的，比如我们就可以把数据库中的库存数据转移到Redis缓存中，所有减库存操作都在Redis中进行，然后再通过后台进程把Redis中的用户秒杀请求同步到数据库中。

**数据库层**

数据库层是最脆弱的一层，一般在应用设计时在上游就需要把请求拦截掉，数据库层只承担“能力范围内”的访问请求。所以，上面通过在服务层引入队列和缓存，让最底层的数据库高枕无忧。

**案例：利用消息中间件和缓存实现简单的秒杀系统**

Redis是一个分布式缓存系统，支持多种数据结构，我们可以利用Redis轻松实现一个强大的秒杀系统。

我们可以采用Redis 最简单的key-value数据结构，用一个原子类型的变量值(AtomicInteger)作为key，把用户id作为value，库存数量便是原子变量的最大值。对于每个用户的秒杀，我们使用 RPUSH key value插入秒杀请求， 当插入的秒杀请求数达到上限时，停止所有后续插入。

然后我们可以在台启动多个工作线程，使用 LPOP key 读取秒杀成功者的用户id，然后再操作数据库做最终的下订单减库存操作。

当然，上面Redis也可以替换成消息中间件如ActiveMQ、RabbitMQ等，也可以将缓存和消息中间件 组合起来，缓存系统负责接收记录用户请求，消息中间件负责将缓存中的请求同步到数据库。

## 14. classLoader 引起的内存泄漏



1、如果一个classloader存在内存泄露，那么它将会一直持有其加载的所有类对象，而每个类对象又持有了其所有静态变量。而在一般的应用程序当中，静态对象中常常维护了对象的缓存，单例对象以及各种配置和应用程序状态等数据。即使，在你的应用中也许没有任何静态的缓存，那也不意味着你使用的框架以及一些第三方资源不会这么做。因此，classloader内存泄露，导致的后果是往往是很惨重的。

2、那么classloader内存泄露，很难发生吗？错。一不小心引用了一个由另一个classloader加载的对象，就会导致classloader内存泄露。尽管这个对象似乎是无害的，但是，它依然维持了classloader的引用和所有相关的应用程序数据。应用中一个这样的误操作可能将会导致最终的java.lang.OutOfMemoryError:PermGen space error 
所以，classloader内存泄露，很容易发生



**总结：如果由一个classloader加载的对象被另一个classloader加载的对象引用，可能会引起classloader内存泄露。**

那么有哪些情况能引起classloader内存泄露问题呢？

**1、应用加载器打开的线程未关闭**

**2、应用加载器关联的ThreadLocale未释放**

**3、其他加载器引用了应用加载器的实例**

  知道了classloader内存泄露引起的大致原因，那么在实际应用中如何解决呢？下面我以tiny框架的websample项目作为示例，来讲解下classloader内存泄露问题的排除过程。

  从tomcat6.0.25开始，就有了"Find Leaks"按钮，用于检测应用是否存在classloader泄露的功能。首先在tomcat8下启动应用，然后通过控制台关闭应用，然后查看控制台日志，如果日志信息提示某个线程未关闭，或者ThreadLocal绑定的实现未释放，就说明应用存在classloader内存泄露问题，然后逐一解决问题，如果实在解决不了，比如是第三方包引入的问题，并且不能通过升级版本来解决的，可以借助Mattias Jiderhamn大神开发的"classloader-leak-prevention"来解决这些问题。

## 15 分库分表

#### 分表

比如你单表都几千万数据了，你确定你能扛住么？绝对不行，**单表数据量太大**，会极大影响你的 sql **执行的性能**，到了后面你的 sql 可能就跑的很慢了。一般来说，就以我的经验来看，单表到几百万的时候，性能就会相对差一些了，你就得分表了。

分表是啥意思？就是把一个表的数据放到多个表中，然后查询的时候你就查一个表。比如按照用户 id 来分表，将一个用户的数据就放在一个表中。然后操作的时候你对一个用户就操作那个表就好了。这样可以控制每个表的数据量在可控的范围内，比如每个表就固定在 200 万以内。

#### 分库

分库是啥意思？就是你一个库一般我们经验而言，最多支撑到并发 2000，一定要扩容了，而且一个健康的单库并发值你最好保持在每秒 1000 左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。

这就是所谓的**分库分表**，为啥要分库分表？你明白了吧。

| #            | 分库分表前                   | 分库分表后                                   |
| ------------ | ---------------------------- | -------------------------------------------- |
| 并发支撑情况 | MySQL 单机部署，扛不住高并发 | MySQL从单机到多机，能承受的并发增加了多倍    |
| 磁盘使用情况 | MySQL 单机磁盘容量几乎撑满   | 拆分为多个库，数据库服务器磁盘使用率大大降低 |
| SQL 执行性能 | 单表数据量太大，SQL 越跑越慢 | 单表数据量减少，SQL 执行效率明显提升         |

### 用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？

这个其实就是看看你了解哪些分库分表的中间件，各个中间件的优缺点是啥？然后你用过哪些分库分表的中间件。

比较常见的包括：

- cobar
- TDDL
- atlas
- sharding-jdbc
- mycat

#### cobar

阿里 b2b 团队开发和开源的，属于 proxy 层方案。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。

#### TDDL

淘宝团队开发的，属于 client 层方案。支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。

#### atlas

360 开源的，属于 proxy 层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。

#### sharding-jdbc

当当开源的，属于 client 层方案。确实之前用的还比较多一些，因为 SQL 语法支持也比较多，没有太多限制，而且目前推出到了 2.0 版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、TCC 事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从 2017 年一直到现在，是有不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也**可以选择的方案**。

#### mycat

基于 cobar 改造的，属于 proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 sharding jdbc 来说，年轻一些，经历的锤炼少一些。

#### 总结

综上，现在其实建议考量的，就是 sharding-jdbc 和 mycat，这两个都可以去考虑使用。

sharding-jdbc 这种 client 层方案的**优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高**，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要**耦合** sharding-jdbc 的依赖；

mycat 这种 proxy 层方案的**缺点在于需要部署**，自己运维一套中间件，运维成本高，但是**好处在于对于各个项目是透明的**，如果遇到升级之类的都是自己中间件那里搞就行了。

通常来说，这两个方案其实都可以选用，但是我个人建议中小型公司选用 sharding-jdbc，client 层方案轻便，而且维护成本低，不需要额外增派人手，而且中小型公司系统复杂度会低一些，项目也没那么多；但是中大型公司最好还是选用 mycat 这类 proxy 层方案，因为可能大公司系统和项目非常多，团队很大，人员充足，那么最好是专门弄个人来研究和维护 mycat，然后大量项目直接透明使用即可。

### 你们具体是如何对数据库如何进行垂直拆分或水平拆分的？

**水平拆分**的意思，就是把一个表的数据给弄到多个库的多个表里去，但是每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。水平拆分的意义，就是将数据均匀放更多的库里，然后用多个库来抗更高的并发，还有就是用多个库的存储容量来进行扩容。

![img](https:////upload-images.jianshu.io/upload_images/10089464-0e01dfe246b5c7ac.png?imageMogr2/auto-orient/strip|imageView2/2/w/474)

**垂直拆分**的意思，就是**把一个有很多字段的表给拆分成多个表**，**或者是多个库上去**。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，会**将较少的访问频率很高的字段放到一个表里去**，然后**将较多的访问频率很低的字段放到另外一个表里去**。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。

![img](https:////upload-images.jianshu.io/upload_images/10089464-ab3069913c0f097c.png?imageMogr2/auto-orient/strip|imageView2/2/w/320)

database-split-vertically

这个其实挺常见的，不一定我说，大家很多同学可能自己都做过，把一个大表拆开，订单表、订单支付表、订单商品表。

还有**表层面的拆分**，就是分表，将一个表变成 N 个表，就是**让每个表的数据量控制在一定范围内**，保证 SQL 的性能。否则单表数据量越大，SQL 性能就越差。一般是 200 万行左右，不要太多，但是也得看具体你怎么操作，也可能是 500 万，或者是 100 万。你的SQL越复杂，就最好让单表行数越少。

好了，无论分库还是分表，上面说的那些数据库中间件都是可以支持的。就是基本上那些中间件可以做到你分库分表之后，**中间件可以根据你指定的某个字段值**，比如说 userid，**自动路由到对应的库上去，然后再自动路由到对应的表里去**。

你就得考虑一下，你的项目里该如何分库分表？一般来说，垂直拆分，你可以在表层面来做，对一些字段特别多的表做一下拆分；水平拆分，你可以说是并发承载不了，或者是数据量太大，容量承载不了，你给拆了，按什么字段来拆，你自己想好；分表，你考虑一下，你如果哪怕是拆到每个库里去，并发和容量都ok了，但是每个库的表还是太大了，那么你就分表，将这个表分开，保证每个表的数据量并不是很大。

而且这儿还有两种**分库分表的方式**：

- 一种是按照 range 来分，就是每个库一段连续的数据，这个一般是按比如**时间范围**来的，但是这种一般较少用，因为很容易产生热点问题，大量的流量都打在最新的数据上了。
- 或者是按照某个字段hash一下均匀分散，这个较为常用。

range 来分，好处在于说，扩容的时候很简单，因为你只要预备好，给每个月都准备一个库就可以了，到了一个新的月份的时候，自然而然，就会写新的库了；缺点，但是大部分的请求，都是访问最新的数据。实际生产用 range，要看场景。

hash 分发，好处在于说，可以平均分配每个库的数据量和请求压力；坏处在于说扩容起来比较麻烦，会有一个数据迁移的过程，之前的数据需要重新计算 hash 值重新分配到不同的库或表。

## 2. 现在有一个未分库分表的系统，未来要分库分表，如何设计才可以让系统从未分库分表**动态切换**到分库分表上？

### 面试题剖析

这个其实从 low 到高大上有好几种方案，我们都玩儿过，我都给你说一下。

### 停机迁移方案

我先给你说一个最 low 的方案，就是很简单，大家伙儿凌晨 12 点开始运维，网站或者 app 挂个公告，说 0 点到早上 6 点进行运维，无法访问。

接着到 0 点停机，系统停掉，没有流量写入了，此时老的单库单表数据库静止了。然后你之前得写好一个**导数的一次性工具**，此时直接跑起来，然后将单库单表的数据哗哗哗读出来，写到分库分表里面去。

导数完了之后，就 ok 了，修改系统的数据库连接配置啥的，包括可能代码和 SQL 也许有修改，那你就用最新的代码，然后直接启动连到新的分库分表上去。

验证一下，ok了，完美，大家伸个懒腰，看看看凌晨 4 点钟的北京夜景，打个滴滴回家吧。

但是这个方案比较 low，谁都能干，我们来看看高大上一点的方案。

![img](https:////upload-images.jianshu.io/upload_images/10089464-df96ac2689c6f9dd.png?imageMogr2/auto-orient/strip|imageView2/2/w/863)

database-shard-method-1

### 双写迁移方案

这个是我们常用的一种迁移方案，比较靠谱一些，不用停机，不用看北京凌晨 4 点的风景。

简单来说，就是在线上系统里面，之前所有写库的地方，增删改操作，**除了对老库增删改，都加上对新库的增删改**，这就是所谓的**双写**，同时写俩库，老库和新库。

然后**系统部署**之后，新库数据差太远，用之前说的导数工具，跑起来读老库数据写新库，写的时候要根据 gmt_modified 这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是比新库的数据新才会写。简单来说，就是不允许用老数据覆盖新数据。

导完一轮之后，有可能数据还是存在不一致，那么就程序自动做一轮校验，比对新老库每个表的每条数据，接着如果有不一样的，就针对那些不一样的，从老库读数据再次写。反复循环，直到两个库每个表的数据都完全一致为止。

接着当数据完全一致了，就 ok 了，基于仅仅使用分库分表的最新代码，重新部署一次，不就仅仅基于分库分表在操作了么，还没有几个小时的停机时间，很稳。所以现在基本玩儿数据迁移之类的，都是这么干的。



![img](https:////upload-images.jianshu.io/upload_images/10089464-24013794eb61fa14.png?imageMogr2/auto-orient/strip|imageView2/2/w/864)

database-shard-method-2

## 3. 如何设计可以动态扩容缩容的分库分表方案？

## 考点分析

对于分库分表来说，主要是面对以下问题：

- 选择一个数据库中间件，调研、学习、测试；
- 设计你的分库分表的一个方案，你要分成多少个库，每个库分成多少个表，比如 3 个库，每个库 4 个表；
- 基于选择好的数据库中间件，以及在测试环境建立好的分库分表的环境，然后测试一下能否正常进行分库分表的读写；
- 完成单库单表到分库分表的**迁移**，双写方案；
- 线上系统开始基于分库分表对外提供服务；
- 扩容了，扩容成 6 个库，每个库需要 12 个表，你怎么来增加更多库和表呢？

这个是你必须面对的一个事儿，就是你已经弄好分库分表方案了，然后一堆库和表都建好了，基于分库分表中间件的代码开发啥的都好了，测试都 ok 了，数据能均匀分布到各个库和各个表里去，而且接着你还通过双写的方案咔嚓一下上了系统，已经直接基于分库分表方案在搞了。

那么现在问题来了，你现在这些库和表又支撑不住了，要继续扩容咋办？这个可能就是说你的每个库的容量又快满了，或者是你的表数据量又太大了，也可能是你每个库的写并发太高了，你得继续扩容。

这都是玩儿分库分表线上必须经历的事儿。

### 面试题剖析

### 停机扩容（不推荐）

这个方案就跟停机迁移一样，步骤几乎一致，唯一的一点就是那个导数的工具，是把现有库表的数据抽出来慢慢倒入到新的库和表里去。但是最好别这么玩儿，有点不太靠谱，因为既然**分库分表**就说明数据量实在是太大了，可能多达几亿条，甚至几十亿，你这么玩儿，可能会出问题。

从单库单表迁移到分库分表的时候，数据量并不是很大，单表最大也就两三千万。那么你写个工具，多弄几台机器并行跑，1小时数据就导完了。这没有问题。

如果 3 个库 + 12 个表，跑了一段时间了，数据量都 1~2 亿了。光是导 2 亿数据，都要导个几个小时，6 点，刚刚导完数据，还要搞后续的修改配置，重启系统，测试验证，10 点才可以搞完。所以不能这么搞。

### 优化后的方案

一开始上来就是 32 个库，每个库 32 个表，那么总共是 1024 张表。

我可以告诉各位同学，这个分法，第一，基本上国内的互联网肯定都是够用了，第二，无论是并发支撑还是数据量支撑都没问题。

每个库正常承载的写入并发量是 1000，那么 32 个库就可以承载32 * 1000 = 32000 的写并发，如果每个库承载 1500 的写并发，32 * 1500 = 48000 的写并发，接近 5万/s 的写入并发，前面再加一个MQ，削峰，每秒写入 MQ 8 万条数据，每秒消费 5 万条数据。

有些除非是国内排名非常靠前的这些公司，他们的最核心的系统的数据库，可能会出现几百台数据库的这么一个规模，128个库，256个库，512个库。

1024 张表，假设每个表放 500 万数据，在 MySQL 里可以放 50 亿条数据。

每秒的 5 万写并发，总共 50 亿条数据，对于国内大部分的互联网公司来说，其实一般来说都够了。

谈分库分表的扩容，**第一次分库分表，就一次性给他分个够**，32 个库，1024 张表，可能对大部分的中小型互联网公司来说，已经可以支撑好几年了。

一个实践是利用 `32 * 32` 来分库分表，即分为 32 个库，每个库里一个表分为 32 张表。一共就是 1024 张表。根据某个 id 先根据 32 取模路由到库，再根据 32 取模路由到库里的表。

| orderId | id % 32 (库) | id / 32 % 32 (表) |
| ------- | ------------ | ----------------- |
| 259     | 3            | 8                 |
| 1189    | 5            | 5                 |
| 352     | 0            | 11                |
| 4593    | 17           | 15                |

刚开始的时候，这个库可能就是逻辑库，建在一个数据库上的，就是一个mysql服务器可能建了 n 个库，比如 32 个库。后面如果要拆分，就是不断在库和 mysql 服务器之间做迁移就可以了。然后系统配合改一下配置即可。

比如说最多可以扩展到32个数据库服务器，每个数据库服务器是一个库。如果还是不够？最多可以扩展到 1024 个数据库服务器，每个数据库服务器上面一个库一个表。因为最多是1024个表。

这么搞，是不用自己写代码做数据迁移的，都交给 dba 来搞好了，但是 dba 确实是需要做一些库表迁移的工作，但是总比你自己写代码，然后抽数据导数据来的效率高得多吧。

哪怕是要减少库的数量，也很简单，其实说白了就是按倍数缩容就可以了，然后修改一下路由规则。

这里对步骤做一个总结：

1. 设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是 32库 * 32表，对于大部分公司来说，可能几年都够了。
2. 路由的规则，orderId 模 32 = 库，orderId / 32 模 32 = 表
3. 扩容的时候，申请增加更多的数据库服务器，装好 mysql，呈倍数扩容，4 台服务器，扩到 8 台服务器，再到 16 台服务器。
4. 由 dba 负责将原先数据库服务器的库，迁移到新的数据库服务器上去，库迁移是有一些便捷的工具的。
5. 我们这边就是修改一下配置，调整迁移的库所在数据库服务器的地址。
6. 重新发布系统，上线，原先的路由规则变都不用变，直接可以基于 n 倍的数据库服务器的资源，继续进行线上系统的提供服务。

## 4. 分库分表之后，id 主键如何处理？

### 考点分析

其实这是分库分表之后你必然要面对的一个问题，就是 id 咋生成？因为要是分成多个表之后，每个表都是从 1 开始累加，那肯定不对啊，需要一个**全局唯一**的 id 来支持。所以这都是你实际生产环境中必须考虑的问题。

### 面试题剖析

### 基于数据库的实现方案

#### 数据库自增 id

这个就是说你的系统里每次得到一个 id，都是往一个库的一个表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个 id。拿到这个 id 之后再往对应的分库分表里去写入。

这个方案的好处就是方便简单，谁都会用；**缺点就是单库生成**自增 id，要是高并发的话，就会有瓶颈的；如果你硬是要改进一下，那么就专门开一个服务出来，这个服务每次就拿到当前 id 最大值，然后自己递增几个 id，一次性返回一批 id，然后再把当前最大 id 值修改成递增几个 id 之后的一个值；但是**无论如何都是基于单个数据库**。

**适合的场景**：你分库分表就俩原因，要不就是单库并发太高，要不就是单库数据量太大；除非是你**并发不高，但是数据量太大**导致的分库分表扩容，你可以用这个方案，因为可能每秒最高并发最多就几百，那么就走单独的一个库和表生成自增主键即可。

#### 设置数据库 sequence 或者表自增字段步长

可以通过设置数据库 sequence 或者表的自增字段步长来进行水平伸缩。

比如说，现在有 8 个服务节点，每个服务节点使用一个 sequence 功能来产生 ID，每个 sequence 的起始 ID 不同，并且依次递增，步长都是 8。

![img](https:////upload-images.jianshu.io/upload_images/10089464-496353a73d743290.png?imageMogr2/auto-orient/strip|imageView2/2/w/419)

database-id-sequence-step

**适合的场景**：在用户防止产生的 ID 重复时，这种方案实现起来比较简单，也能达到性能目标。但是服务节点固定，步长也固定，将来如果还要增加服务节点，就不好搞了。

### UUID

好处就是本地生成，不要基于数据库来了；不好之处就是，UUID 太长了、占用空间大，**作为主键性能太差**了；更重要的是，UUID 不具有有序性，会导致 B+ 树索引在写的时候有过多的随机写操作（连续的 ID 可以产生部分顺序写），还有，由于在写的时候不能产生有顺序的 append 操作，而需要进行 insert 操作，将会读取整个 B+ 树节点到内存，在插入这条记录后会将整个节点写回磁盘，这种操作在记录占用空间比较大的情况下，性能下降明显。

适合的场景：如果你是要随机生成个什么文件名、编号之类的，你可以用 UUID，但是作为主键是不能用 UUID 的。

```java
UUID.randomUUID().toString().replace(“-”, “”) -> sfsdf23423rr234sfdaf
```

### 获取系统当前时间

这个就是获取当前时间即可，但是问题是，**并发很高的时候**，比如一秒并发几千，**会有重复的情况**，这个是肯定不合适的。基本就不用考虑了。

适合的场景：一般如果用这个方案，是将当前时间跟很多其他的业务字段拼接起来，作为一个 id，如果业务上你觉得可以接受，那么也是可以的。你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的编号。

### snowflake 算法

snowflake 算法是 twitter 开源的分布式 id 生成算法，采用 Scala 语言实现，是把一个 64 位的 long 型的 id，1 个 bit 是不用的，用其中的 41 bit 作为毫秒数，用 10 bit 作为工作机器 id，12 bit 作为序列号。

- 1 bit：不用，为啥呢？因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。
- 41 bit：表示的是时间戳，单位是毫秒。41 bit 可以表示的数字多达 `2^41 - 1`，也就是可以标识 `2^41 - 1` 个毫秒值，换算成年就是表示69年的时间。
- 10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10台机器上哪，也就是1024台机器。但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 `2^5`个机房（32个机房），每个机房里可以代表 `2^5` 个机器（32台机器）。
- 12 bit：这个是用来记录同一个毫秒内产生的不同 id，12 bit 可以代表的最大正整数是 `2^12 - 1 = 4096`，也就是说可以用这个 12 bit 代表的数字来区分**同一个毫秒内**的 4096 个不同的 id。

```ruby
0 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000000
public class IdWorker {

    private long workerId;
    private long datacenterId;
    private long sequence;

    public IdWorker(long workerId, long datacenterId, long sequence) {
        // sanity check for workerId
        // 这儿不就检查了一下，要求就是你传递进来的机房id和机器id不能超过32，不能小于0
        if (workerId > maxWorkerId || workerId < 0) {
            throw new IllegalArgumentException(
                    String.format("worker Id can't be greater than %d or less than 0", maxWorkerId));
        }
        if (datacenterId > maxDatacenterId || datacenterId < 0) {
            throw new IllegalArgumentException(
                    String.format("datacenter Id can't be greater than %d or less than 0", maxDatacenterId));
        }
        System.out.printf(
                "worker starting. timestamp left shift %d, datacenter id bits %d, worker id bits %d, sequence bits %d, workerid %d",
                timestampLeftShift, datacenterIdBits, workerIdBits, sequenceBits, workerId);

        this.workerId = workerId;
        this.datacenterId = datacenterId;
        this.sequence = sequence;
    }

    private long twepoch = 1288834974657L;

    private long workerIdBits = 5L;
    private long datacenterIdBits = 5L;

    // 这个是二进制运算，就是 5 bit最多只能有31个数字，也就是说机器id最多只能是32以内
    private long maxWorkerId = -1L ^ (-1L << workerIdBits);

    // 这个是一个意思，就是 5 bit最多只能有31个数字，机房id最多只能是32以内
    private long maxDatacenterId = -1L ^ (-1L << datacenterIdBits);
    private long sequenceBits = 12L;

    private long workerIdShift = sequenceBits;
    private long datacenterIdShift = sequenceBits + workerIdBits;
    private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits;
    private long sequenceMask = -1L ^ (-1L << sequenceBits);

    private long lastTimestamp = -1L;

    public long getWorkerId() {
        return workerId;
    }

    public long getDatacenterId() {
        return datacenterId;
    }

    public long getTimestamp() {
        return System.currentTimeMillis();
    }

    public synchronized long nextId() {
        // 这儿就是获取当前时间戳，单位是毫秒
        long timestamp = timeGen();

        if (timestamp < lastTimestamp) {
            System.err.printf("clock is moving backwards.  Rejecting requests until %d.", lastTimestamp);
            throw new RuntimeException(String.format(
                    "Clock moved backwards.  Refusing to generate id for %d milliseconds", lastTimestamp - timestamp));
        }

        if (lastTimestamp == timestamp) {
            // 这个意思是说一个毫秒内最多只能有4096个数字
            // 无论你传递多少进来，这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围
            sequence = (sequence + 1) & sequenceMask;
            if (sequence == 0) {
                timestamp = tilNextMillis(lastTimestamp);
            }
        } else {
            sequence = 0;
        }

        // 这儿记录一下最近一次生成id的时间戳，单位是毫秒
        lastTimestamp = timestamp;

        // 这儿就是将时间戳左移，放到 41 bit那儿；
        // 将机房 id左移放到 5 bit那儿；
        // 将机器id左移放到5 bit那儿；将序号放最后12 bit；
        // 最后拼接起来成一个 64 bit的二进制数字，转换成 10 进制就是个 long 型
        return ((timestamp - twepoch) << timestampLeftShift) | (datacenterId << datacenterIdShift)
                | (workerId << workerIdShift) | sequence;
    }

    private long tilNextMillis(long lastTimestamp) {
        long timestamp = timeGen();
        while (timestamp <= lastTimestamp) {
            timestamp = timeGen();
        }
        return timestamp;
    }

    private long timeGen() {
        return System.currentTimeMillis();
    }

    // ---------------测试---------------
    public static void main(String[] args) {
        IdWorker worker = new IdWorker(1, 1, 1);
        for (int i = 0; i < 30; i++) {
            System.out.println(worker.nextId());
        }
    }

}
```

怎么说呢，大概这个意思吧，就是说 41 bit 是当前毫秒单位的一个时间戳，就这意思；然后 5 bit 是你传递进来的一个**机房** id（但是最大只能是 32 以内），另外 5 bit 是你传递进来的**机器** id（但是最大只能是 32 以内），剩下的那个 12 bit序列号，就是如果跟你上次生成 id 的时间还在一个毫秒内，那么会把顺序给你累加，最多在 4096 个序号以内。

所以你自己利用这个工具类，自己搞一个服务，然后对每个机房的每个机器都初始化这么一个东西，刚开始这个机房的这个机器的序号就是 0。然后每次接收到一个请求，说这个机房的这个机器要生成一个 id，你就找到对应的 Worker 生成。

利用这个 snowflake 算法，你可以开发自己公司的服务，甚至对于机房 id 和机器 id，反正给你预留了 5 bit + 5 bit，你换成别的有业务含义的东西也可以的。

这个 snowflake 算法相对来说还是比较靠谱的，所以你要真是搞分布式 id 生成，如果是高并发啥的，那么用这个应该性能比较好，一般每秒几万并发的场景，也足够你用了。