---
title: centos6 升级python
date: 2018-12-25 17:29:53
categories:
- elk
tags:
- python
- elk
---



第一种架构

![](http://wangrrui.com/2018-12-25-025422.jpg)

**架构解读 : （整个架构从左到右，总共分为5层）**

**第一层、数据采集层**

最左边的是业务服务器集群，上面安装了filebeat做日志采集，同时把采集的日志分别发送给两个logstash服务。

**第二层、数据处理层，数据缓存层**

logstash服务把接受到的日志经过格式处理，转存到本地的kafka broker+zookeeper 集群中。

**第三层、数据转发层**

这个单独的Logstash节点会实时去kafka broker集群拉数据，转发至ES DataNode。

**第四层、数据持久化存储**

ES DataNode 会把收到的数据，写磁盘，建索引库。

**第五层、数据检索，数据展示**

ES Master + Kibana 主要协调ES集群，处理数据检索请求，数据展示





![](http://wangrrui.com/2018-12-25-025525.jpg)



**二.对上面的工作流程进行简单描述。**

（1）将filebeat部署到需要采集日志的服务器上，filebeat将采集到的日志数据传输到kafka中。

（2）kafka将获取到的日志信息存储起来，并且作为输入(input)传输给logstash。

（3）logstash将kafka中的数据作为输入，并且把kafka中的数据进行过滤等其他操作，然后把操作后得到的数据输入（output）到es(elasticsearch)中。

 （4）es(基于lucene搜索引擎）对logstash中的数据进行处理，并且将数据作为输入传送给kibna进行显示。

链接：https://www.jianshu.com/p/b4c3fb0babaa





2.如果完成后如果数据显示不了，可以先到根据工作流程到各个节点查询数据是否存储和传输成功。如查询filebeat是否成功把数据传输到了kafka，可以使用kafka中如下命令查询：

bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning

查看日志filebeat中的数据是否正常在kafka中存储。

在filebeat配置的日志中手动添加日志： echo "test" > access.log，可以在kafka中动态的显示出该信息

作者：jimmy_w

链接：https://www.jianshu.com/p/b4c3fb0babaa

來源：简书

简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。